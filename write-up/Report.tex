\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{subcaption}

\title
{
    Single-Image 3DGS Scene Reconstruction with Geometry-Aware Priors
}
\author
{
    Machine Visual Perception Course Project Report
}

\begin{document}
\maketitle

\section*{Information}
Authors: TEST
\newline
Group Number: TEST

\section{Chapter 1: Introduction and Motivation}
\subsection{Section 1.1: Introduction to the problem}
[Provide a thorough introduction to the problem and why it is important. Briefly explain what general techniques there are and how your project fits.]

\noindent
RCONSTRUCTING 3D MODELS FROM IMAGES IS ACHANLEGNING TASK THAT REQUIRES LARGES NUMBER OF REFERNCE/INPTU IMAGES AND INTENSE PROCESSING
TYPICALLY DONE WITH ENRF OR OTHER
MODERN ADVANCEMENTS FOCUS ON REDUCING INPOUT PHOTO NUMBER ND PROCESSIGN COMPELXITY
SPECIFICALLY, Developments in 3D Gaussian Splatting methods allow for 3D scene reconstruction using single or few RGB images. 

While faster than other scene reconstruction techniques and requiring only a "one-shot" pass, these approaches often suffer from challenges such as layout/scale drift, over-smooth geometry and hallucinations in occluded regions. 

This project focuses on one recent method, SplatterImage \cite{splatterImage}, as a baseline. 
By storing 3D Gaussians in an image, it reduces reconstruction to learning an image-to-image neural network, allowing the use of a 2D U-Net to form the representation. 
Each pixel stores the parameters for a corresponding 3D Gaussian, allowing for reconstruction in a single feed-forward pass. This reduces the need for large amounts of compute.

Despite its speed, SplatterImage does have some issues that have been noted in related works, 
particularly in reconstructing structures unseen in the input view, including for views significantly different from the source. 
This project aims to address this by proposing a lightweight augmentation to the model, by integrating explicit geometry priors 
(such as planes, normals, visibility cues, depth, segmentation, edge maps) with minimal architectural changes.

EXPAND/REWORD ABOVE

\subsection{Section 1.2: Background and related work}
[Include a few very relevant related works and how your work relates to those, expanding on the previous section. We do not expect you to cover all previous works.]

Needs elaboration and rewording

Over the years, other representations for single-view 3D reconstruction have been used. Traditional methods typically use explicit 3D representations such as point clouds \cite{pointCloud} or meshes \cite{mesh}. Implicit representations like NeRF \cite{nerf} have also been used, but are slow to render. 

discuss triplane?

The triplane representation was proposed to efficiently and expressively represent 3D volumes \cite{triplane}, as a compromise between rendering speed and memory consumption. They were shown to scale to large datasets like Objaverse \cite{objaversexl}\cite{objaverse}, but at the cost of hundreds of GPUs for multiple days \cite{triplanebasedmodel}.

discuss 3dgs

3D Gaussian splatting \cite{3dgs} was proposed to offer real-time radiance field rendering by introducing a 3D Gaussian scene representation, speeding up scene optimization and novel view synthesis while maintaining a high quality.

discuss splatterimage

Splatter Image \cite{splatterImage} then applies Gaussian Splatting to monocular reconstrution by using a set of 3D Gaussians as the 3D representation. It predicts a 3D Gaussian for each of the input image pixels and uses a 2D image as the container of the 3D Gaussians, storing the parameters of one Gaussian per pixel. This reduces the reconstruction problem to learning an image-to-image neural network, allowing the reconstructor to be implemented utilizing only efficient 2D operators. The use of Gaussian Splatting in this approach increases rendering and space efficiency, which benefits inference and training. Our work continues to expand on this method through investigating different geometry priors and integrating them into the current model as appropriate.

\subsection{Section 1.3: Overview of the idea}
[Provide an overview stating why the idea of the project makes sense and what the main motivation is.]
splatteriamge suffers from hallucination and problems simply due to lack of data
feeding models additional data improves reconstruction 
with modern compute and ml advancements there now exist many good quality pretrained geometry related models
for example generating depth, normal maps, segmentation
we propose using the knowledge/capacibiltiy of these models to predict additional priors of input images, creating a modified model that accepts these priors
these should result in an improved recostruction quality
we propose performing ablation study to see which priors are most effective/significant in changing the reconstruction quality

\section{Chapter 2: Method}
\subsection{Section 2.1: Baseline algorithm}
[Explain the baseline architecture you used to build your algorithm on. You may reproduce figures from the original papers.]

explain 3dshg output
explain spaltteriamge architecture with diagram

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{write-up/images/splatter_image_diagram.png}
    \caption{Overview of SplatterImage taken from \cite{splatterImage}}
    \label{fig:splatter_img_diagram}
\end{figure}

Splatter Image uses a standard image-to-image neural network architecture to predict a Gaussian for each pixel of the input image I, generating the output image M as the Splatter Image. Learning to predict the Splatter Image can be done on a single GPU using at most 20GB of memory at training time for most single-view reconstruction experiments (except for Objaverse, where 2 GPUs were used and 26GB of memory was used on each). Most of this neural network architecture is identical to the SongUNet of \cite{songunet}, but the last layer is replaced with a $1 \times 1$ convolutional layer with $12 + k_c$ output channels, where $k_c \in \{3, 12\}$ depending on the colour model. The output tensor codes for parameters that are then transformed to opacity, offset, depth, scale, rotation and colour respectively. These parameters are then activated by non-linear functions to obtain the Gaussian paramters, such as the opacity and depth. The Gaussian Splatting implementation of \cite{3dgs} is used for rasterization to generate $360^\circ$ views of the original input image.

\subsection{Section 2.2: Algorithm improvements}
[Explain what you implemented to improve over the baseline. You may include figures to explain the idea and logic. Focus on the ideas and not the implementation.]

explain insertion of additional layers
explain addition of transofrmer/FiLM layers to allow multimodal input from segmentation tokens 
explain addition of LORA matrices due to compute limitations/allowign working of exisitng model

\subsection{Section 2.3: Implementation details}
[Explain how you implemented the improvements. You may include code snippets with the corresponding explanations.]

\subsubsection{Normal Map Exploration}

We studied using normal maps as a 3D spatial prior to augment the visual tokens for the Splatter Image. A normal map stores surface normal data as RGB colour information, which shows how light interacts with the surface at a per-pixel level, hence we wanted to investigate if that could help the model generate the 3D surface of the input image.

For our ground truths we use \cite{xu2019disn} which provides a dataset of higher resolution images of the ShapeNet models from \cite{ShapeNet} each paired with a depth image, a normal map and an albedo image at \url{https://github.com/Xharlie/ShapenetRender_more_variation}. We then feed these images into the normal map generation models and compare them to the given normal maps to evaluate their performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/normal_gnd_truth.png}
    \caption{Example of image with maps used as ground truth taken from \cite{Bae2021}}
    \label{fig:ground_truth}
\end{figure}

For the models we referenced \cite{Bae2021} which implements a network which estimates the per-pixel surface normal probability distribution and uses uncertainty-guided sampling to improve the quality of prediction of surface normals. The paper provided code at \url{ https://github.com/baegwangbin/surface_normal_uncertainty} that implemented this method on a network trained on ScanNet \cite{dai2017scannet}, with the ground truth and data split provided by FrameNet \cite{huang2019framenet}, and another trained on NYUv2 \cite{NYUv2}, with the ground truth and data split provided by GeoNet \cite{qi2018geonet} \cite{qi2020geonet++}. Both models take in the original image and dimensions of the image as input and return a corresponding normal map with the same dimensions as the given input dimensions.

We run both pretrained models on the dataset.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/original.png}
        \caption{Original}
        \label{fig:highdefinput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN.png}
        \caption{ScanNet output}
        \label{fig:highdefoutput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/GN.png}
        \caption{NYUv2 output}
        \label{fig:gn1}
    \end{subfigure}

    \caption{Comparison of original input and two model outputs}
\end{figure}

We then pass in input dimensions larger than the actual ones into the models, such that a normal map larger than the original input is produced. We then resize the image to the original input dimensions.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/gnd_truth.png}
        \caption{Ground truth from \cite{xu2019disn}}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_resized.png}
        \caption{ScanNet output}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/GN_resized.png}
        \caption{NYUv2 output}
        \label{fig:gn2}
    \end{subfigure}

    \caption{Comparison of model outputs when setting input dimensions as 448x448 instead of 224x224 alongside ground truth}
\end{figure}

The normal map generated for images when given larger input dimensions seem to have more clearly defined edges and surface contouring. It is also important to note that the ground truth for NYUv2 is only defined for the centre crop of the image and the prediction is therefore not accurate outside the centre. This is shown in figures \ref{fig:gn1} and \ref{fig:gn2} where noise is generated around the borders of the normal maps.

To compare our generated normal maps to the ground truth normal maps provided in \cite{xu2019disn}, we first mask out the background of the generated normal maps such that the difference in background colour does not contribute to the evaluation metrics for normal map generation.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/gnd_truth.png}
        \caption{Ground truth from \cite{xu2019disn}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_resized.png}
        \caption{ScanNet output (448x448) version}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_filtered.png}
        \caption{Output with background masking}
    \end{subfigure}

    \caption{Example of masking out background for model evaluation against ground truth}
\end{figure}

We then use Pixel Based Visual Information Fidelity to compare the normal maps generated by the two models to the ground truth. Visual Information Fidelity is a reference image quality metric that quantifies the amount of visual information preserved after image processing \cite{vifIntro} and can be used to measure various image quality attributes such as noise level and sharpness \cite{vifMetrics}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/vif_plot.png}
    \caption{Comparison of VIF between ground truth and different models}
    \label{fig:vifgraph}
\end{figure}

From Figure \ref{fig:vifgraph} we see that the model trained on ScanNet generates normal maps that are closer to the ground truth compared to that trained on NYUv2 on average. Hence, in the final model we decided to use the model trained on ScanNet on the ShapeNet database in \cite{ShapeNet}.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_orig.png}
        \caption{Original image from \cite{ShapeNet} of size 128x128}
        \label{fig:lowdefinput}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_output.png}
        \caption{Output from ScanNet model with 128x128 passed in as input dimensions}
        \label{fig:lowdefoutput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_output_resized.png}
        \caption{Output from ScanNet model with 512x512 passed in as input dimensions}
        \label{fig:lowdefoutputfixed}
    \end{subfigure}

    \caption{Original ShapeNet image and normal map outputs}
\end{figure}

Without passing in dimensions larger than the input image into the model, we can see from comparing Figures \ref{fig:highdefinput} and \ref{fig:highdefoutput} to Figures \ref{fig:lowdefinput} and \ref{fig:lowdefoutput} that the quality of the normal map generated decreases as the resolution of the original input image decreases. Hence, we pass in much larger input dimensions (512x512) to generate a normal map of higher quality, as shown in Figure \ref{fig:lowdefoutputfixed}.

\subsubsection{Depth Map Exploration}
Depth maps store the distance of a surface from the camera per-pixel. These distances vary in type, such as metric, which considers the physical distance from the camera to the observed point, and relative (such as those produced by the models below). Monocular depth estimation (MDE) models input just a singular image, and produce a depth map (relative distance). 

Produced depth maps were compared against the ``ground truths'' produced by \url{https://github.com/Xharlie/ShapenetRender_more_variation}, as was done in the normal priors exploration. An example of the depth map produced by them is visible in Figure  \ref{fig:ground_truth}. However, it is important to note that these depth map ``ground truths'' were not always perfect, as can be seen in the following example: 

- (INSERT) Image needed here of poor ground truth. 

This inclined us to take the quantitative results produced by comparing MDE models tested against these ground truths with a pinch of salt. For each produced depth map, the following metrics were used to compare against the ground truths. 
\begin{enumerate}
\item \textbf{Absolute Relative Error}: Measures the average difference between the predicted depth and the ground truth, normalised by the ground truth depth.
\item \textbf{Root Mean Squared Error (RMSE)}: Calculates the standard deviation of the residual errors.
\item \textbf{Scale-invariant RMSE (SI-RMSE)}: Computes the RMSE while ignoring the unknown absolute scale and shift between the prediction and ground truth.
\item $\mathbf{\delta}$ at $\mathbf{1.25}$ ($\delta_{1.25}$): Represents the percentage of predicted pixels $p$ that satisfy the condition $\max(\frac{p}{p^{gt}}, \frac{p^{gt}}{p}) < 1.25$, which takes into account close pixel-wise agreement.
\end{enumerate}

The following table (LINK to label) summarises the mean metrics across the MiDaS models tested. 
 
\subsubsection{Segmentation and Salient Object Detection Exploration}

Separating pixels belonging to the foreground object, through a segmentation mask or, as will be detailed below, using a salient object detection (SOD) model, can be another prior. This involves producing a binary mask that separates an object from its background. 

Initially, we explored standard semantic and panoptic segmentation models, such as those found in the Detectron2 \cite{wu2019detectron2} model zoo, and the Segment Anything Model (SAM) \cite{kirillov2023segany}. These models are often used for segmentation, but as illustrated in Figure \ref{fig:seg_issues}, these produced non-contiguous masks that often had sections that included more background pixels. Segmentation models are also limited on their training classes, and despite being tested on categories in this set, their masks were improved on by salient object detection models. 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/SAM_output.png} 
        \caption{SAM Results (3 candidate masks produced per input)}
        \label{fig:sam_example}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/PanopticFPN_output.png}
        \caption{Panoptic Segmentation (Detectron2 Model) Result}
        \label{fig:panoptic_example}
    \end{subfigure}
    
    \caption{Sample outputs from standard segmentation approaches.}
    \label{fig:seg_issues}
\end{figure}

SOD models identify the most visually distinct object in a scene, which allows producing a binary mask that tightly hugs the object boundary. 

To quantitatively evaluate SOD models, we noted that the ShapeNet images used (the same as in the normal and depth priors section) had transparent backgrounds, allowing using the alpha channel to be used as the `ground truth' for the object silhouette. 

We tested three SOD architectures: rembg (based on the U-2-Net architecture) \cite{Gatis_rembg_2025}, InSPyReNet \cite{kim2022inspyrenet}, and BiRefNet \cite{zheng2024birefnet}. 
We evaluated performance using Mean Absolute Error (MAE), Intersection over Union (IoU), and $F_{\beta}$-Measure.


\subsubsection{Prior Model Integration}
TWO MODEL SCRIPTS, PREGENERATED FOR TRAINING/EVAL/VAL
FINAL MODAL ORCHESTRATOR PATTERN
PER MODEL SCRIPT BACTEHD, SAVES TO IMAGE DUE TO SPACE LIMITATIONS
GENERATE PARQUET PER PRIOR
PARQUETS UPLOADED TO HF

ON THE FLY VERSION?
ANOTEHR OCRHSESTOROR, IMAGES ADDED AND FED INTO MODEL TO ALLOW NOVEL IMAGES

\subsubsection{Model Changes}
MODEL CHANGES
added layer parameters
on creation if congfig, inject lora and perform weight graft

\subsubsection{Training and Evaluation Changes}
new priors stacked into input
eval/train on ready dataset have minimal changes
added new dataloader code
novel evaluator?

\subsection{Section 2.4: Data pipelines}
[Explain your data format, how you consume the data in your algorithms, and data augmentation.]

EXPLAIN SRN CARS format
We took our input data from the ShapeNet-SRN dataset from \cite{srn} at $128 \times 128$ resolution.

EXPLAIN TRANSFORMATION TO PARQUET
EXPLAIN STORE OF uint8s
UPLOADED TO HF

TRAINIGN LOADS HF DATASET
LOAD HF PRETRAINED WEIGHTS
PERFORMS WEIGHT GRAFT
TRAINING OCCURS
WEIGHTS FUSED UPLOADED TO HF

\subsection{Section 2.5: Training procedures}
[Explain which framework and optimizers you use, how you implemented the training logic.]

USE TORCH
LIST LORA INSERTTION LIRBARY
LIST BASELINE WEIGHT PARAMS

WE WRITE TRAINIG NNTOEBOOK RUSN ON CLOAB WITH A100
LSIT TRAINING PARAMS
WEIGHTS FUSED AND UPLAODED TO HF

\subsection{Section 2.6: Testing and validation procedures}
[Explain which framework you use, how you implemented the testing/ validation logic.]

\section{Chapter 3: Experiments and Evaluation}
\subsection{Section 3.1: Datasets}
[Explain the datasets utilized: what they contain, why they are utilized, assumptions, limitations, possible extensions.]

The standard benchmark for evaluating single-view 3D reconstruction is ShapeNet-SRN \cite{srn}, hence we used this to test and evaluate our main model implementation. For this dataset, we specifically use the "Car" class, which used the "car" class of ShapeNet v2 \cite{ShapeNet} with 2.5k 3D CAD model instances. The SRN dataset was generated by disabling transparencies and specularities and training on 50 observations of each instance at a resolution of $128 \times 128$ pixels, with camera poses being randomly generated on a sphere with the object at the origin. A limitation of this dataset is the lack of subject variety in the dataset as the model may end up overfitting to cars. A possible extension to address this limitation could be to include other classes in the ShapeNet-SRN database to make sure that the model can still generalise to other types of objects.

An extension of this dataset is implemented in \cite{xu2019disn}, which presents a Deep Implicit Surface Network to generate a 3D mesh from a 2D image by predicting the underlying signed distance fields. In the paper, they generated a 2D dataset composed of renderings of the models in ShapeNet Core \cite{ShapeNet}. For each mesh model, the dataset provides 366 renderings with samller variation and 36 views with larger variation (bigger yaw angle range and larger distance variation). The object is allowed to move away from the origin, which provides more degrees of freedom in terms of camera parameters, and the "roll" angle of the camera is ignored since it was deemed very rare in real-world scenarios. The images were rendered at a higher resolution of $224 \times 224$ pixels and were paired with a depth image, a normal map and an albedo image as shown in figure \ref{fig:ground_truth}. This dataset was mainly used as a ground truth to evaluate the generation of geometry priors (e.g. normal map and depth map). A limitation of this dataset would be its small size since only 72 examples are present, and a possible extension could be to include more examples by implementing similar techniques on other objects in the ShapeNet core dataset.

\subsection{Section 3.2: Training and testing results}
[Explain the training and testing results with graphs and elaborating on why they make sense, what could be improved.]

\subsection{Section 3.3: Qualitative results}
[Show in figures and explain visual results. Include different interesting cases covering different aspects/ limitations/ dataset diversity. If not converged, explain what we can expect once converged. Include any other didactic examples here.]

\subsection{[Optional] Section 3.4: Quantitative results}
[A table and associated explanations for quantitative results.]

\subsection{[Optional] Section 3.5: Comparison to state-of-the-art}
[Qualitative and/ or quantitative comparisons to one or more recent works, especially the baseline work.]

\section{Chapter 4: Conclusions and Future Directions}
\subsection{Section 4.1: Conclusions}
[Summarize what the project was about and the main conclusions.]

\subsection{Section 4.2: Discussion of limitations}
[Explain the limitations of your technique. You may want to refer to previous sections or show figures on the limitations.]

halluciantion in hidden areas still a problem
data, lot of data and compute needed

\subsection{Section 4.3: Future directions}
[State a few future directions for research and development. These typically follow from the discussion on limitations.]

\subsection{Section 4.4: Project Contribution}
"You may find the template for the project report here. We do not enforce any page limits but please make sure to address each section appropriately as explained in the document. In particular, please pay special attention to clarifying the contribution of each group member."
Should we clarify here or throughout document?

\bibliographystyle{unsrt}
\bibliography{citations}

\end{document}