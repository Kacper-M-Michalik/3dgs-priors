\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{multirow} 
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage[section]{placeins}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumitem}

\titleformat{\section}
  {\normalfont\Large\bfseries}
  {}
  {0pt}
  {}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1, 1, 1}
\definecolor{moonstoneblue}{rgb}{0.45, 0.66, 0.76}

\lstdefinestyle{markdown-style}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,    
    breakatwhitespace=false,         
    breaklines=true,                  
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,    
    frame=single,
    rulecolor=\color{moonstoneblue},
    framerule=1.4pt,            
    captionpos=b,  
    aboveskip=1.5em,
    belowskip=1.5em
}

\lstset{
    style=markdown-style
}

\title
{
    Single-Image 3DGS Scene Reconstruction with Geometry-Aware Priors
}
\author
{
    Machine Visual Perception Course Project Report
}

\begin{document}
\maketitle

\section*{Information}
Authors: Kacper Michalik, Radhika Iyer, Alex Loh
\newline
Group Number: 7
\newline
Supervisor: Ahmet Canberk Baykal

\section{Chapter 1: Introduction and Motivation}
\subsection{Introduction to the problem}

\noindent
The advancement of 3D data acquisition, reconstruction, and rendering methods remains a fundamental and persistent open problem in computer vision.
Efficient, high-quality 3D reconstruction is increasingly critical for applications ranging from augmented reality (AR/VR), autonomous devices 
(for example in perception or navigation in robotics or self-driving cars) to digital artistry 
(geometry acquisition for models in VFX, games or other digital products), driving significant research interest in this domain.

\noindent
Historically 3D reconstruction has been a challenging task that requires large number of reference images and even larger amounts of compute.
Advancements in computer hardware and machine-learning methods have significantly improved the efficiency and accuracy 
of 3D reconstruction, extending its applicability for a wider variety of tasks and hardware platforms; continuing this trend, 
one area of focus is made on further reducing the number of input images required whilst maintaining high quality reconstruction, 
thereby improving computational efficiency and reducing data acquisition hardware requirements.
Namely, in 2020 Neural Radiance Fields (NeRF) \cite{nerf} introduced a revolutionary method for 3D reconstruction
capable of high quality novel view synthesis. This is done by learning a continuous volumetric density and radiance function,
typically using a deep learning model, which can then be queried by a ray-march for some new camera pose, enabling novel views to be synthesized.
In 2023, 3D Gaussian Splatting (3DGS) \cite{3dgs} introduced an alternative method, offering major improvements in computational performance.
Instead of an implicit function, 3DGS introduces a new explicit representation, that of a set of 3D Gaussians (called a Gaussian splat), Gaussians are volumes defined by parameters 
such as position, colour, opacity, rotation and scale, which can be efficiently rasterized to generate novel views. Reconstruction is typically achieved by
optimizing the set of Gaussians to produce novel view with minimized loss, alternatively deep learning methods can be used to directly
predict the Gaussian splat. Developments in 3D Gaussian Splatting methods have allowed for 3D scene reconstruction using few or even single RGB images.
While faster than other scene reconstruction techniques and requiring only a "one-shot" pass, these approaches often suffer from challenges
such as layout/scale drift, over-smooth geometry and hallucinations in occluded regions \cite{tang2024hisplathierarchical3dgaussian}.
\newline

\noindent
This project focuses on one recent method, Splatter Image \cite{splatterImage}, as a baseline. 
Splatter Image allows single or few RGB image 3DGS reconstruction. Achieved by predicting 3D Gaussians as pixels in a multichannel image; 
this representation reduces reconstruction to learning an image-to-image neural network, allowing the use of a 2D U-Net to form the representation.
Each pixel stores the parameters for a corresponding 3D Gaussian, allowing for reconstruction in a single feed-forward pass. 
This overall architecture allows for a compute-efficient model. Despite its speed, Splatter Image has some issues that have been noted in related works,
particularly in reconstructing structures unseen in the input view, including for views significantly different from the source. 
We believe there are two reasons for this problem. 
The first is inherent to Splatter Images architecture, unlike methods that utilize explicit 3D feature volumes, Splatter Image's
choice of 3D reconstruction as a 2D-to-2D image translation task limits its ability to learn geometric priors, 
as the model lacks an internal 3D representation to resolve issues like depth ambiguities.
The second is that the 3DGS prediction based on only single or few RGB image features alone does
not have sufficient conditional information for Splatter Image to infer appropriate geometry information or
structures that are not visible in the input view \cite{shen2024pixelworth3dgaussians};
shown in figure \ref{fig:splatter_image_problems}, Splatter Image has trouble generating the occluded chair leg 
in \ref{fig:chair_splatter_1} and \ref{fig:chair_splatter_2}.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_input.png}
        \caption{Input image}
        \label{fig:chair_input}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_truth_1.png}
        \caption{Ground truth}
        \label{fig:chair_truth_1}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_splatter_1.png}
        \caption{Prediction}
        \label{fig:chair_splatter_1}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_truth_2.png}
        \caption{Ground truth}
        \label{fig:chair_truth_2}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_splatter_2.png}
        \caption{Prediction}
        \label{fig:chair_splatter_2}
    \end{subfigure}

    \caption{Splatter Image outputs compared with ground truth taken from \cite{shen2024pixelworth3dgaussians}}
    \label{fig:splatter_image_problems}
\end{figure}

\noindent
This project aims to address these issues, improving reconstruction quality, by first researching inferable geometry priors (such as planes, normals, visibility cues, depth, segmentation or edge maps) 
which can be dynamically produced for input images by existing specialized models, then proposing a lightweight augmentation for Splatter Image, allowing
predicted priors to be fed alongside the RGB images, allowing them to guide reconstruction in a more accurate manner, by providing
necessary additional information and preventing Splatter Image from having to learn how to generate these geometric features itself.

\subsection{Background and related work}
[Include a few very relevant related works and how your work relates to those, expanding on the previous section. We do not expect you to cover all previous works.]
\newline

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/photogrammetry.png}
    \caption{An example of 3D reconstruction with many overlapping images from \cite{photogrammetry}}
    \label{fig:photogrammetry}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.7\linewidth]{images/nerf.pdf}
        \caption{NeRF implicit function from \cite{nerf}}
    \label{fig:nerf}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{images/gaussian_splatting.png}
        \caption{3D Gaussian Splatting Process from \cite{3dgs}}
        \label{fig:3dgs}
    \end{minipage}
\end{figure}


\noindent
Traditionally, 3D reconstruction has been performed by multi-stage photogrammetry pipelines, relying on explicit geometric
representations such as meshes or point clouds. The industry-standard workflow begins with Structure from Motion (SfM), 
which matches sparse feature points across many overlapping images to estimate camera parameters and generate a sparse point cloud.
This is typically followed by Multi-View Stereo (MVS) algorithms to compute dense depth maps, which are combined to
generate a standard 3D mesh using techniques like Delaunay triangulation\cite{delaunayTriangulation} or Moving Least Squares with Marching Cubes\cite{marchingCubes}.
While effective for static, diffuse environments, these methods struggle significantly with surfaces such as 
transparent windows or reflective metals, this as they rely on strict photometric consistency and lack a mechanism to deal with view-dependent radiance.
Additionally, these methods require large numbers of high-resolution images with substantial overlap to achieve high-quality reconstruction.
This heavy data acquisition requirement means the methods create a computational bottleneck, requiring hours of processing time on high-end hardware, and
are impractical in the first place without complex image acquisition setups, such as in Figure \ref{fig:photogrammetry}.

\noindent
A paradigm shift occurred in 2020 with the introduction of Neural Radiance Fields\cite{nerf}. NeRF moves away from explicit geometry representations 
to an implicit volumetric representation. In NeRF an underlying continuous volumetric scene function, represented using a deep learning model, 
is optimized using a set of input images with known camera poses; the input to the function/model
is a single continuous 5D coordinate (spatial location and viewing direction) and the output is the volume density and view-dependent emitted radiance at that location.
Novel views are synthesized by querying 5D coordinates along camera rays for some new camera pose (ray-marching), and output colours and densities are rendered
into an image. NeRF allowed for higher quality 3D reconstruction, eventually using sparser image sets (PixelNeRF, RegNeRF), compared to traditional photogrammetry methods,
achieving state-of-the-art results and becoming the gold standard for novel view synthesis. TODO: CITE

\noindent
In 2023 the field was further revolutionized by 3D Gaussian Splatting, offering a computationally high-performance alternative to NeRF.
3DGS offers a new explicit geometry representation, modelling a scene as a collection of parameterized 3D Gaussians\cite{3dgs}.
Unlike NeRF, 3D Gaussian Splatting does not rely on a neural network to generate a scene. Instead, in the original paper, reconstruction 
is achieved by first initializing a set of Gaussians, either randomly or from a sparse point cloud (typically from Structure from Motion), where
each Gaussian is defined by a set of parameters: position (mean), covariance (scale and rotation), opacity, and colour 
(represented using Spherical Harmonics to allow for view-dependent radiance). Then an optimization process is run
that first adjusts the Gaussians' parameters to minimize the error between rendered images and the ground truth; and secondly performs dynamic management
of the density of Gaussians within the scene, by splitting, cloning or pruning Gaussians in an interleaved manner; namely Gaussians in under-reconstructed areas 
are cloned, Gaussians with high variance are split, and Gaussians in areas of low opacity and of excessive size are pruned.
Finally, for rendering, the Gaussians are rasterized using a custom tile-based rasterizer to produce resulting views, allowing millions of Gaussians rendered in
real-time, allowing for real-time novel view synthesis. 
While the original method relies on per-scene optimization to generate 3D Gaussian Splats, recent works have begun using deep learning methods to 
predict sets of Gaussians directly, with most recent developments allowing for 3D scene reconstruction using few or even single RGB images.
\newline

ADD IAMGES OF F3D GAUS, GAUSS VIDEODREAMER, TGS, SPLATTERIMAGE
\newline

\noindent
Since the introduction of 3DGS, a number of deep learning architectures and processing pipelines based on the method have been developed 
to find the most accurate and efficient implementation capable of producing high quality 3D Gaussian Splats.
Recent examples include ExScene, Wonderland, F3D-Gaus\cite{f3dgaus}, Gauss VideoDreamer\cite{hao2025gaussvideodreamer3dscenegeneration}, TGS and Splatter Image\cite{splatterImage}.

\noindent
F3D-Gaus
TODO

\noindent
Gauss VideoDreamer
TODO

\noindent
The triplane representation was proposed to efficiently and expressively represent 3D volumes \cite{triplane}, as a compromise between rendering speed and memory consumption. 
They were shown to scale to large datasets like Objaverse \cite{objaversexl}\cite{objaverse}, but at the cost of hundreds of GPUs for multiple days \cite{triplanebasedmodel}.
TODO

\noindent
Another recent method is Splatter Image \cite{splatterImage} then applies Gaussian Splatting to monocular reconstruction by using a set of 3D Gaussians as the 3D representation. 
It predicts a 3D Gaussian for each of the input image pixels and uses a 2D image as the container of the 3D Gaussians, storing the parameters of one Gaussian per pixel. 
This reduces the reconstruction problem to learning an image-to-image neural network, allowing the reconstructor to be implemented utilizing only efficient 2D operators. 
The use of Gaussian Splatting in this approach increases rendering and space efficiency, which benefits inference and training. 
Our work continues to expand on this method through investigating different geometry priors and integrating them into the current model as appropriate.
TODO

\subsection{Overview of the idea}
[Provide an overview stating why the idea of the project makes sense and what the main motivation is.]

Splatter Images high computational performance and relative reconstruction quality makes it a highly desirable model for one-shot 
3D reconstruction

the model still occasionally suffers from challenges such as layout/scale drift, over-smooth geometry and poor quality 
hallucinations in occluded regions, particularly in reconstructing structures unseen in the input view, 
including for views significantly different from the source. 

We believe there are two reasons for this problem. The first is inherent to Splatter
Images architecture, unlike methods that utilize explicit 3D feature volumes, Splatter Imageâ€™s choice of
3D reconstruction as a 2D-to-2D image translation task limits its ability to learn geometric priors, as the
model lacks an internal 3D representation to resolve issues like depth ambiguities. The second is that the
3DGS prediction based on only single or few RGB image features alone does not have sufficient conditional
information for Splatter Image to infer appropriate geometry information or structures that are not visible
in the input view.

this is fundamanetally due to the lack of inforamtion the model has from a single input image

Currently there exists specialised, accureate models that can perorm on-shot prediction with hig haccuracy spefific geomtric features of images
trainined on many examples so store lot of konwdleg about corectly halkucinating occluded reegions
in a realsitic/more accuraet manner

We propsoe exploting thse models to gain additional sources of inormation which could be fed into the model
these priros solve both of our beleived problems, namely they provide
provide large amounts of additional information, tackling issue no 2, and stop the model from nedign to learn hwo to infer these geometric
features itself, which it is unsuitable/poor at, tackling issue no 1.
We bleive as such these priors would guide recosntruction in a more accurate manner, impriving the recosntruction quality of the model, whislt
requring minimal architectural changes to Splatter Image, as only have to change inpotu channel count and do minor modifications
if we want to add multimodal data, generally preserving its performance, 

we also propose performing ablation study to see which priors are most effective/significant in changing the reconstruction quality

THJERBY TACKLING

\section{Chapter 2: Method}
\subsection{Baseline algorithm}
[Explain the baseline architecture you used to build your algorithm on. You may reproduce figures from the original papers.]

\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/splatter_image_diagram.png}
        \caption{Overview of SplatterImage\cite{splatterImage}}
    \label{fig:splatter_img_diagram}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/u-net.png}
        \caption{U-net architecture\cite{unet} that Song U-Net\cite{songunet} is based on}
        \label{fig:u-net_diagram}
    \end{minipage}
\end{figure}

\noindent

XXXX FIX
\newline

Splatter Image uses a standard image-to-image neural network architecture to predict a Gaussian for each pixel of the input image I, 
generating the output image M as the Splatter Image. Learning to predict the Splatter Image can be done on a single GPU using at most 
20GB of memory at training time for most single-view reconstruction experiments (except for Objaverse, where 2 GPUs were used and 26GB 
of memory was used on each). Most of this neural network architecture is identical to the SongUNet of \cite{songunet}, but the last layer
is replaced with a $1 \times 1$ convolutional layer with $12 + k_c$ output channels, where $k_c \in \{3, 12\}$ depending on the colour model. 
The output tensor codes for parameters that are then transformed to opacity, offset, depth, scale, rotation and colour respectively. 
These parameters are then activated by non-linear functions to obtain the Gaussian parameters, such as the opacity and depth. 
The Gaussian Splatting implementation of \cite{3dgs} is used for rasterization to generate $360^\circ$ views of the original input image.
TALKA BOUT ARCH MORE, 
Each blue box corresponds to a multi-channel feature map, with the number of channels denoted on top of each box. The x-y size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.
\newline

XXX

\subsection{Algorithm improvements}
[Explain what you implemented to improve over the baseline. You may include figures to explain the idea and logic. Focus on the ideas and not the implementation.]

\subsubsection{Model Improvements}

\noindent
The first modification to Splatter Image's architecture is to allow the model to be initialised with
a dynamic number of input channels, as opposed to the standard 3 for RGB; the required number of channels
is calculated by the \verb|GaussianSplatPredictor| module at runtime, based on the supplied training 
configuration. This allows models to be created that use the desired combination of additional priors.
\newline

\noindent
The second modification is support for multimodal priors. While priors like depth or normal maps, which are images 
themselves, can be appended as additional channels to RGB images to generate the final model input, structurally different priors
cannot simply be appended in the same way.
For example, modern segmentation models, like some of those we researched, are capable of producing 
classifications (and instance IDs in the case of instance or panoptic segmentation) for identified segments.
These classifications are typically in the form of strings or vector embeddings (which can be produced from strings regardless).
These vector embeddings do not have matching dimensions to the RGB images and thus cannot be added as an extra channel 
in the model input. There are a variety of ways these multimodal priors can be provided to the model, one method is broadcasting;
in broadcasting multimodal inputs are appended to an image by replicating them across every pixel.
Since the multimodal input is typically a 1D vector (say a scalar or vector embedding) and the image is a 3D tensor 
(Height $\times$ Width $\times$ Channels), the vector is replicated at every single pixel location by being concatenated along 
the channel dimension. This method is poor due to its computational cost and massive 
data redundancy: the network is forced to process and store
the exact same values millions of times, which wastes GPU memory and computation.
\noindent
Alternatively, Feature-wise Linear Modulation (FiLM) offers a much more efficient method for multimodal data input
for U-Nets and Convolutional Layers. Instead of multimodal data being inserted into the input image,
FiLM injects this data by modulating the intermediate feature maps of the network. FiLM layers are 
inserted into the model at specific points, for example between a convolution and a ReLU activation or at the end of a UNet block, these FiLM layers 
contain a generator, which is a separate, small neural network (like an MLP or RNN) which takes only the multimodal data 
(such as the segmentation embedding, which we will call \textbf{z}) as input, and generates two output terms, $\gamma$ (scale) and $\beta$ (shift). 
The FiLM layer then takes the intercepted feature map and applies the FiLM equation to it:

\begin{equation}
    \hat{F} = \gamma(\mathbf{z}) \cdot F + \beta(\mathbf{z})
\end{equation}

\noindent
This modulated feature map is finally passed on in the network. The method is highly computationally efficient, 
only requiring a multiplication and addition be performed to terms in the feature map, and allows the models image input to remain
unchanged. As such this strategy of inserting FiLM layers into Splatter Image is how we achieve multimodal data support.
\newline

\noindent
An alternative to FiLM is also considered, namely cross-attention. 
FIXXX
\newline
Cross-attention modules can be used as slot-in replacements 
for FiLM layers inside the U-Net's blocks (in the bottleneck and decoder).
The U-Net's image features act as the Query (Q). The multimodal input embeddings act as the Key (K) 
and Value (V). More expressive than FiLM as it allows for fine-grained spatial conditioning (per-pixel).,
this method is consdiered state-of-the-art for high-performance conditional generation.
FIXXX

\subsubsection{Low-Rank Adaptation (LoRA)}
\label{section:lora}
To address the computational constraints of training the full U-Net architecture, we integrated Low-Rank Adaptation (LoRA) \cite{HuLoRA}
directly into our GaussianPredictor, to instead allow for fine-tuning. While our implementation shares a similar class structure to the official \texttt{loralib} library
\cite{hu2022lora} (utilizing mixins to wrap \texttt{Linear} and \texttt{Conv2d} layers), we manually adapted the forward pass to support 
the specific channel dimensions of the Splatter Image architecture (further details can be found in \textbf{Section 2.3.6: Manual LoRA Integration}).

\noindent
Adapters are small modules placed after the frozen modules we wish to adapt, such as linear and convolutional layers. The adapter accepts the same input dimension as the original layer and produces the same output dimension. This allows the output from the adapter to be summed element-wise with the frozen layer's output.

\noindent
Instead of learning a new large weight matrix for these adapters, the weight update is decomposed into two smaller low-rank matrices. For a weight matrix $W \in \mathbb{R}^{d_{out} \times d_{in}}$, the update is defined as $W + \Delta W$, where $\Delta W$ is factored into:
\begin{equation*}
    A \in \mathbb{R}^{r \times d_{in}} \quad \text{and} \quad B \in \mathbb{R}^{d_{out} \times r}
\end{equation*}
Here, we see that $r \ll \min(d_{in}, d_{out})$, where $r$ is the rank hyperparameter (detailed below). During training, only the parameters of $A$ and $B$ are updated, while the original weights $W$ remain frozen.

\noindent
For \texttt{Conv2d} layers, we treat the kernel $W \in \mathbb{R}^{C_{out} \times C_{in} \times k \times k}$ as a flattened matrix of shape $C_{out} \times (C_{in} \cdot k \cdot k)$. The flattened representation is decomposed into $B$ and $A$, allowing us to apply LoRA to the full spatial kernel. By including the spatial dimensions ($k \times k$) in the decomposition, the adapter can learn spatial feature refinements, rather than being limited to channel-wise linear projections. This approach follows the implementation found in \texttt{loralib} \cite{hu2022lora}.

\noindent
During the forward pass, the input $x$ is processed by both the frozen weights $W$ and the LoRA branch:
\begin{equation}
    h = Wx + \frac{\alpha}{r} BAx
\end{equation}
where $\frac{\alpha}{r}$ is a scaling factor \cite{hu2022lora}. This scaling normalises the updates across different rank choices, reducing the need to re-tune the learning rate when $r$ changes.
We utilise three hyperparameters to control this adaptation:
\begin{itemize}
    \item Rank ($r$): The rank of the low-rank matrices $A$ and $B$. Higher ranks increase the number of parameters and the capacity of the adaptation, but also increase computational cost
    \item Alpha ($\alpha$): A scaling factor applied to the LoRA update during the forward pass. The update is scaled relative to the weights that have been frozen from the pretrained model.
    \item Dropout ($p$): The dropout probability applied to the LoRA layers during training. This randomly disables activations, which aims to prevent overfitting. 

\end{itemize}

\noindent
Following \cite{hu2022lora}, $A$ uses Kaiming uniform intialisation, and $B$ is initialised to zero. This ensures that $\Delta W = 0$ at the start of training, which preserves the  behavior of the pre-trained model initially. 

\subsection{Implementation details}
[Explain how you implemented the improvements. You may include code snippets with the corresponding explanations.]
\newline

\noindent
All code associated with the project can be found in the following repositories:
\newline

\noindent
3DGS-priors (Top level repository for the project): \url{https://github.com/Kacper-M-Michalik/3dgs-priors}
\newline
\noindent
Splatter Image Fork: \url{https://github.com/Kacper-M-Michalik/splatter-image}
\newline

\noindent
Generated datasets and model weights can be found in the following repositories:
\newline

\noindent
Datasets with Predicted Priors: \url{https://huggingface.co/datasets/MVP-Group-Project/srn_cars_priors}
\newline
\noindent
Pretrained Models: \url{https://huggingface.co/MVP-Group-Project/splatter-image-priors}

\subsubsection{Planes and Normal Maps Exploration}

\noindent
We considered providing the model with structural information to be one of the most likely
avenues of improvement. These structural priors were considered in two flavours, in
the form of predicted scene planes and scene surface normal maps.
\newline

\noindent
When researching plane prediction, we reached the conclusion that this flavour would in fact be unlikely to help
guide reconstruction. For example teddy bears (as seen on the CO3D\cite{co3d} Teddybears dataset) have complex, convex shapes as shown in \ref{fig:teddy}, 
lacking dominant planes on their surface, using a planar prior might confuse the network, causing it to 
flatten the bear's features or causing poor quality hallucination; as such we decided against using planes as a prior.

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{images/teddy.png}
    \caption{Example of CO3D Teddybears dataset from \cite{co3d}}
    \label{fig:teddy}
\end{figure}

\noindent
A much more favourable option were normal maps. Normal maps store surface normal data as RGB colour information, showing the 
orientation of a surface on a per-pixel level, we considered this to be an excellent prior as it supports both complex shapes
such as teddy bears, but can equally well describe a planar surface. Hence, we selected this prior as a prime candidate 
that could improve the models' 3D surface reconstruction and help guide accurate hallucination in occluded regions.
We take our ground truth images from a dataset (\url{https://github.com/Xharlie/ShapenetRender_more_variation}) provided by \cite{xu2019disn} which contain higher resolution images of ShapeNet\cite{ShapeNet} models. Each RGB image of a ShapeNet model is paired with its corresponding depth map, normal map and albedo map as shown in figure \ref{fig:ground_truth}. 
We feed these images into the normal map generation models and compare against the ground truth normal maps, using 
Pixel Based Visual Information Fidelity as a metric to evaluate their performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/normal_gnd_truth.png}
    \caption{Example of image with maps used as ground truth taken from \cite{Bae2021}}
    \label{fig:ground_truth}
\end{figure}

\noindent
For normal map generation we used models from \cite{Bae2021} which implements a network which estimates the per-pixel surface normal probability distribution and uses uncertainty-guided sampling to improve the quality of prediction of surface normals. The paper provided code at \url{ https://github.com/baegwangbin/surface_normal_uncertainty} that implemented this method on a network trained on ScanNet \cite{dai2017scannet}, with the ground truth and data split provided by FrameNet \cite{huang2019framenet}, and another trained on NYUv2 \cite{NYUv2}, with the ground truth and data split provided by GeoNet \cite{qi2018geonet} \cite{qi2020geonet++}. Both models take in the original image and dimensions of the image as input and return a corresponding normal map with the same dimensions as the given input dimensions.
We run both pretrained models on the dataset.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/original.png}
        \caption{Original}
        \label{fig:highdefinput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN.png}
        \caption{ScanNet output (224x224)}
        \label{fig:highdefoutput}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/GN.png}
        \caption{NYUv2 output (224x224)}
        \label{fig:gn1}
    \end{subfigure}

    \caption{Comparison of original input and two model outputs}
\end{figure}

\noindent
We then pass in input dimensions larger than the actual ones into the models, such that a normal map larger than the original input is produced. We then resize the image to the original input dimensions.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/gnd_truth.png}
        \caption{Ground truth from \cite{xu2019disn}}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_resized.png}
        \caption{ScanNet output (448x448)}
        \label{fig:highdefoutputresized}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/GN_resized.png}
        \caption{NYUv2 output (448x448)}
        \label{fig:gn2}
    \end{subfigure}

    \caption{Comparison of model outputs when setting input dimensions as 448x448 instead of 224x224 alongside ground truth}
\end{figure}

\noindent
The normal map generated for images when given larger input dimensions seem to have more clearly defined edges and surface contouring as shown in figures \ref{fig:highdefoutputresized} and \ref{fig:gn2} as compared to figures \ref{fig:highdefoutput} and \ref{fig:gn1}. It is also important to note that the ground truth for NYUv2 is only defined for the centre crop of the image and the prediction is therefore not accurate outside the centre. This is shown in figures \ref{fig:gn1} and \ref{fig:gn2} where noise is generated around the borders of the normal maps.

\noindent
To compare our generated normal maps to the ground truth normal maps provided in \cite{xu2019disn}, we first mask out the background of the generated normal maps such that the difference in background colour does not contribute to the evaluation metrics for normal map generation.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/gnd_truth.png}
        \caption{Ground truth from \cite{xu2019disn}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_resized.png}
        \caption{ScanNet output (448x448)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_filtered.png}
        \caption{Output with background masking}
    \end{subfigure}

    \caption{Example of masking out background for model evaluation against ground truth}
\end{figure}

\noindent
We then use Pixel Based Visual Information Fidelity to compare the normal maps generated by the two models to the ground truth. Visual Information Fidelity is a reference image quality metric that quantifies the amount of visual information preserved after image processing \cite{vifIntro} and can be used to measure various image quality attributes such as noise level and sharpness \cite{vifMetrics}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/vif_plot.png}
    \caption{Comparison of VIF between ground truth and different models}
    \label{fig:vifgraph}
\end{figure}

\noindent
From Figure \ref{fig:vifgraph} we see that that normal map generation quality increases when passing in larger input arguments and that the model trained on ScanNet generates normal maps that are closer to the ground truth compared to that trained on NYUv2 on average. Hence, in the final model we decided to use the model trained on ScanNet on the ShapeNet database in \cite{ShapeNet}.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_orig.png}
        \caption{Original image from \cite{ShapeNet} of size 128x128}
        \label{fig:lowdefinput}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_output.png}
        \caption{Output from ScanNet model with 128x128 passed in as input dimensions}
        \label{fig:lowdefoutput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_output_resized.png}
        \caption{Output from ScanNet model with 512x512 passed in as input dimensions}
        \label{fig:lowdefoutputfixed}
    \end{subfigure}

    \caption{Original ShapeNet image and normal map outputs}
\end{figure}

\noindent
Without passing in dimensions larger than the input image into the model, we can see from comparing Figures \ref{fig:highdefinput} and \ref{fig:highdefoutput} to Figures \ref{fig:lowdefinput} and \ref{fig:lowdefoutput} that the quality of the normal map generated decreases as the resolution of the original input image decreases. Hence, we pass in much larger input dimensions (512x512) to generate a normal map of higher quality, as shown in Figure \ref{fig:lowdefoutputfixed}.

\subsubsection{Depth Map Exploration}
Depth maps store the distance of a surface from the camera per-pixel. These distances vary in type, such as metric, which considers the physical distance from the camera to the observed point, and relative (such as those produced by the models below). Monocular depth estimation (MDE) models input just a singular image, and produce a depth map (relative distance). 

\noindent
Produced depth maps were compared against the ``ground truths'' produced by \url{https://github.com/Xharlie/ShapenetRender_more_variation}, as was done in the normal priors exploration. An example of the depth map produced by them is visible in Figure  \ref{fig:ground_truth}. However, it is important to note that these depth map ``ground truths'' were not always perfect, as can be seen in the following example: 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{images/Depth_PoorGT1.png}
    \caption{Example of poor depth ground truth data}
    \label{fig:depth_poor_gt}
\end{figure}

\noindent
This inclined us to take the quantitative results produced by comparing MDE models tested against these ground truths with a pinch of salt. For each produced depth map, the following metrics were used to compare against the ground truths. 
\begin{enumerate}
\item \textbf{Absolute Relative Error}: Measures the average difference between the predicted depth and the ground truth, normalised by the ground truth depth.
\item \textbf{Root Mean Squared Error (RMSE)}: Calculates the standard deviation of the residual errors.
\item \textbf{Scale-invariant RMSE (SI-RMSE)}: Computes the RMSE while ignoring the unknown absolute scale and shift between the prediction and ground truth.
\item $\mathbf{\delta}$ at $\mathbf{1.25}$ ($\delta_{1.25}$): Represents the percentage of predicted pixels $p$ that satisfy the condition $\max(\frac{p}{p^{gt}}, \frac{p^{gt}}{p}) < 1.25$, which takes into account close pixel-wise agreement.
\end{enumerate}

\noindent
The following table summarises the metrics across the MiDaS models tested. 

\begin{table}[H]
\centering
\caption{Comparison of MiDaS models on set of easy and hard images.}
\label{tab:depth_metrics}
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Difficulty} & \textbf{Model} & \textbf{AbsRel} $\downarrow$ & \textbf{RMSE} $\downarrow$ & \textbf{SI-RMSE} $\downarrow$ & \textbf{$\delta < 1.25$} $\uparrow$ \\ \midrule
Easy & DPT\_Hybrid & $0.089 \pm 0.12$ & $20.38 \pm 19.39$ & 0.123 & 0.909 \\
Easy & DPT\_Large & $0.091 \pm 0.12$ & $20.56 \pm 19.74$ & 0.124 & 0.909 \\
Easy & MiDaS\_small & $0.096 \pm 0.13$ & $21.54 \pm 20.44$ & 0.129 & 0.918 \\
\midrule
Hard & DPT\_Hybrid & $0.101 \pm 0.15$ & $19.65 \pm 17.99$ & 0.128 & 0.907 \\
Hard & DPT\_Large & $0.170 \pm 0.41$ & $22.40 \pm 22.12$ & 0.151 & 0.906 \\
Hard & MiDaS\_small & $0.190 \pm 0.45$ & $24.45 \pm 23.56$ & 0.164 & 0.900 \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent
The quantitative degradation of \texttt{DPT\_Large} on the Hard set contradicts visual inspection. This discrepancy can be attributed to the quality of the available Ground Truth (GT) depth maps (as discussed earlier). 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/Depth_PoorGT2Res.png}
    \caption{Depth Maps produced by MiDaS models on image with poor GT depth map.}
    \label{fig:depth_poor_gt_res}
\end{figure}

\noindent
Although \texttt{DPT\_Hybrid} appears to align more closely with the GT depth map, \texttt{DPT\_Large} is what is used in the final depth prior generation (as described in \textbf{2.3.4 Selected Prior Integration}). One reason is that the model produces depth maps with cleaner edges along the object boundaries (unlike \texttt{DPT\_Hybrid}), which can be seen to have closer sections of the object blend into the foreground pixels. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/DepthExampleResult.png}
    \caption{Depth Maps produced by the MiDaS models}
    \label{fig:depth_example}
\end{figure}

\subsubsection{Segmentation and Salient Object Detection Exploration}

Separating pixels belonging to the foreground object, through a segmentation mask or, as will be detailed below, using a salient object detection (SOD) model, can be another prior. This involves producing a binary mask that separates an object from its background. 

\noindent
Initially, we explored standard semantic and panoptic segmentation models, such as those found in the Detectron2 \cite{wu2019detectron2} model zoo, and the Segment Anything Model (SAM) \cite{kirillov2023segany}. These models are often used for segmentation, but as illustrated in Figure \ref{fig:seg_issues}, these produced non-contiguous masks that often had sections that included more background pixels. Segmentation models are also limited on their training classes, and despite being tested on categories in this set, their masks were improved on by salient object detection models. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/SAM_output.png} 
        \caption{SAM Results (3 candidate masks produced per input)}
        \label{fig:sam_example}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/PanopticFPN_output.png}
        \caption{Panoptic Segmentation (Detectron2 Model) Result}
        \label{fig:panoptic_example}
    \end{subfigure}
    
    \caption{Sample outputs from standard segmentation approaches.}
    \label{fig:seg_issues}
\end{figure}

\noindent
SOD models identify the most visually distinct object in a scene, which allows producing a binary mask that tightly hugs the object boundary. 
To quantitatively evaluate SOD models, we noted that the ShapeNet images used (the same as in the normal and depth priors section) had transparent backgrounds, allowing using the alpha channel to be used as the `ground truth' for the object silhouette. 

\noindent
We tested three SOD architectures: \texttt{rembg} (based on the U-2-Net architecture) \cite{Gatis_rembg_2025}, \texttt{InSPyReNet} \cite{kim2022inspyrenet}, and \texttt{BiRefNet} \cite{zheng2024birefnet}. 
We evaluated performance using Mean Absolute Error (MAE), Intersection over Union (IoU), and $F_{\beta}$-Measure.
$F_{\beta}$ is a weighted harmonic mean of precision and recall, defined as:
\begin{equation}
F_{\beta} = \frac{(1 + \beta^2)\,\text{Precision} \cdot \text{Recall}}
                 {\beta^2 \cdot \text{Precision} + \text{Recall}}
\end{equation}

\noindent
It is commonly used is salient object detection to assess the quality of binary masks produced. When $\beta = 1$, precision and recall are equally weighted. Greater values of $\beta$ prioritise recall, while lower ones prioritise precision. We set $\beta^2$ to 0.3, following conventional practice in SOD literature, to emphasise precision over recall \cite{achantaFbeta}, \cite{chengFbeta}, \cite{borjiFbeta}.   When identifying a single salient object, false positives (background pixels incorrectly classified as foreground)  are often considered worse than small false negative sections along the boundary of the object. 
\newline

\noindent
Note that the ShapeNet images used are split into `Easy' and `Hard' as categories, as before. 

\begin{table}[H]
    \centering
    \caption{Comparison of Salient Object Detection models on ShapeNet renders. $\uparrow$ indicates higher is better, $\downarrow$ indicates lower is better.}
    \label{tab:sod_results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Difficulty} & \textbf{Model} & \textbf{IoU} $\uparrow$ & \textbf{$F_{\beta}$} $\uparrow$ & \textbf{MAE} $\downarrow$ \\
        \midrule
        \multirow{3}{*}{Easy} 
         & rembg (U-2-Net) & \textbf{0.986} & 0.991 & \textbf{0.004} \\
         & InSPyReNet      & 0.983 & \textbf{0.996} & \textbf{0.004} \\
         & BiRefNet        & 0.966 & 0.979 & 0.006 \\
        \midrule
        \multirow{3}{*}{Hard} 
         & rembg (U-2-Net) & \textbf{0.980} & 0.988 & \textbf{0.005} \\
         & InSPyReNet      & 0.966 & \textbf{0.991} & 0.006 \\
         & BiRefNet        & 0.952 & 0.973 & 0.007 \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
Figure \ref{fig:fbeta_boxplot} illustrates the distribution of $F_{\beta}$ scores across both ``Easy'' and ``Hard'' datasets. \texttt{InSPyReNet} has the tightest interquartile range, particularly on the Easy set.  \texttt{rembg} demonstrates similar stability but with a slightly broader spread on the `Hard ' dataset. \texttt{BiRefNet} is competitive (note the scale of the y-axis, with all achieving scores greater than 0.95), but comparatively shows a lower median score and higher variance, suggesting it is more sensitive to specific geometries. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/SOD_Fbeta.png}
    \caption{Distribution of $F_{\beta}$ scores for each model across the two difficulty levels.}
    \label{fig:fbeta_boxplot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/SODPrecisionRecall.png}
    \caption{Precision vs. Recall trade-off}
    \label{fig:prec_recall}
\end{figure}

\noindent
The scatter plot in Figure \ref{fig:prec_recall} shows how the models operate with different aims; \texttt{InSPyReNet} (in orange) clusters tightly towards the top of the high-performance region (top right), suggesting a priority of precision (often very close to $1.0$). In practice, this may mean eliminating some background noise, but this may mean missing parts of the object. \texttt{rembg} (blue) also has  high recall and precision, and it could also be used as a robust general-purpose model for segmentation (that does not clip parts of the object of interest). 

\noindent
To identify the most frequent ``winner," we counted the number of images where each model achieved the highest $F_{\beta}$ score. \texttt{InSPyReNet} achieved the highest score on the majority (28 vs \texttt{rembg}'s 8). 

\noindent
However, to contextualise these results, all three models achieved very high scores (with $F_{\beta} > 0.97$ and IOU $> 0.95$), largely as the ShapeNet images have a clear foreground object against a uniform background. This reduces the number of possible cases of background vs foreground confusion, which means the models differ meaningfully here on fine structures at the object boundary. In practice, these can include wing mirrors, antennae, etc. Therefore, the differences in the models come from how well they capture these fine details.

\subsubsection{Selected Prior Integration}
During model selection, basic notebooks were written to operate and evaluate the relevant models. 
Once a model was selected, the corresponding notebook would be refactored into a high-performance 
script designed to process full datasets, producing ready-processed priors for every RGB image within the given dataset. 
Specifically, inference code would be rewritten to ensure it operates on high performance hardware such as the GPU,
and that all operations were executed in a batched manner.

\begin{lstlisting}[language=Python, caption=Batched processing of depths]
with torch.no_grad():
    for batch_imgs, batch_filenames in loader:      
        # Prediction, B C H W format              
        batch_imgs = batch_imgs.to(device)            
        preds = model(batch_imgs)

        # Batched downsize
        preds = torch.nn.functional.interpolate(
            preds.unsqueeze(1),
            size=target_size,
            mode="bicubic",
            align_corners=False
        ).squeeze(1)

        # Batched 0 to 1 normalization
        batch_flat = preds.flatten(start_dim=1)
        min_val, max_val = torch.aminmax(batch_flat, dim=1, keepdim=True)
        min_val = min_val.view(preds.size(0), 1, 1)
        max_val = max_val.view(preds.size(0), 1, 1)
        preds_normalized = (preds - min_val) / (max_val - min_val + 1e-8)                    
        preds_uint8 = preds_normalized.mul(255).byte().cpu().numpy()

        # H W format -> 128 * 128
        for j, file_id in enumerate(batch_filenames):   
            full_batch.append(ProcessedImage(
                uuid=uuid,                             
                file_id=file_id.item(),
                image=preds_uint8[j].tobytes()
            ))
\end{lstlisting}

\noindent
An example of a performant rewrite is that of the depth generation python script. 
As shown in the excerpt, inference operates on batches supplied by a DataLoader, with all other operations also being executed 
batchwise on the device (the GPU in our case); only once the batch is fully processed is it moved back to regular (host) 
memory, and the individual priors extracted with relevant metadata for use/saving.
\noindent
We also note that predicted depths are quantized to 8-bit unsigned integers, all predicted priors are quantized this way;
for example normals are quantized from 3 * 32-bit floats to 3 * 8-bit unsigned integers; this is due to compute and storage limitations.
For example the SRN cars dataset contain 387,956 images, each 128 * 128 pixels, storing only priors such as depths and normals
as 32-bit floats for each image would require: 

\begin{align*}
    \textbf{Depths:} \quad & 387,956 \times 128 \times 128 \times 4 \text{ bytes} \\
    &= 25,425,084,416 \text{ bytes} \approx \mathbf{25.43 \text{ GB}} \\[1em]
    \textbf{Normals:} \quad & 387,956 \times 128 \times 128 \times 3 \times 4 \text{ bytes} \\
    &= 76,275,253,248 \text{ bytes} \approx \mathbf{76.28 \text{ GB}}
\end{align*}

\noindent
This makes storing the dataset with calculated priors impractical, be it in memory or disk, quantization allows us to cut
these requirements down to a quarter of the original size.
\newline

\noindent
Prior generation scripts were succesfully implemented for:
\begin{itemize}
    \item Depth
    \item Surface Normals
    \item Segmentation
\end{itemize}
All prior generation scripts can be found in the \verb|/geometry-priors| folder within the 3DGS-priors repository.
\newline

\noindent
As to improve training and evaluation performance, we chose to generate priors in advance for selected datasets. As such we developed
a pipeline that executes the prior generation scripts and constructs a ready-to-use dataset, alongside a custom Torch Dataset class that can read said dataset.
Implementation details can be found in \textbf{Section 2.4: Data pipelines}.

\subsubsection{Model Changes}

\noindent
The first change to the model was to have the top-level \verb|GaussianSplatPredictor| class
dynamically calculate the number of input channels required based on the training configuration, 
this information was then passed to the underlying Convolutional layers and UNet blocks during initialisation.

\begin{lstlisting}[language=Python, caption=Channel calculation code]
def calc_channels(cfg):    
    # Base RGB channels
    in_channels = 3

    # Older configs may not have relvant options, select() returns None if the option is missing
    if OmegaConf.select(cfg, "data.use_pred_depth") is True:
        in_channels += 1
    if OmegaConf.select(cfg, "data.use_pred_normal") is True:
        in_channels += 3

    return in_channels
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=New model initialisation code]
# Calculate number of input channels        
self.in_channels = calc_channels(cfg)

# Initialise correct model depending on if Gaussian mean offsets are to be calculated
if cfg.model.network_with_offset:
    split_dimensions, scale_inits, bias_inits = self.get_splits_and_inits(True, cfg)
    self.network_with_offset = networkCallBack(cfg, 
                                cfg.model.name,
                                self.in_channels,
                                split_dimensions,
                                scale = scale_inits,
                                bias = bias_inits)
    assert not cfg.model.network_without_offset, "Can only have one network"
if cfg.model.network_without_offset:
    split_dimensions, scale_inits, bias_inits = self.get_splits_and_inits(False, cfg)
    self.network_wo_offset = networkCallBack(cfg, 
                                cfg.model.name,
                                split_dimensions,
                                scale = scale_inits,
                                bias = bias_inits)
    assert not cfg.model.network_with_offset, "Can only have one network"
\end{lstlisting}

added film layer for text input
\newline

\noindent
Due to time and compute constraints, combined with the selected segmentation model not producing classification or instance labels, we did not
implement the cross-attention mechanism for multimodal data input into the model, leaving it as an option for future development.
\newline

\noindent
One of our desired features was to allow a model with new priors to be fine-tuned using existing weights, namely those from the
pretrained Splatter Image models. This was achieved by grafting the old weights of a pretrained model onto a new instance of a GaussianSplatPredictor. This grafting mechanism was implemented externally to the model, as such its implementation details are covered in
\textbf{Section 2.3.7: Training and Evaluation Changes}.

\subsubsection{Manual LoRA Integration}
To adapt Splatter Image for fine-tuning on the ShapeNet-SRN Cars dataset (with depth and normal priors), we added Low-Rank Adaptation (LoRA) \cite{HuLoRA} into the GaussianPredictor module of Splatter Image.
LoRA adds low-rank trainable matrices into frozen, pretrained layers. This preserves the base model, while further fine-tuning it to the new data. 

\noindent
Existing libraries such as Hugging Face's PEFT \cite{peft} provide out-of-the-box LoRA integration, we found them incompatible with the weight-grafting
mechanism required by our SplatterImage training pipeline. Due to compute limitations, we rely on Splatter Image grafting weights from pretrained checkpoints where input channel dimensions 
change (e.g., when adding depth or normal map channels). This leads to shape mismatches during initialisation steps (such as for PEFT's LoRA layers) 
since the pre-trained weights cannot be directly loaded into layers with modified input dimensions. 

\noindent
We added LoRA manually within the \texttt{GaussianPredictor} and \texttt{train\_network} modules, using code from Microsoft's \texttt{loralib} \cite{hu2022lora}.
We modified the underlying \texttt{Linear} and \texttt{Conv2d} layers of the U-Net architecture to include a \texttt{LoRALayer} mixin. This allows the freezing of pre-trained base weights while injecting the LoRA matrices ($A$ and $B$) directly into the forward pass. As mentioned earlier, this preserves the frozen pretrained weights from the original 3-channel RGB model, but is compatible with the new channels. 

\subsubsection{Training and Evaluation Changes}
The existing Splatter Image evaluation script needed minimal changes, thanks to how we integrated our priors into the project.
The training script initially needed minimal changes, but then was split into two versions, one with minimal changes for standard training,
and one with additional changes required for LoRA support. 
\newline

\noindent
For evaluation code, the first change was adding support for loading our pretrained models available on HuggingFace.

\begin{lstlisting}[language=Python, caption=New model loading code]
# Load pretrained model from HuggingFace if no local model specified
if args.experiment_path is None:        
    # Eval run on the our new dataset with priors
    if dataset_name in ["cars_priors"]:
        cfg_path = hf_hub_download(repo_id="MVP-Group-Project/splatter-image-priors", 
                                filename="model-depth-normal/config.yaml")
        model_path = hf_hub_download(repo_id="MVP-Group-Project/splatter-image-priors",
                            filename="model-depth-normal/model_best.pth")
    
    # Eval run on previous Splatter Image datasets
    else:            
        cfg_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", 
                                filename="config_{}.yaml".format(dataset_name))
        if dataset_name in ["gso", "objaverse"]:
            model_name = "latest"
        else:
            model_name = dataset_name
        model_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", 
                            filename="model_{}.pth".format(model_name))
    
else:
    cfg_path = os.path.join(experiment_path, ".hydra", "config.yaml")
    model_path = os.path.join(experiment_path, "model_latest.pth")

# load cfg
training_cfg = OmegaConf.load(cfg_path)   
\end{lstlisting}

\noindent
Input preparation was also changed, namely priors are concatenated (if enabled) as additional channels to the input RGB images,
before being fed into the network. Splatter Image uses the PyTorch DataLoader alongside custom Dataset classes 
to load batches of RGB images to perform inference on. The Dataset classes do not directly return a tensor of images, 
but a dictionary containing relevant batch data:

\begin{lstlisting}[language=Python, caption=Excerpt of srn.py Dataset code]
images_and_camera_poses = {
    "gt_images": self.all_rgbs[example_id][frame_idxs].clone(),
    "world_view_transforms": self.all_world_view_transforms[example_id][frame_idxs],
    "view_to_world_transforms": self.all_view_to_world_transforms[example_id][frame_idxs],
    "full_proj_transforms": self.all_full_proj_transforms[example_id][frame_idxs],
    "camera_centers": self.all_camera_centers[example_id][frame_idxs]
}

images_and_camera_poses = self.make_poses_relative_to_first(images_and_camera_poses)
images_and_camera_poses["source_cv2wT_quat"] = self.get_source_cw2wT(images_and_camera_poses["view_to_world_transforms"])

return images_and_camera_poses    
\end{lstlisting}

\noindent
This allowed our priors to be introduced into the evaluation code with ease, by simply creating a new custom Torch Dataset class
in \verb|srn_priors.py| that provides priors as new key/value pairs in the returned dictionary.
Priors were specifically returned as PyTorch tensors matching the RGB image batch. 
The priors can then be accessed and concatenated with the RGB images in a specific order to generate the 
final input tensor for Splatter Image to perform inference on. Assertions are also performed to verify that priors are 
indeed available for the currently active Dataset.

\begin{lstlisting}[language=Python, caption=Splatter Image input tensor preparation code]
# Concatenate selected priors
input_images = data["gt_images"][:, :model_cfg.data.input_images, ...]
if "use_pred_depth" in model_cfg.data and model_cfg.data.use_pred_depth:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicated maps!"
    input_images = torch.cat([input_images,
                    data["pred_depths"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
if "use_pred_normal" in model_cfg.data and model_cfg.data.use_pred_normal:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicated maps!"
    input_images = torch.cat([input_images,
                    data["pred_normals"][:, :model_cfg.data.input_images, ...]],
                    dim=2)

# Get camera to center depth
if model_cfg.data.origin_distances:
    input_images = torch.cat([input_images,
                    data["origin_distances"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
\end{lstlisting}

\noindent
For the training code, two changes had to be made.
The first change was to model loading, adding support for training using existing HuggingFace model weights 
from the base Splatter Image project.        

\begin{lstlisting}[language=Python, caption=New training setup code]
# Resume from HuggingFace pretrained weights, perform weight graft if now training with additional priors
elif cfg.opt.pretrained_hf:
    category = cfg.data.category
    if category == "cars_priors":
        category = "cars"

    model_name = category
    if cfg.data.category in ["gso", "objaverse"]:
        model_name = "latest"

    cfg_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", filename="config_{}.yaml".format(category))            
    model_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", filename="model_{}.pth".format(model_name))
    old_cfg = OmegaConf.load(cfg_path)   
    assert is_base_model(old_cfg)
    
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)

    # Check if new model uses priors
    if is_base_model(cfg):      
        try:
            gaussian_predictor.load_state_dict(checkpoint["model_state_dict"])
        except RuntimeError:
            gaussian_predictor.load_state_dict(checkpoint["model_state_dict"], strict=False)
    else:                       
        gaussian_predictor = graft_weights_with_channel_expansion(checkpoint["model_state_dict"], gaussian_predictor, old_cfg, cfg)
        print("Grafting performed successfully")
            
    best_PSNR = checkpoint["best_PSNR"] 
    print('Loaded model from a pretrained Huggingface checkpoint')   
    OmegaConf.save(config=cfg, f=os.path.join(vis_dir, "config.yaml"))
\end{lstlisting}

\noindent
The program first downloads the appropriate base splatter image configuration and model files. 
The current training configuration is then checked to see if it uses any priors, with \verb|is_base_model| 
returning true if no priors are to be used. In the case priors have been requested, the new gaussian\_predictor instance
will have mismatched layer dimensions compared to the pretrained Splatter Image model, meaning the pretrained weights
cannot be directly loaded into gaussian\_predictor, in this case weights are copied manually into a state dictionary with
layers of correct dimensions, we refer to this process as grafting, and is achieved using the 
\verb|graft_weights_with_channel_expansion| function in \verb|prior_utils.py|.

\begin{lstlisting}[language=Python, caption=Grafting code]
def graft_weights_with_channel_expansion(old_state_dict, new_model, old_cfg, new_cfg):
    new_state_dict = new_model.state_dict()

    # Iterate over all layers
    for name, new_param in new_state_dict.items():
        if name not in old_state_dict:
            # New LoRA parameters not in base model checkpoint. Skip to avoid KeyError.
            if "lora_" in name:
                continue
            print("Failed to find layer {} in HuggingFace model state_dict".format(name))
            raise Exception("Mismatched source model for graft")

        old_param = old_state_dict[name]

        # Directly copy tensors if matching in size (handles most layers)
        if (new_param.shape == old_param.shape):
            new_state_dict[name] = old_param.clone()
            continue

        # In theory we should only reach here for Conv2D layers, as such only need to handle weights, and these should only have extra channels in shape[1]
        if ('weight' in name):
            # Dimension check for Conv2D weights
            if new_param.dim() == 4 and old_param.dim() == 4:
                assert new_param.shape[0] == old_param.shape[0], "Grafting only supported for adding channels, not changing resolution"
                assert new_param.shape[1] > old_param.shape[1], "Cannot truncate channels during graft, can only add channels"

                new_weights = new_param.clone()
                new_weights[:, :old_param.shape[1], :, :] = old_param
                new_weights[:, old_param.shape[1]:, :, :] = 0.0

                new_state_dict[name] = new_weights
            else:
                 print(f"Warning: Skipping graft for {name} due to dimension mismatch")
        else:
            raise Exception("Failed layer graft")
            
    new_model.load_state_dict(new_state_dict)
    return new_model
\end{lstlisting}

\noindent
Grafting works by iterating over the newly configured model's state dictionary, looking up the equivalent layers in the 
pretrained model's state dictionary. If equivalent layers match in shape, that means no changes have been made and the
tensor from the pretrained model's layer is copied directly into the new state dictionary. If there is a mismatch in shape however, we
create a new tensor matching the new model layer's shape and manually copy the values from the pretrained layer into the 
appropriate lower dimensions of the tensor, then zero-initialize all remaining elements in the tensor. 
By zero-initializing the newly added parameters, the expanded layer remains mathematically equivalent to the original.
This ensures the new model's output is identical to that of the pretrained one, despite the change in architecture.
The newly configured model is reloaded with the newly generated state dictionary and finally the ready model is returned.
This grafting mechanism works despite the behaviour of the newly configured model remaining the same, as back-propagation 
will compute non-zero gradients for the new inputs channels, allowing the model to update the zero-initialized parameters 
and gradually integrate the new channels, making them useful.
\newline

\noindent
The second change to training code was similar to that of the evaluation script, updating input preparation, the resulting updated
code block is identical to that in \textbf{Listing 6}.
\newline

\noindent
For the LoRA-specific training script, within the modified \texttt{GaussianPredictor}, \texttt{requires\_grad=False} is explicitly set for all pre-trained backbone weights (see \texttt{gaussian\_predictor\_lora.py}), leaving only the low-rank matrices $A$ and $B$ as trainable. This ensures that optimizer updates are restricted to the adapter layers, with instances defined as \texttt{LoRALayer}.
\newline

CODE BLOCK

\subsection{Data pipelines}
[Explain your data format, how you consume the data in your algorithms, and data augmentation.]
\newline

DIAGRAM: SRN FILE STRUCTURE -> ORCHESTRATOR -> PARALLEL MODELS -> PARQUET
DIAGRAM: PARQUET -> HUGGINGFACE STRUCTURE -> HUGGINGFACE
\newline

\noindent
For training and evaluation performance reasons, we chose to generate priors in advance for selected datasets. As such we developed a 
pipeline that follows an orchestrator pattern; a central notebook initializes a Virtual Machine (VM) for each prior generation model,
then installs the necessary dependencies into them, defined in the relevant \verb|PriorName_requirements.txt| files located in the \verb|/geometry-priors| folder. 
Once the environment is configured, the orchestrator executes the prior generation scripts to produce a complete, 
ready-to-use dataset.

\begin{lstlisting}[language=Python, caption=VM setup]
# Create venv for each prior model
!python3 -m venv /content/models/depth --without-pip
!python3 -m venv /content/models/normal --without-pip

# Have to manually install pip to correctly build venvs on colab
!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
!/content/models/depth/bin/python3 get-pip.py
!/content/models/normal/bin/python3 get-pip.py

# Verify venv's work
!/content/models/depth/bin/pip --version
!/content/models/normal/bin/pip --version

!ls -l /content/models/
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Package setup]
# Setup models

# Depth
!/content/models/depth/bin/pip install -r /content/3dgs-priors/geometry-priors/depth_requirements.txt

# Normals
!git clone https://github.com/baegwangbin/surface_normal_uncertainty.git /content/3dgs-priors/geometry-priors/surface_normal_uncertainty/
!mkdir /content/3dgs-priors/geometry-priors/surface_normal_uncertainty/checkpoints/
!gdown 1lOgY9sbMRW73qNdJze9bPkM2cmfA8Re- -O /content/3dgs-priors/geometry-priors/surface_normal_uncertainty/checkpoints/scannet.pt
!/content/models/normal/bin/pip install -r /content/3dgs-priors/geometry-priors/normal_requirements.txt
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Dataset and prior generation script execution]
# Launch models to process dataset
!/content/models/normal/bin/python /content/3dgs-priors/geometry-priors/generate_normal.py --in_folder="{in_folder}" --out_folder="{out_folder}" --save_iter=250

!/content/models/depth/bin/python /content/3dgs-priors/geometry-priors/generate_depth.py --in_folder="{in_folder}" --out_folder="{out_folder}" --save_iter=250

!python /content/3dgs-priors/geometry-priors/generate_base.py --in_folder="{in_folder}" --out_folder="{out_folder}" --save_iter=500
\end{lstlisting}

The processign scripts geenerate PARQUET FIELS IN THE SET DESTIATION, WITH EXAMPLE ROWS:
EXAMPLE RGBS
EXAMPLE PSOES
EXAMPLE INTRINS
EXMAPLE DEPTHS
EXAMPE NORMALS

These can now be used however the user wants to, we add an additional processing step to transform aprquet files into
file structure appropriate for uplaod to huggingface, matching the following config:
The parquet files can also be processed into a file structure appropriate for upload to HuggingFace, where the pregenereated datset remains
avaiable. Models use the pregenerated datasets via a custom laoder we implemented, taht slots in into all of the exisitng training/evlaution code.

\begin{lstlisting}[language=Python, caption=srn\_cars\_priors HuggingFace file structure config]
---
configs:
- config_name: srn_cars_intrins
  data_files:
  - split: train
    path: intrins/train/*.parquet
  - split: test
    path: intrins/test/*.parquet
  - split: val
    path: intrins/val/*.parquet

- config_name: srn_cars_poses
  data_files:
  - split: train
    path: poses/train/*.parquet
  - split: test
    path: poses/test/*.parquet
  - split: val
    path: poses/val/*.parquet
  
- config_name: srn_cars_rgbs
  data_files:
  - split: train
    path: rgbs/train/*.parquet
  - split: test
    path: rgbs/test/*.parquet
  - split: val
    path: rgbs/val/*.parquet

- config_name: srn_cars_depths
  data_files:
  - split: train
    path: depths/train/*.parquet
  - split: test
    path: depths/test/*.parquet
  - split: val
    path: depths/val/*.parquet

- config_name: srn_cars_normals
  data_files:
  - split: train
    path: normals/train/*.parquet
  - split: test
    path: normals/test/*.parquet
  - split: val
    path: normals/val/*.parquet
---
\end{lstlisting}

CODE BLOCK FOR HF UPLOAD

A DIAGRAM OF HOW THIS WHOLE PEIPLEINE WORKS IS SEEN IN FIGURES

Take a concretne example, we offer one ready pregenerated dataset on HF:
We took our input data from the ShapeNet-SRN dataset from \cite{srn} at $128 \times 128$ resolution.
transform pipeline standard, the depth, rgb, normal columns
store images as uint8s in C H W format, this is as there is X images, do math, thus this manmy GB of data,
we must un fortunately for compute reasons take this loss of accuracy.

AS STATED EARILR SECTION, WE HAVE CUSTOM DATASET FOR LAODER, THAT DOWNLOADS HF DATASET, AND PROCESSES TO STORE 
AS CORRECT PANDAS DATAFRAME TO RETURN DATA

DURIGN TRAINING WANDB SAVES WEIGHT OCCASNIONALLY
WEIGHTS ARE FUSED AT SAVE
THESE WEIGHTS ARE UPLOADED TO HF WITH THIS NAMING SCHEME:

NAMING CODE BLOCK

UPLAOD CODE BLOCK

\subsection{Training procedures}
[Explain which framework and optimizers you use, how you implemented the training logic.]

THE ENTIRE MODEL IS PROGRAMMED USING TORCH
WE USE PEFT OR WHATEVRE TO ADD LORA Layers

OUR TRAINING IS SET BY CONFIG FROM WADB?

SHWO CONFGI

WHEN MODEL TRAINIGN OCCURS CONFIG OPTIOSN ARE VERFIEID TO BE COMPATIABLE
OPTIONS INCLUIDE USING LORA vs NOT, WHICH PRIORS TO USE, AND WHETEHR YOU WNAT TO START THE MODEL WITH EXISITNG WEIGHTS (if so grafted)
WEIGHTS FUSED AT SAVE

TRAINGING IS RUN USING DEFAULT SPALTTER IAMGE APRAMSTERS, Namely:
LIST OPTIMSIER
OtHER OPTIOSM

THE AVAIABLE MODCEL WEIGHST ON HF WERE TRAINIG NNTOEBOOK RUSN ON CLOAB WITH A100
ITERS: 60K

In order to isolate the parameter-efficient adaptation due to LoRA, we used a dedicated training script (\texttt{train\_network\_lora.py}). While preserving the data pipeline and training loop architecture as the standard framework, it is necessary to enforce strict freezing where the gradients for the U-Net backbone are disabled. Optimisation is therefore restricted solely to the injected low-rank matrices (as described in \textbf{2.2.1 Low-Rank Adaptation (LoRA)}). 

\subsection{Testing and validation procedures}
[Explain which framework you use, how you implemented the testing/ validation logic.]
\newline

\noindent
As discussed in previous sections, our changes often include a variety of runtime assertions to verify 
correctness, an example can be found in our input preparation changes, where assertions are performed to check if
every prior requested for training is available in the training/test/validation datasets requested.
\newline

\begin{lstlisting}[language=Python, caption=Excerpt from Splatter Image input tensor preparation code]
if "use_pred_depth" in model_cfg.data and model_cfg.data.use_pred_depth:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicated maps!"
    input_images = torch.cat([input_images,
                    data["pred_depths"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
if "use_pred_normal" in model_cfg.data and model_cfg.data.use_pred_normal:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicated maps!"
    input_images = torch.cat([input_images,
                    data["pred_normals"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
\end{lstlisting}

\noindent
Notebooks performing unit and integration tests were additionally written. Namely, 3 such notebooks can be found in 
the \verb|testing| folder of the \verb|3DGS-priors| repository.
\newline

\noindent
The \verb|eval_test.ipynb| notebook performs tests to verify if the modified evaluation script 
successfully performs evaluation both on previous datasets (such as ShapeNet cars) and our new dataset
\verb|cars_priors|. It does so by running correctly configured evaluations and checking whether the resulting
\verb|scortes.txt| files exist.
\newline

\noindent
The \verb|graft_test.ipynb| notebook performs tests checking whether the 
\verb|graft_weights_with_channel_expansion| function returns successfully, for a variety of model configurations.
It additionally verifies whether an automatically grafted model's state dictionary exactly matches the state dictionary of a 
manually grafted reference model.
\newline

\noindent
The \verb|downloader_test.ipynb| notebook tests our custom Dataset and ready generated \verb|cars_priors| data.
It does so via the standard method for testing new Dataset implementations; by loading both the previous reference \verb|srn.py|
Dataset and our new \verb|srn_priors.py| Dataset, walking both in an ordered mannered using Torch DataLoaders with shuffling disabled,
and then comparing the resulting batches, which in our case means comparing common dictionary entries, where common key/values entries should be identical.
\newline

\noindent
All tests in these notebooks were passed successfully; each notebook should have visible cell outputs showing this.

\section{Chapter 3: Experiments and Evaluation}
\subsection{Datasets}
[Explain the datasets utilized: what they contain, why they are utilized, assumptions, limitations, possible extensions.]

The standard benchmark for evaluating single-view 3D reconstruction is ShapeNet-SRN \cite{srn}, hence we used this to test and evaluate our main model implementation. For this dataset, we specifically use the "Car" class, which used the "car" class of ShapeNet v2 \cite{ShapeNet} with 2.5k 3D CAD model instances. The SRN dataset was generated by disabling transparencies and specularities and training on 50 observations of each instance at a resolution of $128 \times 128$ pixels, with camera poses being randomly generated on a sphere with the object at the origin. A limitation of this dataset is the lack of subject variety in the dataset as the model may end up overfitting to cars. A possible extension to address this limitation could be to include other classes in the ShapeNet-SRN database to make sure that the model can still generalise to other types of objects.

An extension of this dataset is implemented in \cite{xu2019disn}, which presents a Deep Implicit Surface Network to generate a 3D mesh from a 2D image by predicting the underlying signed distance fields. In the paper, they generated a 2D dataset composed of renderings of the models in ShapeNet Core \cite{ShapeNet}. For each mesh model, the dataset provides 36 renderings with smaller variation and 36 views with larger variation (bigger yaw angle range and larger distance variation). The object is allowed to move away from the origin, which provides more degrees of freedom in terms of camera parameters, and the "roll" angle of the camera is ignored since it was deemed very rare in real-world scenarios. The images were rendered at a higher resolution of $224 \times 224$ pixels and were paired with a depth image, a normal map and an albedo image as shown in figure \ref{fig:ground_truth}. This dataset was mainly used as a ground truth to evaluate the generation of geometry priors (e.g. normal map and depth map). A limitation of this dataset would be its small size since only 72 samples are available for us to use, such that the performance of geometry prior generation may not be evaluated correctly. However, in the same GitHub repository, the script to generate these images from the ShapeNet Core dataset is provided, so a possible extension given more time could be to include more images by running the script on other objects in the ShapeNet Core dataset.

\subsection{Training and testing results}
[Explain the training and testing results with graphs and elaborating on why they make sense, what could be improved.]

\subsection{Qualitative results}
[Show in figures and explain visual results. Include different interesting cases covering different aspects/ limitations/ dataset diversity. If not converged, explain what we can expect once converged. Include any other didactic examples here.]

\subsection{[Optional] Quantitative results}
[A table and associated explanations for quantitative results.]

For novel view synthesis evaluation, PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index) and LPIPS (Learned Perceptual Image Patch Similarity) are standard metrics used to evaluate image quality from different perspectives\cite{zhang2025advancesfeedforward3dreconstruction}. 
PSNR is calculated using the mean squared error and is commonly used to quantify reconstruction quality, such that a high PSNR suggests higher reconstruction quality. Meanwhile, SSIM depends on 3 metrics mimicking the human visual perception system: luminance, contrast and structure, where an SSIM score close to 1 indicates high similarity while a score closer to -1 indicates low similarity. Lastly, unlike pixel-wise metrics like PSNR and SSIM that assume pixel independence, LPIPS measures the perceptual similarity between images by comparing their features extracted from a deep neural network, where a low LPIPS score indicates that the compared images are perceptually similar to humans.

To evaluate whether integrating different geometry priors into the model is effective, we performed a set of experiments testing different combinations of geometry priors. We measured the performance using PSNR, SSIM and LPIPS. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/priors_psnr_distribution.png}
    \caption{PSNR distribution for different combinations of added geometry priors}
    \label{fig:priors_psnr}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/priors_ssim_distribution.png}
    \caption{SSIM distribution for different combinations of added geometry priors}
    \label{fig:priors_ssim}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/priors_lpips_distribution.png}
    \caption{LPIPS distribution for different combinations of added geometry priors}
    \label{fig:priors_lpips}
\end{figure}

Adding combinations of depth and normal priors to augment the model with geometry priors seems to make the model perform more consistently, as shown by the lower variance in all metrics as compared to the baseline in figures \ref{fig:priors_psnr}, \ref{fig:priors_ssim} and \ref{fig:priors_lpips}. All augmented models have a higher mean PSNR and SSIM from figures \ref{fig:priors_psnr} and \ref{fig:priors_ssim} respectively which suggests better image reconstruction quality, with the model adding both depth and normal priors performing the best. However, all augmented models have a higher LPIPS score than the baseline \ref{fig:priors_lpips}, suggesting that the difference between the ground truth and the images generated by the model perceived by humans are larger for augmented models than the baseline. We conclude that integrating these priors may not be beneficial in some cases, for example cases where successful image reconstruction is defined as being similar to the ground truth based on a human's perception.

\subsubsection{LoRA experiment results}
To evaluate whether Low-Rank Adaptation (as described in \textbf{Section 2.3.5: Model Changes}) is effective, we performed a set of finetuning experiments. LoRA behaviour can be adjusted through three hyperparameters: rank, alpha and dropout (as mentioned in section \ref{section:lora}).

We tested multiple configurations by varying these hyperparameters, while keeping all other components. We measured the performance using PSNR, SSIM and LPIPS. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/lora_experiments_psnr_distribution.png}
    \caption{PSNR distribution for different LoRA configurations}
    \label{fig:lora_psnr}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/lora_experiments_ssim_distribution.png}
    \caption{SSIM distribution for different LoRA configurations}
    \label{fig:lora_ssim}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/lora_experiments_lpips_distribution.png}
    \caption{LPIPS distribution for different LoRA configurations}
    \label{fig:lora_lpips}
\end{figure}

From figures \ref{fig:lora_psnr}, \ref{fig:lora_ssim} and \ref{fig:lora_lpips}, we can see that all changed models now perform more consistently, as shown by the lower variance in all metrics as compared to the baseline. The PSNR metrics in figure \ref{fig:lora_psnr} and the SSIM metrics in figure \ref{fig:lora_ssim} also suggest better image reconstruction overall due to having higher mean PSNRs and SSIMs than the baseline, with the configuration $r=4, \alpha=1$ performing the best with the highest mean PSNR and SSIM. This configuration also has a lower mean LPIPS score than the baseline, further supporting our claim that this configuration is best for augmenting the model, while the other 2 configurations have higher mean LPIPS scores.

(significance testing?? idk anymore)

\subsection{[Optional] Comparison to state-of-the-art}
[Qualitative and/ or quantitative comparisons to one or more recent works, especially the baseline work.]

(Placeholder LoRA results comparison with SplatterImage) We can compare our LoRA results with the original Splatter Image paper \cite{splatterImage}. 

\section{Chapter 4: Conclusions and Future Directions}
\subsection{Conclusions}
[Summarize what the project was about and the main conclusions.]

\subsection{Discussion of limitations}
[Explain the limitations of your technique. You may want to refer to previous sections or show figures on the limitations.]

halluciantion in hidden areas still a problem, as at end of the day you dont know what there, only doign best guess basd on alrge datasets
lot of data cnd comptue needed to perofrm accurate reweults
we lacked compute to generate and test more models priors

\subsection{Future directions}
[State a few future directions for research and development. These typically follow from the discussion on limitations.]

generating and testing mroe priors (espoecially more priors aoriented to hidden areas, could bring up planes again)
implementing and comapring cross-attention vs FiLM for multimodal data
trying longer training times
trying alternative datasets
Trying alternatigve metrics for 3D reconstruction, like Chaumfeer distance, for exampel could sample
off bunch of points of SRN cars meshes to genrate point cloud, then treat gaussians as point cloud and do comaprison

\subsection{Project Contributions}

\noindent
\textbf{Report Writing Contributions:}
\newline
Section 1.1: Kacper and Alex
\newline
Section 1.2: Kacper
\newline
Section 1.3: Kacper
\newline
Section 2.1: Kacper and Radhika and Alex
\newline
Section 2.2: Kacper and Radhika
\newline
Section 2.3: Kacper and Radhika and Alex
\newline
Section 2.4: Kacper
\newline
Section 2.5: Kacper and Radhika
\newline
Section 2.6: Kacper
\newline
Section 3.1: Alex
\newline
Section 3.2: Radhika
\newline
Section 3.3: FILL IN
\newline
Section 3.4: Radhika and Alex
\newline
Section 3.5: FILL IN
\newline
Section 4.1: FILL IN
\newline
Section 4.2: FILL IN
\newline
Section 4.3: FILL IN
\newline
Image and citation collection: Kacper and Radhika and Alex
\newline

\noindent
\textbf{Presentation Contributions:}
\newline
Slides: Alex
\newline
Recording: Kacper and Radhika and Alex
\newline

\noindent
\textbf{Technical Contributions:}
\newline
Depths exploration: Radhika
\newline
Segmentation exploration: Radhika
\newline
Normals exploration: Alex
\newline
Planes exploration: Alex
\newline
Splatter Image setup and bugfixes: Radhika and Alex
\newline
Splatter Image architectural modification (Grafting, Channel changes, FiLM, Cross-Attention): Kacper
\newline
Splatter Image LoRA integration: Radhika
\newline
Splatter Image Training Modification: Kacper and Radhika
\newline
Splatter Image Eval Modification: Kacper
\newline
Optimised depth generation: Kacper
\newline
Optimised normals generation: Kacper and Alex
\newline
Optimised segmentation generation: Radhika
\newline
Splatter Image \verb|cars_priors| custom Dataset: Kacper
\newline
HuggingFace dataset data pipeline: Kacper
\newline
Results processing code: Alex
\newline
Graphing code: Alex
\newline
Testing custom Dataset: Kacper
\newline
Testing evaluation: Kacper
\newline
Testing prior configurations: Kacper
\newline
Testing LoRA configurations: Radhika 

\bibliographystyle{unsrt}
\bibliography{citations}

\end{document}