\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{multirow} 
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage[section]{placeins}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumitem}

\titleformat{\section}
  {\normalfont\Large\bfseries}
  {}
  {0pt}
  {}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1, 1, 1}
\definecolor{moonstoneblue}{rgb}{0.45, 0.66, 0.76}

\lstdefinestyle{markdown-style}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,    
    breakatwhitespace=false,         
    breaklines=true,                  
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,    
    frame=single,
    rulecolor=\color{moonstoneblue},
    framerule=1.4pt,            
    captionpos=b,  
    aboveskip=1.5em,
    belowskip=1.5em
}

\lstset{
    style=markdown-style
}

\title
{
    Single-Image 3DGS Scene Reconstruction with Geometry-Aware Priors
}
\author
{
    Machine Visual Perception Course Project Report
}

\begin{document}
\maketitle

\section*{Information}
Authors: Kacper Michalik, Radhika Iyer, Alex Loh
\newline
Group Number: 7
\newline
Supervisor: Ahmet Canberk Baykal

\section{Chapter 1: Introduction and Motivation}
\subsection{Introduction to the problem}

\noindent
The advancement of 3D data acquisition, reconstruction, and rendering methods remains a fundamental and persistent open problem in computer vision.
Efficient, high-quality 3D reconstruction is increasingly critical for applications ranging from augmented reality (AR/VR), autonomous devices 
(for example in perception or navigation in robotics or self-driving cars) to digital artistry 
(geometry acquisition for models in VFX, games or other digital products), driving significant research interest in this domain.

\noindent
Historically 3D reconstruction has been a challenging task that requires large number of reference images and even larger amounts of compute.
Advancements in computer hardware and machine-learning methods have significantly improved the efficiency and accuracy 
of 3D reconstruction, extending its applicability for a wider variety of tasks and hardware platforms; continuing this trend, 
one area of focus is made on further reducing the number of input images required whilst maintaining high quality reconstruction, 
thereby improving computational efficiency and reducing data acquisition hardware requirements.
Namely, in 2020 Neural Radiance Fields (NeRF) \cite{nerf} introduced a cutting-edge method for 3D reconstruction
capable of high quality novel view synthesis. This is done by learning a continuous volumetric density and radiance function,
typically using a deep learning model, which can then be queried by a ray-march for some new camera pose, enabling novel views to be synthesized.
In 2023, 3D Gaussian Splatting (3DGS) \cite{3dgs} introduced an alternative method, offering major improvements in computational performance.
Instead of an implicit function, 3DGS introduces a new explicit representation, that of a set of 3D Gaussians (called a Gaussian splat), Gaussians are volumes defined by parameters 
such as position, colour, opacity, rotation and scale, which can be efficiently rasterized to generate novel views. Reconstruction is typically achieved by
optimizing the set of Gaussians to produce novel views with minimized loss, alternatively deep learning methods can be used to directly
predict the Gaussian splat. Developments in 3D Gaussian Splatting methods have allowed for 3D scene reconstruction using few or even single RGB images.
While faster than other scene reconstruction techniques and requiring only a "one-shot" pass, these approaches often suffer from challenges
such as layout/scale drift, over-smooth geometry and hallucinations in occluded regions \cite{tang2024hisplathierarchical3dgaussian}.
\newline

\noindent
This project focuses on one recent method, Splatter Image \cite{splatterImage}, as a baseline. 
Splatter Image allows single or few RGB image 3DGS reconstruction. Achieved by predicting 3D Gaussians as pixels in a multichannel image; 
this representation reduces reconstruction to learning an image-to-image neural network, allowing the use of a 2D U-Net to form the representation.
Each pixel stores the parameters for a corresponding 3D Gaussian, allowing for reconstruction in a single feed-forward pass. 
This overall architecture allows for a compute-efficient model. Despite its speed, Splatter Image has some issues that have been noted in related works,
particularly in reconstructing structures unseen in the input view, including for views significantly different from the source. 
We believe there are two reasons for this problem. 
The first is inherent to Splatter Image's architecture, unlike methods that utilize explicit 3D feature volumes, Splatter Image's
choice of 3D reconstruction as a 2D-to-2D image translation task limits its ability to learn geometric priors, 
as the model lacks an internal 3D representation to reason about and resolve issues like depth ambiguities.
The second is that the 3DGS prediction based on only single or few RGB image features alone does
not have sufficient conditional information for Splatter Image to infer appropriate geometry information or
structures, especially those that are not fully visible in the input view \cite{shen2024pixelworth3dgaussians};
shown in figure \ref{fig:splatter_image_problems}, Splatter Image has trouble generating the occluded chair leg 
in \ref{fig:chair_splatter_1} and \ref{fig:chair_splatter_2}.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_input.png}
        \caption{Input image}
        \label{fig:chair_input}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_truth_1.png}
        \caption{Ground truth}
        \label{fig:chair_truth_1}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_splatter_1.png}
        \caption{Prediction}
        \label{fig:chair_splatter_1}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_truth_2.png}
        \caption{Ground truth}
        \label{fig:chair_truth_2}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_splatter_2.png}
        \caption{Prediction}
        \label{fig:chair_splatter_2}
    \end{subfigure}

    \caption{Splatter Image outputs compared with ground truth taken from \cite{shen2024pixelworth3dgaussians}}
    \label{fig:splatter_image_problems}
\end{figure}

\noindent
This project aims to address these issues, improving reconstruction quality, by first researching inferable geometry priors (such as planes, normals, visibility cues, depth, segmentation or edge maps) 
which can be dynamically produced for input images by existing specialized models, then proposing a lightweight augmentation for Splatter Image, allowing
predicted priors to be fed alongside the RGB images, allowing them to guide and improve reconstruction, by providing
necessary geometric prior information and preventing Splatter Image from having to learn these geometric features itself.

\subsection{Background and related work}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/photogrammetry.png}
    \caption{An example of 3D reconstruction with many overlapping images from \cite{photogrammetry}}
    \label{fig:photogrammetry}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.7\linewidth]{images/nerf.pdf}
        \caption{NeRF implicit function from \cite{nerf}}
    \label{fig:nerf}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{images/gaussian_splatting.png}
        \caption{3D Gaussian Splatting Process from \cite{3dgs}}
        \label{fig:3dgs}
    \end{minipage}
\end{figure}


\noindent
Traditionally, 3D reconstruction has been performed by multi-stage photogrammetry pipelines, relying on explicit geometric
representations such as meshes or point clouds. The industry-standard workflow begins with Structure from Motion (SfM), 
which matches sparse feature points across many overlapping images to estimate camera parameters and generate a sparse point cloud.
This is typically followed by Multi-View Stereo (MVS) algorithms to compute dense depth maps, which are combined to
generate a standard 3D mesh using techniques like Delaunay triangulation \cite{delaunayTriangulation} or Moving Least Squares with Marching Cubes \cite{marchingCubes}.
While effective for static, diffuse environments, these methods struggle significantly with surfaces such as 
transparent windows or reflective metals, this as they rely on photometric consistency and lack a mechanism to deal with view-dependent radiance.
Additionally, these methods require large numbers of high-resolution images with substantial overlap to achieve high-quality reconstruction.
This heavy data acquisition requirement means these methods create a computational bottleneck, requiring hours of processing time on high-end hardware, and
are impractical in the first place without complex image acquisition setups, such as in Figure \ref{fig:photogrammetry}.

\noindent
A paradigm shift occurred in 2020 with the introduction of Neural Radiance Fields \cite{nerf}. NeRF moves away from explicit geometry representations 
to an implicit volumetric representation. In NeRF an underlying continuous volumetric scene function, represented using a deep learning model, 
is optimized using a set of input images with known camera poses; the input to the function/model
is a single continuous 5D coordinate (spatial location and viewing direction) and the output is the volume density and view-dependent radiance at that location.
Novel views are synthesized by querying 5D coordinates along camera rays for some new camera pose (ray-marching), and output colours are rendered
into an image. NeRF allowed for higher quality 3D reconstruction, eventually using sparser image sets (PixelNeRF, RegNeRF), compared to traditional photogrammetry methods,
achieving state-of-the-art results and becoming the gold standard for novel view synthesis \cite{zhang2025advancesfeedforward3dreconstruction}.

\noindent
In 2023 a further breakthrough in the field was achieved by the introduction of 3D Gaussian Splatting, offering a computationally high-performance alternative to NeRF.
3DGS offers a new explicit geometry representation, modelling a scene as a collection of parameterized 3D Gaussians \cite{3dgs}.
Unlike NeRF, 3D Gaussian Splatting does not rely on a neural network to generate a scene. Instead, in the original paper, reconstruction 
is achieved by first initializing a set of Gaussians, either randomly or from a sparse point cloud (typically from Structure from Motion), where
each Gaussian is defined by a set of parameters: position (mean), covariance (scale and rotation), opacity, and colour 
(represented using Spherical Harmonics to allow for view-dependent radiance). Then an optimization process is run
that first adjusts the Gaussians' parameters to minimize the error between rendered images and the ground truth; and secondly performs dynamic management
of the density of Gaussians within the scene, by splitting, cloning or pruning Gaussians in an interleaved manner; namely Gaussians in under-reconstructed areas 
are cloned, Gaussians with high variance are split, and Gaussians of low opacity and of excessive size are pruned.
Finally, for rendering, the Gaussians are rasterized using a custom tile-based rasterizer to produce resulting views, allowing millions of Gaussians rendered in
real-time, allowing for real-time novel view synthesis. 
While the original method relies on per-scene optimization to generate 3D Gaussian Splats, recent works have begun using deep learning methods to 
predict sets of Gaussians directly, with most recent developments allowing for 3D scene reconstruction using few or even single RGB images.

\noindent
Since the introduction of 3DGS, a number of deep learning architectures and processing pipelines based on the method have been developed 
to find the most accurate and efficient implementation capable of producing high quality 3D Gaussian Splats.
Recent examples include ExScene \cite{exscene}, Wonderland \cite{wonderland}, F3D-Gaus \cite{f3dgaus}, Gauss
VideoDreamer \cite{hao2025gaussvideodreamer3dscenegeneration}, TGS \cite{tgs} and Splatter Image \cite{splatterImage}.

\noindent
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/f3d-gaus.png}
    \caption{F3D-Gaus Framework from \cite{f3dgaus}}
    \label{fig:f3dgaus}
\end{figure}

\noindent
F3D-Gaus was proposed to help pixel-aligned Gaussian Splatting generate plausible novel viewpoints by introducing a cycle-aggregative strategy. As shown in figure \ref{fig:f3dgaus}, given a single RGB image $I_0$ and depth map $D_0$, the model directly feeds them forward to output the pixel-aligned Gaussian Splatting representation $GS_0$. The model then renders the image $\tilde{I_1}$ and depth map $\tilde{D_1}$ for the novel view, and then outputs its corresponding Gaussian Splatting representation $GS_1$. $GS_0$ and $GS_1$ are then aggregated to produce images for supervision. By enforcing this cycle consistency through aggregation, the model naturally learns through this self-supervised approach to extrapolate across views by fusing multiple representations, enhancing 3D coherence during inference. This helps to ensure that 3D representations from novel viewpoints are both aligned with and complementary to those from the original viewpoint. After complementary aggregation, F3D-Gaus then applies a video in-painting model \cite{propainter} to the rendered images and depth maps to correct inconsistencies in geometry and texture caused by large viewpoint shifts, resulting in more reliable and visually consistent outputs. 
While the source code and a pretrained model is available, the model would require 13 days of an A100 GPU to train if any updates were made, so we deemed this unsuitable to work on.

\noindent
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/gauss_video_dreamer.png}
    \caption{GaussVideoDreamer Framework from \cite{hao2025gaussvideodreamer3dscenegeneration}}
    \label{fig:gauss_video_dreamer}
\end{figure}

\noindent
For an input image, GaussVideoDreamer first initializes a coarse video sequence depicting occluded regions of the input image under smooth novel view trajectories and inconsistency-aware Gaussian Splats. After that, at periodic optimization intervals, the model renders all viewpoint images and their corresponding inconsistency prediction masks from the Gaussian Splats. The masks and rendered images then guide a video diffusion model to perform progressive inpainting to refine the video sequence, which then in turn optimizes the Gaussian splatting representation and gradually generates better novel view images. However, the generation quality of GaussVideoDreamer is mainly limited by the video diffusion prior (AnimateDiff \cite{animatediff}), which was adapted from an image diffusion model lacking dedicated video inpainting training. As such, when generating videos it may result in frame inconsistencies in longer sequences, progressive colour shifts and high-frequency detail degradation, which reduces novel view reconstruction quality. 
There was also no publicly available implementation of this method. In addition, since a proprietary evaluation dataset of 20 generated single-view images with corresponding text prompts was used in the paper, it would be difficult to compare it with other implementations.
Therefore, we also decided not to explore this method.

\noindent
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/tgs.png}
    \caption{TGS Framework from \cite{tgs}}
    \label{fig:tgs}
\end{figure}

\noindent
In a triplane representation \cite{triplane}, explicit features of the image are aligned along three axis-aligned orthogonal feature planes. We can then query any 3D position by projecting it onto the three feature planes, retrieving the corresponding feature vector via bilinear interpolation and aggregating the three feature vectors via summation. An additional lightweight decoder network interprets the aggregated 3D features as colour and density. This representation is fast and scales efficiently with resolution by keeping the decoder small and shifting the bulk of the expressive power into the explicit features. TGS \cite{tgs} uses this representation in Triplane-Gaussian, a new hybrid representation. As shown in figure \ref{fig:tgs}, TGS first encodes the input image into a set of latent feature tokens, which are then passed into a point cloud decoder and a triplane decoder. A point cloud and a triplane can be de-tokenized from the output of these decoders. After the point cloud decoder, TGS adapts a point upsampling module to densify the point cloud and utilizes a geometry-aware encoding to project point cloud features into the initial positional embedding of triplane latent. Lastly, 3D Gaussians are decoded by the point cloud, the triplane features and the image features for novel view rendering through Gaussian splatting.
While an official implementation of this is available at \href{https://github.com/VAST-AI-Research/TriplaneGaussian}{https://github.com/VAST-AI-Research/TriplaneGaussian} and a pretrained model is also available, the model is only trained on the Objaverse-LVIS dataset and the training time and compute needed to adapt this implementation also made it unsuitable for our project.

\subsection{Overview of the idea}

\noindent
Splatter Image is a model introduced in late 2023, whose main highlights are its computational efficiency, and 
ability to perform one-shot 3D scene reconstruction, whilst maintaining high quality reconstruction capabilities.
This combination make it a highly desirable model. Nonetheless, the model's occasional problems such as layout/scale drift, over-smooth geometry and poor quality hallucinations in occluded regions,
pose a serious problem in its usage and wider adoption. As stated in the introduction, we believe the cause of these reconstruction failures to be twofold; 
first is inherent to Splatter Image's architecture, the models choice of 3D reconstruction as a 2D-to-2D image translation task limits its ability to learn geometric priors, as the
model lacks an internal 3D representation to reason about said priors. 
Second is our belief that single or few RGB images simply does not have sufficient conditional
information for Splatter Image to infer adequate geometric information about 3D scenes such as object structures, especially those that are not 
fully visible in the input view. Modern machine learning now offers highly specialized models capable of performing high-quality 
monocular prediction of geometric features from RGB images. These models such as MiDaS and Depth Anything, are trained on vast datasets and have 
tailored architectures to reason about relevant 3D information; as such they store much valuable latent geometric knowledge, which is valuable in 
situations exactly like those where Splatter Image struggles, namely when trying to accurately hallucinate occluded regions or trying to understand
complex 3D surfaces and structures. The ready availability of these models allows us to exploit them to gain additional sources of high quality geometric 
information that can be fed into Splatter Image, solving both of our supposed problems simultaneously:
preventing the Splatter Image model from needing to implicitly learn geometric features, 
whilst also providing rich 3D information required for high quality reconstruction.
\newline

\noindent
As such we believe dynamically producing and feeding these priors will effectively help guide Splatter Image in 3D reconstruction, resulting in superior reconstruction quality
and strongly reducing the frequency of reconstruction failures. All achieved whilst preserving Splatter Image's key features
such as its computational efficiency, as it requires minimal architectural changes to accept these new priors in its input; altogether making 
Splatter Image a more competitive one-shot 3D reconstruction model. Furthermore, this project gives us an opportunity to perform an ablation study to 
see which priors are most significant in resulting 3D reconstruction quality, this study may provide useful guidance for other projects;
namely helping researchers/developers decide on model architectures that work with optimal sets of geometric priors; 
or helping those that may be interested in creating similar adaptations/pipelines as us.

\section{Chapter 2: Method}
\subsection{Baseline algorithm}

\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/splatter_image_diagram.png}
        \caption{Overview of SplatterImage\cite{splatterImage}}
    \label{fig:splatter_img_diagram}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/u-net.png}
        \caption{U-net architecture\cite{unet} that Song U-Net\cite{songunet} is based on}
        \label{fig:u-net_diagram}
    \end{minipage}
\end{figure}

\noindent
Splatter Image uses a standard image-to-image neural network architecture to predict a Gaussian for each pixel of the input image, 
generating the resulting Splatter Image as output. For this it uses a U-Net, as these have demonstrated excellent performance in image generation 
\cite{rombach2022highresolutionimagesynthesislatent}, and their ability to capture small image details helps to obtain higher-quality novel view reconstructions. 

\noindent
As shown in figure \ref{fig:u-net_diagram}, a U-Net model architecture consists of a contracting path (left side) and an expansive path (right side). 
The contracting path aims to capture context and follows the typical architecture of a convolutional network, which consists of the repeated application of convolutions, each followed by an activation function and a max pooling operation for downsampling. 
At each downsampling step we increase the number of feature channels. 
Afterwards, each step in the expansive path consists of an upsampling of the feature map followed by a convolution that decreases the number of feature channels, enabling precise localization.

\noindent
For the U-Net used in Splatter Image, learning to predict the Splatter Image can be done on a single GPU using at most 
20GB of memory at training time for most single-view reconstruction experiments (except for Objaverse, where 2 GPUs were used and 26GB 
of memory was used on each). Most of this neural network architecture is identical to the SongUNet used in \cite{songunet}, with the exception that the last layer
is replaced with a $1 \times 1$ convolutional layer with $12 + k_c$ output channels, where $k_c \in \{3, 12\}$ depending on the model configuration. 
The output tensor codes for parameters that are then transformed by non-linear functions to obtain the final Gaussian splat parameters, 
such as opacity, offset, depth, scale, rotation and colour respectively.
The original Gaussian Splatting fast tile-based rasterizer implementation from \cite{3dgs} is used to perform rasterization to generate $360^\circ$ novel views of the 3D reconstruction
defined by the predicted Gaussian splat.

\subsection{Algorithm improvements}
\subsubsection{Model Improvements}

\noindent
The first modification to Splatter Image's architecture is to allow the model to be initialised with
a dynamic number of input channels, as opposed to the standard 3 for RGB; the required number of channels
is calculated by the \verb|GaussianSplatPredictor| module at runtime, based on the supplied training 
configuration. This allows models using the desired combination of additional priors to be instantiated.
\newline

\noindent
The second modification is support for multimodal priors. While priors like depth or normal maps, which are images 
themselves, can be appended as additional channels to RGB images to generate the final model input, structurally different priors
cannot simply be appended in the same way.
For example, modern segmentation models, like some of those we researched, are capable of producing 
classifications (and instance IDs in the case of instance or panoptic segmentation) for identified segments.
These classifications are typically in the form of strings or vector embeddings (which can be produced from strings regardless).
These vector embeddings do not have matching dimensions to the RGB images and thus cannot be added as an extra channel 
in the model input. There are a variety of ways these multimodal priors can be provided to the model, one method is broadcasting;
in broadcasting multimodal inputs are appended to an image by replicating them across every pixel.
Since the multimodal input is typically a 1D vector (say a scalar or vector embedding) and the image is a 3D tensor 
(Height $\times$ Width $\times$ Channels), the vector is replicated at every single pixel location by being concatenated along 
the channel dimension. This method is poor due to its computational cost and massive 
data redundancy: the network is forced to process and store
the exact same values many times, which wastes GPU memory and compute.
\noindent
Alternatively, Feature-wise Linear Modulation (FiLM) offers a significantly more efficient method for multimodal data input
for U-Nets and Convolutional Layers. Instead of multimodal data being inserted into the input image,
FiLM injects this data by modulating the intermediate feature maps of the network. FiLM layers are 
inserted into the model at specific points, for example between a convolution and a ReLU activation or at the end of a UNet block, these FiLM layers 
contain a generator, which is a separate, small neural network (like an MLP or RNN) which takes only the multimodal data 
(such as the segmentation embedding, which we will call \textbf{z}) as input, and generates two output terms, $\gamma$ (scale) and $\beta$ (shift). 
The FiLM layer then takes the intercepted feature map and applies the FiLM equation to it:

\begin{equation}
    \hat{F} = \gamma(\mathbf{z}) \cdot F + \beta(\mathbf{z})
\end{equation}

\noindent
This modulated feature map is finally passed on within the network. The method is highly efficient, 
only requiring a multiplication and addition be performed to terms in the feature map, and allows the models image input to remain
unchanged. As such this strategy of inserting FiLM layers into Splatter Image is how we achieve multimodal data support.

\subsubsection{Low-Rank Adaptation (LoRA)}
\label{sec:lora_parameters}
To address the computational constraints of training the full U-Net architecture, we integrated Low-Rank Adaptation (LoRA) \cite{HuLoRA} directly into our GaussianPredictor, to instead allow for fine-tuning. 

\noindent
Adapters are small modules placed after the frozen modules we wish to adapt, such as linear and convolutional layers. The adapter accepts the same input dimension as the original layer and produces the same output dimension. This allows the output from the adapter to be summed element-wise with the frozen layer's output.

\noindent
Instead of learning a new large weight matrix for these adapters, the weight update is decomposed into two smaller low-rank matrices. For a weight matrix $W \in \mathbb{R}^{d_{out} \times d_{in}}$, the update is defined as $W + \Delta W$, where $\Delta W$ is factored into:
\begin{equation*}
    A \in \mathbb{R}^{r \times d_{in}} \quad \text{and} \quad B \in \mathbb{R}^{d_{out} \times r}
\end{equation*}
Here, we see that $r \ll \min(d_{in}, d_{out})$, where $r$ is the rank hyperparameter (detailed below). During training, only the parameters of $A$ and $B$ are updated, while the original weights $W$ remain frozen.

\noindent
For \texttt{Conv2d} layers, we treat the kernel $W \in \mathbb{R}^{C_{out} \times C_{in} \times k \times k}$ as a flattened matrix of shape $C_{out} \times (C_{in} \cdot k \cdot k)$. Flattening allows the low rank adapter to process spatial neighbourhoods, and the flattened representation is decomposed into $B$ and $A$, allowing us to apply LoRA to the full spatial kernel. By including the spatial dimensions ($k \times k$) in the decomposition, the adapter can learn spatial feature refinements, rather than being limited to channel-wise linear projections. This approach follows the implementation found in \texttt{loralib} \cite{hu2022lora}.

\noindent
During the forward pass, the input $x$ is processed by both the frozen weights $W$ and the LoRA branch:
\begin{equation}
    h = Wx + \frac{\alpha}{r} BAx
\end{equation}
where $\frac{\alpha}{r}$ is a scaling factor \cite{hu2022lora}. This scaling normalises the updates across different rank choices, reducing the need to re-tune the learning rate when $r$ changes.
We utilise three hyperparameters to control this adaptation:
\begin{itemize}
    \item Rank ($r$): The rank of the low-rank matrices $A$ and $B$. Higher ranks increase the number of parameters and the capacity of the adaptation, but also increase computational cost
    \item Alpha ($\alpha$): A scaling factor applied to the LoRA update during the forward pass. The update is scaled relative to the weights that have been frozen from the pretrained model.
    \item Dropout ($p$): The dropout probability applied to the LoRA layers during training. This randomly disables activations, which aims to prevent overfitting. 

\end{itemize}

\noindent
Following \cite{hu2022lora}, $A$ uses Kaiming uniform intialisation, and $B$ is initialised to zero. This ensures that $\Delta W = 0$ at the start of training, which preserves the  behavior of the pre-trained model initially. 

\noindent
During inference, the LoRA weights can be optionally merged into the base weights by computing  $W_{merged} = W + \frac{\alpha}{r}BA$, eliminating the separate branch computation and reducing overhead for efficiency. 

\subsection{Implementation details}
\noindent
All code associated with the project can be found in the following repositories:
\newline

\noindent
3DGS-priors (Top level repository for the project): \url{https://github.com/Kacper-M-Michalik/3dgs-priors}
\newline
\noindent
Splatter Image Fork: \url{https://github.com/Kacper-M-Michalik/splatter-image}
\newline

\noindent
Generated datasets and model weights can be found in the following repositories:
\newline

\noindent
Datasets with Predicted Priors: \url{https://huggingface.co/datasets/MVP-Group-Project/srn_cars_priors}
\newline
\noindent
Pretrained Models: \url{https://huggingface.co/MVP-Group-Project/splatter-image-priors}

\subsubsection{Planes and Normal Maps Exploration}
\label{sec:normal}

\noindent
We considered providing the model with structural information to be one of the most likely
avenues of improvement. These structural priors were considered in two flavours, in
the form of predicted scene planes and scene surface normal maps.
\newline

\noindent
When researching plane prediction, we reached the conclusion that this flavour would in fact be unlikely to help
guide reconstruction. For example teddy bears (as seen on the CO3D\cite{co3d} Teddybears dataset) have complex, convex shapes as shown in \ref{fig:teddy}, 
lacking dominant planes on their surface, using a planar prior might confuse the network, causing it to 
flatten the bear's features or causing poor quality hallucination; as such we decided against using planes as a prior.

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{images/teddy.png}
    \caption{Example of CO3D Teddybears dataset from \cite{co3d}}
    \label{fig:teddy}
\end{figure}

\noindent
A much more favourable option were normal maps. Normal maps store surface normal data as RGB colour information, showing the 
orientation of a surface on a per-pixel level, we considered this to be an excellent prior as it supports both complex shapes
such as teddy bears, but can equally well describe a planar surface. Hence, we selected this prior as a prime candidate 
that could improve the models' 3D surface reconstruction and help guide accurate hallucination in occluded regions.
We take our ground truth images from a dataset (\url{https://github.com/Xharlie/ShapenetRender_more_variation}) provided by \cite{xu2019disn} which contain higher resolution images of ShapeNet\cite{ShapeNet} models. Each RGB image of a ShapeNet model is paired with its corresponding depth map, normal map and albedo map as shown in figure \ref{fig:ground_truth}. 
We feed these images into the normal map generation models and compare against the ground truth normal maps, using 
Pixel Based Visual Information Fidelity as a metric to evaluate their performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/normal_gnd_truth.png}
    \caption{Example of image with maps used as ground truth taken from \cite{Bae2021}}
    \label{fig:ground_truth}
\end{figure}

\noindent
For normal map generation we used models from \cite{Bae2021} which implements a network that estimates the per-pixel surface normal probability distribution and uses uncertainty-guided sampling to improve the quality of prediction of surface normals. The paper provided code at \url{ https://github.com/baegwangbin/surface_normal_uncertainty} that implemented this method on a network trained on ScanNet \cite{dai2017scannet}, with the ground truth and data split provided by FrameNet \cite{huang2019framenet}, and another trained on NYUv2 \cite{NYUv2}, with the ground truth and data split provided by GeoNet \cite{qi2018geonet} \cite{qi2020geonet++}. Both models take in the original image and dimensions of the image as input and return a corresponding normal map with the same dimensions as the given input dimensions.
We run both pretrained models on the dataset.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/original.png}
        \caption{Original}
        \label{fig:highdefinput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN.png}
        \caption{ScanNet output (224x224)}
        \label{fig:highdefoutput}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/GN.png}
        \caption{NYUv2 output (224x224)}
        \label{fig:gn1}
    \end{subfigure}

    \caption{Comparison of original input and two model outputs}
\end{figure}

\noindent
We then pass in input dimensions larger than the actual ones into the models, such that a normal map larger than the original input is produced. We then resize the image to the original input dimensions.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/gnd_truth.png}
        \caption{Ground truth from \cite{xu2019disn}}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_resized.png}
        \caption{ScanNet output (448x448)}
        \label{fig:highdefoutputresized}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/GN_resized.png}
        \caption{NYUv2 output (448x448)}
        \label{fig:gn2}
    \end{subfigure}

    \caption{Comparison of model outputs when setting input dimensions as 448x448 instead of 224x224 alongside ground truth}
\end{figure}

\noindent
The normal maps generated for images given larger input dimensions tend to have more clearly defined edges and surface contouring as shown in figures \ref{fig:highdefoutputresized} and \ref{fig:gn2} as compared to figures \ref{fig:highdefoutput} and \ref{fig:gn1}. It is also important to note that the ground truth for NYUv2 is only defined for the centre crop of the image and the prediction is therefore not accurate outside the centre. This is shown in figures \ref{fig:gn1} and \ref{fig:gn2} where noise is generated around the borders of the normal maps.

\noindent
To compare our generated normal maps to the ground truth normal maps provided in \cite{xu2019disn}, we first mask out the background of the generated normal maps such that the difference in background colour does not contribute to the evaluation metrics for normal map generation.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/gnd_truth.png}
        \caption{Ground truth from \cite{xu2019disn}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_resized.png}
        \caption{ScanNet output (448x448)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_filtered.png}
        \caption{Output with background masking}
    \end{subfigure}

    \caption{Example of masking out background for model evaluation against ground truth}
\end{figure}

\noindent
We then use Pixel Based Visual Information Fidelity to compare the normal maps generated by the two models to the ground truth. Visual Information Fidelity is a reference image quality metric that quantifies the amount of visual information preserved after image processing \cite{vifIntro} and can be used to measure various image quality attributes such as noise level and sharpness \cite{vifMetrics}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/vif_plot.png}
    \caption{Comparison of VIF between ground truth and different models}
    \label{fig:vifgraph}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Comparison of normal map generation models on ShapeNet renders. $\uparrow$ indicates higher is better, $\downarrow$ indicates lower is better. The images are labelled easy or hard.}
    \label{tab:normals_results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Difficulty} & \textbf{Model} & \textbf{Input dimensions} & \textbf{Mean VIF} $\uparrow$  & \textbf{Standard deviation of VIF}$\downarrow$\\
        \midrule
        \multirow{6}{*}{Easy} 
        & \multirow{3}{*}{ScanNet}
         & 224x224 & 0.400 & 0.0826 \\
         & & 448x448 & 0.463 & 0.0802 \\
         & & 896x896 & \textbf{0.541} & 0.119\\
        & \multirow{3}{*}{NYUv2}
         & 224x224 & 0.420 & 0.0736 \\
         & & 448x448  & 0.451 & 0.0784 \\
         & & 896x896  & 0.474 & 0.0901 \\
         \midrule
         \multirow{6}{*}{Hard} 
        & \multirow{3}{*}{ScanNet}
         & 224x224 & 0.408 & 0.0905 \\
         & & 448x448 & 0.466 & 0.0976 \\
         & & 896x896 & \textbf{0.526} & 0.125\\
        & \multirow{3}{*}{NYUv2}
         & 224x224 & 0.406 & 0.0749 \\
         & & 448x448  & 0.444 & 0.0839 \\
         & & 896x896  & 0.472 & 0.0981  \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
From figure \ref{fig:vifgraph} and table \ref{tab:normals_results} we see that normal map generation quality increases with larger input arguments and that the model trained on ScanNet on average generates normal maps that are closer to the ground truth compared to that trained on NYUv2. Hence, in the final model we decided to use the model trained on ScanNet on the ShapeNet database in \cite{ShapeNet}.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_orig.png}
        \caption{Original image from \cite{ShapeNet} of size 128x128}
        \label{fig:lowdefinput}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_output.png}
        \caption{Output from ScanNet model with 128x128 passed in as input dimensions}
        \label{fig:lowdefoutput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_output_resized.png}
        \caption{Output from ScanNet model with 512x512 passed in as input dimensions}
        \label{fig:lowdefoutputfixed}
    \end{subfigure}

    \caption{Original ShapeNet image and normal map outputs}
\end{figure}

\noindent
Without passing in dimensions larger than the input image into the model, we can see from comparing Figures \ref{fig:highdefinput} and \ref{fig:highdefoutput} to Figures \ref{fig:lowdefinput} and \ref{fig:lowdefoutput} that the quality of the resulting normal map decreases as the resolution of the original input image decreases. Hence, we pass in larger input dimensions (512x512) during normal map generation as to achieve higher quality maps, as shown in Figure \ref{fig:lowdefoutputfixed}.

\subsubsection{Depth Map Exploration}
\label{sec:depth}
Depth maps store the distance of a surface from the camera per-pixel. These distances vary in type, such as metric, which considers the physical distance from the camera to the observed point, and relative (such as those produced by the models below). Monocular depth estimation (MDE) models input just a singular image, and produce a depth map (relative distance). 

\noindent
Produced depth maps were compared against the ``ground truths'' within \url{https://github.com/Xharlie/ShapenetRender_more_variation}, provided by  \cite{xu2019disn}, as was done in the normal priors exploration. An example of the depth map produced by them is visible in Figure  \ref{fig:ground_truth}. However, it is important to note that these depth map ``ground truths'' were not always perfect, as can be seen in the following example: 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{images/Depth_PoorGT1.png}
    \caption{Example of poor depth ground truth data}
    \label{fig:depth_poor_gt}
\end{figure}

\noindent
This inclined us to take the quantitative results produced by comparing MDE models tested against these ground truths with a pinch of salt. For each produced depth map, the following metrics were used to compare against the ground truths. 
\begin{enumerate}
\item \textbf{Absolute Relative Error}: Measures the average difference between the predicted depth and the ground truth, normalised by the ground truth depth.
\item \textbf{Root Mean Squared Error (RMSE)}: Calculates the standard deviation of the residual errors.
\item \textbf{Scale-invariant RMSE (SI-RMSE)}: Computes the RMSE while ignoring the unknown absolute scale and shift between the prediction and ground truth.
\item $\mathbf{\delta}$ at $\mathbf{1.25}$ ($\delta_{1.25}$): Represents the percentage of predicted pixels $p$ that satisfy the condition $\max(\frac{p}{p^{gt}}, \frac{p^{gt}}{p}) < 1.25$, which takes into account close pixel-wise agreement.
\end{enumerate}

\noindent
The following table summarises the metrics across the MiDaS models tested. 

\begin{table}[H]
\centering
\caption{Comparison of MiDaS models on set of easy and hard images.}
\label{tab:depth_metrics}
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Difficulty} & \textbf{Model} & \textbf{AbsRel} $\downarrow$ & \textbf{RMSE} $\downarrow$ & \textbf{SI-RMSE} $\downarrow$ & \textbf{$\delta < 1.25$} $\uparrow$ \\ \midrule
Easy & DPT\_Hybrid & $0.089 \pm 0.12$ & $20.38 \pm 19.39$ & 0.123 & 0.909 \\
Easy & DPT\_Large & $0.091 \pm 0.12$ & $20.56 \pm 19.74$ & 0.124 & 0.909 \\
Easy & MiDaS\_small & $0.096 \pm 0.13$ & $21.54 \pm 20.44$ & 0.129 & 0.918 \\
\midrule
Hard & DPT\_Hybrid & $0.101 \pm 0.15$ & $19.65 \pm 17.99$ & 0.128 & 0.907 \\
Hard & DPT\_Large & $0.170 \pm 0.41$ & $22.40 \pm 22.12$ & 0.151 & 0.906 \\
Hard & MiDaS\_small & $0.190 \pm 0.45$ & $24.45 \pm 23.56$ & 0.164 & 0.900 \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent
The quantitative degradation of \texttt{DPT\_Large} on the Hard set contradicts visual inspection. This discrepancy can be attributed to the quality of the available Ground Truth (GT) depth maps (as discussed earlier). 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/Depth_PoorGT2Res.png}
    \caption{Depth Maps produced by MiDaS models on image with poor GT depth map.}
    \label{fig:depth_poor_gt_res}
\end{figure}

\noindent
Although \texttt{DPT\_Hybrid} appears to align more closely with the GT depth map, \texttt{DPT\_Large} is what is used in the final depth prior generation (as described in \textbf{2.3.4 Selected Prior Integration}). One reason is that the model produces depth maps with cleaner edges along the object boundaries (unlike \texttt{DPT\_Hybrid}), which can be seen to have closer sections of the object blend into the foreground pixels. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/DepthExampleResult.png}
    \caption{Depth Maps produced by the MiDaS models}
    \label{fig:depth_example}
\end{figure}


\noindent
\textbf{Exploration of Depth Discontinuities}
\noindent
\\We also explored the utility of explicit depth discontinuity features, or depth edges. These reflect sudden changes in depth values (when the first spatial derivative of the depth function is large) within a depth map. These can serve as a proxy for occlusion boundaries, and we were investigated as they potentially aid in separating foreground objects from background geometry.

\noindent
We looked four classical edge detection operators applied directly to the estimated depth maps:
\begin{enumerate}
\item \textbf{Canny Edge Detector:} Tends to produce sparse, crisp boundaries, but with noisy depth maps, it can form edges that are disjoint. Available within the OpenCV (\texttt{cv2.Canny}) library. 
\item \textbf{Sobel Operator:} Approximates using weighted convolution kernels, acting as a low-pass differentiator. 
\item \textbf{Scharr Operator:} Similarly to Sobel, uses weights, works to improve the Sobel operator to provide better rotational symmetry. 
\item \textbf{Prewitt Operator:} Similar to Sobel, but uses a simpler integer-valued kernel (uniform weighting across rows and columns). 
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{images/edgeoperators_results.png}
\caption{Edge operators applied to estimated depth maps. From left to right: Reference RGB, Estimated Depth, Canny, Sobel, Scharr, and Prewitt outputs.}
\label{fig:depth_edges}
\end{figure}

\noindent
Canny produces binary edges that often fragment under the noise inherent in monocular depth estimation. 

\noindent
While these operators successfully identify major structural outlines, we found that they are at times sensitive to artifacts in the estimated depth maps. Consequently, we determined that explicit edge channels added insufficient benefits compared to other priors, and were excluded. However, while these edge operators were not used as input priors, we discuss their potential application in \textbf{Section \ref{sec:future} Future Directions}. 

\subsubsection{Segmentation and Salient Object Detection Exploration}
\label{sec:segmentation}

Separating pixels belonging to the foreground object, through a segmentation mask or, as will be detailed below, using a salient object detection (SOD) model, can be another prior. This involves producing a binary mask that separates an object from its background. 

\noindent
Initially, we explored standard semantic and panoptic segmentation models, such as those found in the Detectron2 \cite{wu2019detectron2} model zoo, and the Segment Anything Model (SAM) \cite{kirillov2023segany}. These models are often used for segmentation, but as illustrated in Figure \ref{fig:seg_issues}, these produced non-contiguous masks that often had sections that included more background pixels. Segmentation models are also limited on their training classes, and despite being tested on categories in this set, their masks were improved on by salient object detection models. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/SAM_output.png} 
        \caption{SAM Results (3 candidate masks produced per input)}
        \label{fig:sam_example}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/PanopticFPN_output.png}
        \caption{Panoptic Segmentation (Detectron2 Model) Result}
        \label{fig:panoptic_example}
    \end{subfigure}
    
    \caption{Sample outputs from standard segmentation approaches.}
    \label{fig:seg_issues}
\end{figure}

\noindent
SOD models identify the most visually distinct object in a scene, which allows producing a binary mask that tightly hugs the object boundary. 
To quantitatively evaluate SOD models, we noted that the ShapeNet images used (the same as in the normal and depth priors section) had transparent backgrounds, allowing using the alpha channel to be used as the `ground truth' for the object silhouette. 

\noindent
We tested three SOD architectures: \texttt{rembg} (based on the U-2-Net architecture) \cite{Gatis_rembg_2025}, \texttt{InSPyReNet} \cite{kim2022inspyrenet}, and \texttt{BiRefNet} \cite{zheng2024birefnet}. 

\noindent
Examples of the binary masks produced by the architectures, given an input image, are in figure \ref{fig:sod_example}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/SODOutputExample.png}
    \caption{Example of Salient Object Detection produced binary masks}
    \label{fig:sod_example}
\end{figure}

\noindent
We evaluated performance using Mean Absolute Error (MAE), Intersection over Union (IoU), and $F_{\beta}$-Measure.
$F_{\beta}$ is a weighted harmonic mean of precision and recall, defined as:
\begin{equation}
F_{\beta} = \frac{(1 + \beta^2)\,\text{Precision} \cdot \text{Recall}}
                 {\beta^2 \cdot \text{Precision} + \text{Recall}}
\end{equation}

\noindent
It is commonly used is salient object detection to assess the quality of binary masks produced. When $\beta = 1$, precision and recall are equally weighted. Greater values of $\beta$ prioritise recall, while lower ones prioritise precision. We set $\beta^2$ to 0.3, following conventional practice in SOD literature, to emphasise precision over recall \cite{achantaFbeta}, \cite{chengFbeta}, \cite{borjiFbeta}.   When identifying a single salient object, false positives (background pixels incorrectly classified as foreground)  are often considered worse than small false negative sections along the boundary of the object. 
\newline

\noindent
Note that the ShapeNet images used are split into `Easy' and `Hard' as categories, as before. 

\begin{table}[H]
    \centering
    \caption{Comparison of Salient Object Detection models on ShapeNet renders. $\uparrow$ indicates higher is better, $\downarrow$ indicates lower is better.}
    \label{tab:sod_results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Difficulty} & \textbf{Model} & \textbf{IoU} $\uparrow$ & \textbf{$F_{\beta}$} $\uparrow$ & \textbf{MAE} $\downarrow$ \\
        \midrule
        \multirow{3}{*}{Easy} 
         & rembg (U-2-Net) & \textbf{0.986} & 0.991 & \textbf{0.004} \\
         & InSPyReNet      & 0.983 & \textbf{0.996} & \textbf{0.004} \\
         & BiRefNet        & 0.966 & 0.979 & 0.006 \\
        \midrule
        \multirow{3}{*}{Hard} 
         & rembg (U-2-Net) & \textbf{0.980} & 0.988 & \textbf{0.005} \\
         & InSPyReNet      & 0.966 & \textbf{0.991} & 0.006 \\
         & BiRefNet        & 0.952 & 0.973 & 0.007 \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
Figure \ref{fig:fbeta_boxplot} illustrates the distribution of $F_{\beta}$ scores across both ``Easy'' and ``Hard'' datasets. \texttt{InSPyReNet} has the tightest interquartile range, particularly on the Easy set.  \texttt{rembg} demonstrates similar stability but with a slightly broader spread on the `Hard ' dataset. \texttt{BiRefNet} is competitive (note the scale of the y-axis, with all achieving scores greater than 0.95), but comparatively shows a lower median score and higher variance, suggesting it is more sensitive to specific geometries. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/SOD_Fbeta.png}
    \caption{Distribution of $F_{\beta}$ scores for each model across the two difficulty levels.}
    \label{fig:fbeta_boxplot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/SODPrecisionRecall.png}
    \caption{Precision vs. Recall trade-off}
    \label{fig:prec_recall}
\end{figure}

\noindent
The scatter plot in Figure \ref{fig:prec_recall} shows how the models operate with different aims; \texttt{InSPyReNet} (in orange) clusters tightly towards the top of the high-performance region (top right), suggesting a priority of precision (often very close to $1.0$). In practice, this may mean eliminating some background noise, but this may mean missing parts of the object. \texttt{rembg} (blue) also has  high recall and precision, and it could also be used as a robust general-purpose model for segmentation (that does not clip parts of the object of interest). 

\noindent
To identify the most frequent ``winner," we counted the number of images where each model achieved the highest $F_{\beta}$ score. \texttt{InSPyReNet} achieved the highest score on the majority (28 vs \texttt{rembg}'s 8). 

\noindent
However, to contextualise these results, all three models achieved very high scores (with $F_{\beta} > 0.97$ and IOU $> 0.95$), largely as the ShapeNet images have a clear foreground object against a uniform background. This reduces the number of possible cases of background vs foreground confusion, which means the models differ meaningfully here on fine structures at the object boundary. In practice, these can include wing mirrors, antennae, etc. Therefore, the differences in the models come from how well they capture these fine details.

\noindent
However, in the end we decided not to use SOD models to generate geometry priors for Splatter Image. This was in part due to the lack of computational resources and time but was also because we thought that since the binary mask only consisted of 1's and 0's, it may not be as expressive as normal and depth maps, which encode more information.

\subsubsection{Selected Prior Integration}
During model selection, basic notebooks were written to operate and evaluate the relevant models. 
Once a model was selected, the corresponding notebook would be refactored into a high-performance 
script designed to process full datasets, producing ready-processed priors for every RGB image within the given dataset. 
Specifically, inference code would be rewritten to ensure it operates on high performance hardware such as the GPU,
and that all operations were executed in a batched manner.

\begin{lstlisting}[language=Python, caption=Batched processing of depths]
with torch.no_grad():
    for batch_imgs, batch_filenames in loader:      
        # Prediction, B C H W format              
        batch_imgs = batch_imgs.to(device)            
        preds = model(batch_imgs)

        # Batched downsize
        preds = torch.nn.functional.interpolate(
            preds.unsqueeze(1),
            size=target_size,
            mode="bicubic",
            align_corners=False
        ).squeeze(1)

        # Batched 0 to 1 normalization
        batch_flat = preds.flatten(start_dim=1)
        min_val, max_val = torch.aminmax(batch_flat, dim=1, keepdim=True)
        min_val = min_val.view(preds.size(0), 1, 1)
        max_val = max_val.view(preds.size(0), 1, 1)
        preds_normalized = (preds - min_val) / (max_val - min_val + 1e-8)                    
        preds_uint8 = preds_normalized.mul(255).byte().cpu().numpy()

        # H W format -> 128 * 128
        for j, file_id in enumerate(batch_filenames):   
            full_batch.append(ProcessedImage(
                uuid=uuid,                             
                file_id=file_id.item(),
                image=preds_uint8[j].tobytes()
            ))
\end{lstlisting}

\noindent
An example of a performant rewrite is that of the depth generation python script. 
As shown in the excerpt, inference operates on batches supplied by a PyTorch DataLoader, with all other operations also being executed 
batchwise on the device (the GPU in our case); only once the batch is fully processed is it moved back to regular (host) 
memory, and the individual priors extracted with relevant metadata for use/saving.
\noindent
We also note that predicted depths are quantized to 8-bit unsigned integers, all predicted priors are quantized this way;
for example normals are quantized from 3 $\times$ 32-bit floats to 3 $\times$ 8-bit unsigned integers; this is due to memory and storage limitations.
For example the SRN cars dataset contain 387,956 images, each 128 $\times$ 128 pixels, storing only priors such as depths and normals
as 32-bit floats for each image would require: 

\begin{align*}
    \textbf{Depths:} \quad & 387,956 \times 128 \times 128 \times 4 \text{ bytes} \\
    &= 25,425,084,416 \text{ bytes} \approx \mathbf{25.43 \text{ GB}} \\[1em]
    \textbf{Normals:} \quad & 387,956 \times 128 \times 128 \times 3 \times 4 \text{ bytes} \\
    &= 76,275,253,248 \text{ bytes} \approx \mathbf{76.28 \text{ GB}}
\end{align*}

\noindent
This makes storing the dataset with calculated priors impractical, be it in memory or disk, quantization allows us to cut
these requirements down to a quarter of the original size.
\newline

\noindent
Prior generation scripts were succesfully implemented for:
\begin{itemize}
    \item Depth
    \item Surface Normals
    \item Segmentation
\end{itemize}
All prior generation scripts can be found in the \verb|/geometry-priors| folder within the 3DGS-priors repository.
\newline

\noindent
As to improve training and evaluation performance, we chose to generate priors in advance for selected datasets. As such we developed
a pipeline that executes the prior generation scripts and constructs a ready-to-use dataset, alongside a custom Torch Dataset class that can read said dataset.
Implementation details can be found in \textbf{Section \ref{sec:data_pipelines}: Data pipelines}.

\subsubsection{Model Changes}

\noindent
The first change to the model was to have the top-level \verb|GaussianSplatPredictor| class
dynamically calculate the number of input channels required based on the training configuration, 
this information was then passed to the underlying Convolutional layers and UNet blocks during initialisation.

\begin{lstlisting}[language=Python, caption=Channel calculation code]
def calc_channels(cfg):    
    # Base RGB channels
    in_channels = 3

    # Older configs may not have relvant options, select() returns None if the option is missing
    if OmegaConf.select(cfg, "data.use_pred_depth") is True:
        in_channels += 1
    if OmegaConf.select(cfg, "data.use_pred_normal") is True:
        in_channels += 3

    return in_channels
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=New model initialisation code]
# Calculate number of input channels        
self.in_channels = calc_channels(cfg)

# Initialise correct model depending on if Gaussian mean offsets are to be calculated
if cfg.model.network_with_offset:
    split_dimensions, scale_inits, bias_inits = self.get_splits_and_inits(True, cfg)
    self.network_with_offset = networkCallBack(cfg, 
                                cfg.model.name,
                                self.in_channels,
                                split_dimensions,
                                scale = scale_inits,
                                bias = bias_inits)
    assert not cfg.model.network_without_offset, "Can only have one network"
if cfg.model.network_without_offset:
    split_dimensions, scale_inits, bias_inits = self.get_splits_and_inits(False, cfg)
    self.network_wo_offset = networkCallBack(cfg, 
                                cfg.model.name,
                                split_dimensions,
                                scale = scale_inits,
                                bias = bias_inits)
    assert not cfg.model.network_with_offset, "Can only have one network"
\end{lstlisting}

\noindent
The SongUNet that Splatter Image is built on already implements support for FiLM layers in its UNet blocks, 
using 2 FCN layers as the FiLM parameter generator. Splatter Image's top level \verb|GaussianSplatPredictor|
only allows FiLM to be used to inject camera pose embeddings. We provide a new configuration option, \verb|custom_embedding|,
and modify GaussianSplatPredictor's forward pass, allowing a custom embedding to be passed in the forward pass 
that overrides the camera pose embedding (as camera pose embeddings are often not used in the project's
datasets, such as \verb|SRN cars|), allowing the embedding to be used by the model without any architectural changes.

\begin{lstlisting}[language=Python, caption=New GaussianSplatPredictor forward pass]
def forward(self, x, 
            source_cameras_view_to_world, 
            source_cv2wT_quat=None,
            focals_pixels=None,
            activate_output=True,
            custom_emb = None):

    ...

    # Get embedding for modulation
    film_emb = None
    if self.cfg.cam_embd.embedding is not None:
        if "custom_embedding" in self.cfg.data and self.cfg.data.custom_embedding:
            assert custom_emb is not None
            film_emb = custom_emb
        else:    
            # Get camera embedding
            cam_embedding = self.get_camera_embeddings(source_cameras_view_to_world)
            assert self.cfg.cam_embd.method == "film"
            film_emb = cam_embedding.reshape(B*N_views, cam_embedding.shape[2])

    ...
    
    # Runs input through SingleImageSongUNetPredictor
    if self.cfg.model.network_with_offset:
        split_network_outputs = self.network_with_offset(x,
                                                            film_camera_emb=film_emb,
                                                            N_views_xa=N_views_xa
                                                            )
\end{lstlisting}

\noindent
The selected segmentation model did not produce classification or instance labels/tokens, as such we did not
test this FiLM implementation with multimodal inputs, leaving that for future investigation.
\newline

\noindent
One of our desired features was to allow a model with new priors to be fine-tuned using existing weights, namely those from the
pretrained Splatter Image models. This was achieved by grafting the old weights of a pretrained model onto a new instance of a GaussianSplatPredictor. This grafting mechanism was implemented externally to the model, as such its implementation details are covered in
\textbf{Section \ref{sec:training_and_eval_changes}: Training and Evaluation Changes}.

\subsubsection{Manual LoRA Integration}
\label{sec:lora}
To adapt Splatter Image for fine-tuning on the ShapeNet-SRN Cars dataset (with depth and normal priors), we added Low-Rank Adaptation (LoRA) \cite{HuLoRA} into the GaussianPredictor module of Splatter Image.
LoRA adds low-rank trainable matrices into frozen, pretrained layers. This preserves the base model, while further fine-tuning it to the new data. 

\noindent
Existing libraries such as Hugging Face's PEFT \cite{peft} provide out-of-the-box LoRA integration, we found them incompatible with the weight-grafting
mechanism required by our SplatterImage training pipeline. Due to compute limitations, we rely on Splatter Image grafting weights from pretrained checkpoints where input channel dimensions 
change (e.g., when adding depth or normal map channels). This leads to shape mismatches during initialisation steps (such as for PEFT's LoRA layers) 
since the pre-trained weights cannot be directly loaded into layers with modified input dimensions. 

\noindent
We added LoRA manually within the \texttt{GaussianPredictor} and \texttt{train\_network} modules, using code from Microsoft's \texttt{loralib} \cite{hu2022lora}.
We modified the underlying \texttt{Linear} and \texttt{Conv2d} layers of the U-Net architecture to include a \texttt{LoRALayer} mixin. This allows the freezing of pre-trained base weights while injecting the LoRA matrices ($A$ and $B$) directly into the forward pass. As mentioned earlier, this preserves the frozen pretrained weights from the original 3-channel RGB model, but is compatible with the new channels. 

\noindent
The \texttt{LoRALayer} mixin is integrated into both \texttt{Linear} and \texttt{Conv2d} classes 
within the U-Net architecture. Each layer checks during initialization whether \texttt{lora\_rank > 0}, 
and if so, creates the low-rank matrices alongside the frozen base weights:

\begin{lstlisting}[language=Python, caption=LoRA Parameter Creation in Conv2d]
if lora_rank > 0 and kernel > 0 and self.weight is not None and not self.up and not self.down:
    # LoRA A: [r, in * k * k], LoRA B: [out, r]
    self.lora_A = nn.Parameter(self.weight.new_zeros((self.r, in_channels * kernel * kernel)))
    self.lora_B = nn.Parameter(self.weight.new_zeros((out_channels, self.r)))
    self.scaling = float(lora_alpha) / max(1, float(self.r))
    self.weight.requires_grad = False
    nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
    nn.init.zeros_(self.lora_B)
\end{lstlisting}

\noindent
These LoRA parameters are distributed throughout the U-Net via the \texttt{block\_kwargs} dictionary in \texttt{SongUNet.\_\_init\_\_()}. The rank, alpha, and dropout values are passed to each \texttt{UNetBlock}, which then forwards them to its constituent \texttt{Conv2d} and \texttt{Linear} layers. 

\noindent
We have \texttt{merge\_lora\_weights()} and \texttt{unmerge\_lora\_weights()} functions to fuse or separate LoRA adaptations from base weights. During merging, the LoRA delta $\frac{\alpha}{r}BA$ is computed and added directly to the frozen weights, eliminating the two-branch computation overhead during inference. 

\begin{lstlisting}[language=Python, caption=LoRA Weight Merging (from gaussian\_predictor\_lora.py)]
def merge_lora_weights(model: nn.Module):
    for m in model.modules():
        if isinstance(m, (Linear, Conv2d)) and m.has_lora and not getattr(m, "_lora_merged", False):
            with torch.no_grad():
                if isinstance(m, Linear):
                    # delta = B @ A -> shape (out, in)
                    delta = (m.lora_B @ m.lora_A).to(m.weight.dtype) * m.scaling
                    m.weight.data += delta
                    m._lora_merged = True
                else:  # Conv2d
                    delta = (m.lora_B @ m.lora_A).view_as(m.weight) * m.scaling
                    m.weight.data += delta
                    m._lora_merged = True
\end{lstlisting}

\noindent
This merging is standard practice in LoRA implementations \cite{hu2022lora} and is equivalent to the two-branch forward pass, while being more computationally efficient.

\subsubsection{Training and Evaluation Changes}
\label{sec:training_and_eval_changes}
The existing Splatter Image evaluation script needed minimal changes, thanks to how we integrated our priors into the project.
The training script initially needed minimal changes, but then was split into two versions, one with minimal changes for standard training,
and one with additional changes required for LoRA support. 
\newline

\noindent
For evaluation code, the first change was adding support for loading our pretrained models available on HuggingFace.

\begin{lstlisting}[language=Python, caption=New model loading code]
# Load pretrained model from HuggingFace if no local model specified
if args.experiment_path is None:        
    # Eval run on the our new dataset with priors
    if dataset_name in ["cars_priors"]:
        cfg_path = hf_hub_download(repo_id="MVP-Group-Project/splatter-image-priors", 
                                filename="model-depth-normal/config.yaml")
        model_path = hf_hub_download(repo_id="MVP-Group-Project/splatter-image-priors",
                            filename="model-depth-normal/model_best.pth")
    
    # Eval run on previous Splatter Image datasets
    else:            
        cfg_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", 
                                filename="config_{}.yaml".format(dataset_name))
        if dataset_name in ["gso", "objaverse"]:
            model_name = "latest"
        else:
            model_name = dataset_name
        model_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", 
                            filename="model_{}.pth".format(model_name))
    
else:
    cfg_path = os.path.join(experiment_path, ".hydra", "config.yaml")
    model_path = os.path.join(experiment_path, "model_latest.pth")

# load cfg
training_cfg = OmegaConf.load(cfg_path)   
\end{lstlisting}

\noindent
Input preparation was also changed, namely priors are concatenated (if enabled) as additional channels to the input RGB images,
before being fed into the network. Splatter Image uses the PyTorch DataLoader alongside custom Dataset classes 
to load batches of RGB images to perform inference on. The Dataset classes do not directly return a tensor of images, 
but a dictionary containing relevant batch data:

\begin{lstlisting}[language=Python, caption=Excerpt of srn.py Dataset code]
images_and_camera_poses = {
    "gt_images": self.all_rgbs[example_id][frame_idxs].clone(),
    "world_view_transforms": self.all_world_view_transforms[example_id][frame_idxs],
    "view_to_world_transforms": self.all_view_to_world_transforms[example_id][frame_idxs],
    "full_proj_transforms": self.all_full_proj_transforms[example_id][frame_idxs],
    "camera_centers": self.all_camera_centers[example_id][frame_idxs]
}

images_and_camera_poses = self.make_poses_relative_to_first(images_and_camera_poses)
images_and_camera_poses["source_cv2wT_quat"] = self.get_source_cw2wT(images_and_camera_poses["view_to_world_transforms"])

return images_and_camera_poses    
\end{lstlisting}

\noindent
This allowed our priors to be introduced into the evaluation code with ease, by simply creating a new custom Torch Dataset class
in \verb|srn_priors.py| that provides priors as new key/value pairs in the returned dictionary.
Priors were specifically returned as PyTorch tensors matching the RGB image batch. 
The priors can then be accessed and concatenated with the RGB images in a specific order to generate the 
final input tensor for Splatter Image to perform inference on. Assertions are also performed to verify that priors are 
indeed available for the currently active Dataset.

\begin{lstlisting}[language=Python, caption=Splatter Image input tensor preparation code]
# Concatenate selected priors
input_images = data["gt_images"][:, :model_cfg.data.input_images, ...]
if "use_pred_depth" in model_cfg.data and model_cfg.data.use_pred_depth:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicted maps!"
    input_images = torch.cat([input_images,
                    data["pred_depths"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
if "use_pred_normal" in model_cfg.data and model_cfg.data.use_pred_normal:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicted maps!"
    input_images = torch.cat([input_images,
                    data["pred_normals"][:, :model_cfg.data.input_images, ...]],
                    dim=2)

# Get camera to center depth
if model_cfg.data.origin_distances:
    input_images = torch.cat([input_images,
                    data["origin_distances"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
\end{lstlisting}

\noindent
For the training code, two changes had to be made.
The first change was to model loading, adding support for training using existing HuggingFace model weights 
from the base Splatter Image project.        

\begin{lstlisting}[language=Python, caption=New training setup code]
# Resume from HuggingFace pretrained weights, perform weight graft if now training with additional priors
elif cfg.opt.pretrained_hf:
    category = cfg.data.category
    if category == "cars_priors":
        category = "cars"

    model_name = category
    if cfg.data.category in ["gso", "objaverse"]:
        model_name = "latest"

    cfg_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", filename="config_{}.yaml".format(category))            
    model_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", filename="model_{}.pth".format(model_name))
    old_cfg = OmegaConf.load(cfg_path)   
    assert is_base_model(old_cfg)
    
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)

    # Check if new model uses priors
    if is_base_model(cfg):      
        try:
            gaussian_predictor.load_state_dict(checkpoint["model_state_dict"])
        except RuntimeError:
            gaussian_predictor.load_state_dict(checkpoint["model_state_dict"], strict=False)
    else:                       
        gaussian_predictor = graft_weights_with_channel_expansion(checkpoint["model_state_dict"], gaussian_predictor, old_cfg, cfg)
        print("Grafting performed successfully")
            
    best_PSNR = checkpoint["best_PSNR"] 
    print('Loaded model from a pretrained Huggingface checkpoint')   
    OmegaConf.save(config=cfg, f=os.path.join(vis_dir, "config.yaml"))
\end{lstlisting}

\noindent
The program first downloads the appropriate base splatter image configuration and model files. 
The current training configuration is then checked to see if it uses any priors, with \verb|is_base_model| 
returning true if no priors are to be used. In the case priors have been requested, the new gaussian\_predictor instance
will have mismatched layer dimensions compared to the pretrained Splatter Image model, meaning the pretrained weights
cannot be directly loaded into gaussian\_predictor, in this case weights are copied manually into a state dictionary with
layers of correct dimensions, we refer to this process as grafting, and is achieved using the 
\verb|graft_weights_with_channel_expansion| function in \verb|prior_utils.py|.

\begin{lstlisting}[language=Python, caption=Grafting code]
def graft_weights_with_channel_expansion(old_state_dict, new_model, old_cfg, new_cfg):
    new_state_dict = new_model.state_dict()

    # Iterate over all layers
    for name, new_param in new_state_dict.items():
        if name not in old_state_dict:
            # New LoRA parameters not in base model checkpoint. Skip to avoid KeyError.
            if "lora_" in name:
                continue
            print("Failed to find layer {} in HuggingFace model state_dict".format(name))
            raise Exception("Mismatched source model for graft")

        old_param = old_state_dict[name]

        # Directly copy tensors if matching in size (handles most layers)
        if (new_param.shape == old_param.shape):
            new_state_dict[name] = old_param.clone()
            continue

        # In theory we should only reach here for Conv2D layers, as such only need to handle weights, and these should only have extra channels in shape[1]
        if ('weight' in name):
            # Dimension check for Conv2D weights
            if new_param.dim() == 4 and old_param.dim() == 4:
                assert new_param.shape[0] == old_param.shape[0], "Grafting only supported for adding channels, not changing resolution"
                assert new_param.shape[1] > old_param.shape[1], "Cannot truncate channels during graft, can only add channels"

                new_weights = new_param.clone()
                new_weights[:, :old_param.shape[1], :, :] = old_param
                new_weights[:, old_param.shape[1]:, :, :] = 0.0

                new_state_dict[name] = new_weights
            else:
                 print(f"Warning: Skipping graft for {name} due to dimension mismatch")
        else:
            raise Exception("Failed layer graft")
            
    new_model.load_state_dict(new_state_dict)
    return new_model
\end{lstlisting}

\noindent
Grafting works by iterating over the newly configured model's state dictionary, looking up the equivalent layers in the 
pretrained model's state dictionary. If equivalent layers match in shape, that means no changes have been made and the
tensor from the pretrained model's layer is copied directly into the new state dictionary. If there is a mismatch in shape however, we
create a new tensor matching the new model layer's shape and manually copy the values from the pretrained layer into the 
appropriate lower dimensions of the tensor, then zero-initialize all remaining elements in the tensor. 
By zero-initializing the newly added parameters, the expanded layer remains mathematically equivalent to the original.
This ensures the new model's output is identical to that of the pretrained one, despite the change in architecture.
The newly configured model is reloaded with the newly generated state dictionary and finally the ready model is returned.
This grafting mechanism works despite the behaviour of the newly configured model remaining the same, as back-propagation 
will compute non-zero gradients for the new inputs channels, allowing the model to update the zero-initialized parameters 
and gradually integrate the new channels, making them useful.
\newline

\noindent
The second change to training code was similar to that of the evaluation script, updating input preparation, the resulting updated
code block is identical to that in \textbf{Listing 6}.
\newline

\noindent
For the LoRA-specific training script, within the modified \texttt{GaussianPredictor}, \texttt{requires\_grad=False} is explicitly set for all pre-trained backbone weights (see \texttt{gaussian\_predictor\_lora.py}), leaving only the low-rank matrices $A$ and $B$ as trainable. This ensures that optimizer updates are restricted to the adapter layers, with instances defined as \texttt{LoRALayer}.
\newline

\noindent
For the LoRA-specific training script (\texttt{train\_network\_lora.py}), parameter freezing is enforced differently. After model initialization and pretrained weight loading (which may include grafting), only LoRA parameters are set to trainable:
\begin{lstlisting}[language=Python, caption=LoRA Parameter Freezing (from train\_network\_lora.py)]
if cfg.opt.lora_finetune:
    # LoRA finetuning, only finetuning adapters
    for name, p in gaussian_predictor.named_parameters():
        p.requires_grad = ("lora" in name.lower())
else:
    # Full finetune 
    for p in gaussian_predictor.parameters():
        p.requires_grad = True

trainable = [p for p in gaussian_predictor.parameters() if p.requires_grad]
optimizer = torch.optim.Adam(
    trainable,
    lr=cfg.opt.base_lr,
    eps=1e-15,
    betas=cfg.opt.betas
)
\end{lstlisting}

\noindent
This selective freezing ensures the optimizer only updates \texttt{lora\_A} and \texttt{lora\_B} matrices. LoRA configurations resulted in a small percent of the original parameters being trainable (compared with Splatter Image).
The dedicated evaluation script \texttt{eval\_lora.py} handles LoRA weights automatically, with a merging step standard in LoRA evaluation \cite{hu2022lora}. 

\subsection{Data pipelines}
\label{sec:data_pipelines}

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/data_pipeline.png}
    \caption{Data Pipeline Diagram}
    \label{fig:data_pipeline}
\end{figure}

\noindent
For training and evaluation performance reasons, we chose to generate priors in advance for selected datasets. As such we developed a 
pipeline that follows an orchestrator pattern; a central notebook initializes a Virtual Machine (VM) for each prior generation model,
then installs the necessary dependencies into them, defined in the relevant \verb|PriorName_requirements.txt| files located in the \verb|/geometry-priors| folder. 
Once the environment is configured, the orchestrator executes the prior generation scripts to produce a complete, 
ready-to-use dataset.

\begin{lstlisting}[language=Python, caption=VM setup]
# Create venv for each prior model
!python3 -m venv /content/models/depth --without-pip
!python3 -m venv /content/models/normal --without-pip

# Have to manually install pip to correctly build venvs on colab
!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
!/content/models/depth/bin/python3 get-pip.py
!/content/models/normal/bin/python3 get-pip.py

# Verify venv's work
!/content/models/depth/bin/pip --version
!/content/models/normal/bin/pip --version

!ls -l /content/models/
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Package setup]
# Setup models

# Depth
!/content/models/depth/bin/pip install -r /content/3dgs-priors/geometry-priors/depth_requirements.txt

# Normals
!git clone https://github.com/baegwangbin/surface_normal_uncertainty.git /content/3dgs-priors/geometry-priors/surface_normal_uncertainty/
!mkdir /content/3dgs-priors/geometry-priors/surface_normal_uncertainty/checkpoints/
!gdown 1lOgY9sbMRW73qNdJze9bPkM2cmfA8Re- -O /content/3dgs-priors/geometry-priors/surface_normal_uncertainty/checkpoints/scannet.pt
!/content/models/normal/bin/pip install -r /content/3dgs-priors/geometry-priors/normal_requirements.txt
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Dataset and prior generation script execution]
# Launch models to process dataset
!/content/models/normal/bin/python /content/3dgs-priors/geometry-priors/generate_normal.py --in_folder="{in_folder}" --out_folder="{out_folder}" --save_iter=250

!/content/models/depth/bin/python /content/3dgs-priors/geometry-priors/generate_depth.py --in_folder="{in_folder}" --out_folder="{out_folder}" --save_iter=250

!python /content/3dgs-priors/geometry-priors/generate_base.py --in_folder="{in_folder}" --out_folder="{out_folder}" --save_iter=500
\end{lstlisting}

\noindent
The processing scripts generate \verb|parquet| format files containing the dataset information
alongside relevant metadata. Parquet files are chosen as they are the industry standard for storing datasets, as the file format allows 
for efficient per column compression, support for streaming and quick read times. 
Currently, 5 parquet files are generated by \verb|generate_cars_datasets.ipynb|, an example implementation of this pipeline for ShapeNet cars
specifically, their contents are:

\begin{description}
   \item[srn\_cars\_intrins] UUID, Model Intrinsics
   \item[srn\_cars\_poses] UUID, Frame ID, Pose matrix 
   \item[srn\_cars\_rgbs] UUID, Frame ID, RGB Image
   \item[srn\_cars\_depths] UUID, Frame ID, Depth Image
   \item[srn\_cars\_normals] UUID, Frame ID, Normal Image
\end{description}

\noindent
These files provide a ready-to-use dataset. If the dataset is to be uploaded to HuggingFace, additional processing must be performed
to transform the files into a file structure that's recognisable by HF for purposes of correctly indexing splits and subsets.
Specifically, we transform the dataset to be compatible with this style of HuggingFace dataset YAML configuration:

\begin{lstlisting}[language=Python, caption=srn\_cars\_priors HuggingFace file structure config]
---
configs:
- config_name: srn_cars_intrins
  data_files:
  - split: train
    path: intrins/train/*.parquet
  - split: test
    path: intrins/test/*.parquet
  - split: val
    path: intrins/val/*.parquet

- config_name: srn_cars_poses
  data_files:
  - split: train
    path: poses/train/*.parquet
  - split: test
    path: poses/test/*.parquet
  - split: val
    path: poses/val/*.parquet
  
- config_name: srn_cars_rgbs
  data_files:
  - split: train
    path: rgbs/train/*.parquet
  - split: test
    path: rgbs/test/*.parquet
  - split: val
    path: rgbs/val/*.parquet

- config_name: srn_cars_depths
  data_files:
  - split: train
    path: depths/train/*.parquet
  - split: test
    path: depths/test/*.parquet
  - split: val
    path: depths/val/*.parquet

- config_name: srn_cars_normals
  data_files:
  - split: train
    path: normals/train/*.parquet
  - split: test
    path: normals/test/*.parquet
  - split: val
    path: normals/val/*.parquet
---
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=HuggingFace upload preparation code]
# Split generated parquets into correct folder/file structure for HuggingFace upload
%cd /content/
!mkdir upload
%cd upload

datatypes = ["rgbs", "intrins", "poses", "depths", "normals"]
splits = ["train", "test", "val"]

for datatype in datatypes:
    # Make a directory for the current dataset file
    !mkdir {datatype}
    %cd {datatype}
    load_path = os.path.join(out_folder, "srn_cars_" + datatype + ".parquet")
    dataset = load_dataset("parquet", data_files=load_path)['train']
    for split in splits:    
        # Generate split specific directory
        !mkdir {split} 
        save_path = "/content/upload/{}/{}/{}.parquet".format(datatype, split, datatype) 
        filter_split = lambda data: data['split'] == split
        filtered = dataset.filter(filter_split).to_pandas().drop(columns=['split'])
        filtered.to_parquet(save_path)
    %cd ..
\end{lstlisting}

\noindent
A diagram depicting this pipeline can be seen at Figure \ref{fig:data_pipeline}.
\newline

\noindent
We offer one ready dataset computed using this pipeline on HuggingFace at \url{https://huggingface.co/datasets/MVP-Group-Project/srn_cars_priors}.
More details about ShapeNet cars, which the precomputed dataset is based off, can be found in \textbf{Section \ref{sec:datasets}: Datasets}.
\newline

\noindent
Training and evaluation scripts using PyTorch DataLoaders to load datasets into memory for inference. To allow
easy use of our precomputed dataset, we implement a custom PyTorch Dataset class, \verb|srn_priors.py|, which can be plugged into 
DataLoaders without any additional code changes. Our custom Dataset class first downloads the datasets from Huggingface
and then preprocesses for later use. Namely, the datasets are converted from Huggingface dataset objects
to pandas dataframes, where rows are also grouped by model UUID and the groups saved to a dictionary, allowing
efficient model to RGB image lookup. The HuggingFace datasets and redundant pandas dataframes
are deallocated from memory as soon as rows are processed, as to minimize memory usage by the precomputed dataset.

\begin{lstlisting}[language=Python, caption=Excerpt from custom PyTorch Dataset]
# Download ready dataset from HuggingFace
print("Started downloading datasets")
dataset_intrins = load_dataset(
    "MVP-Group-Project/srn_cars_priors",
    name="srn_cars_intrins", 
    split=self.dataset_name
)
dataset_poses = load_dataset(
    "MVP-Group-Project/srn_cars_priors",
    name="srn_cars_poses", 
    split=self.dataset_name
)
dataset_rgbs = load_dataset(
    "MVP-Group-Project/srn_cars_priors",
    name="srn_cars_rgbs", 
    split=self.dataset_name
)
dataset_depths = load_dataset(
    "MVP-Group-Project/srn_cars_priors",
    name="srn_cars_depths", 
    split=self.dataset_name
)
dataset_normals = load_dataset(
    "MVP-Group-Project/srn_cars_priors",
    name="srn_cars_normals", 
    split=self.dataset_name
)
print("Downloaded datasets")

# Convert intrinsics files to a dataframe for performance reasons
pre_dataset_intrins = dataset_intrins.to_pandas()
pre_dataset_intrins.sort_values(by=["uuid"], ascending=[True], inplace=True)

# Calculate dataset length
assert len(dataset_poses) == len(dataset_rgbs)
if cfg.data.subset != -1:
    assert cfg.data.subset > 0
    assert len(pre_dataset_intrins) >= cfg.data.subset
    self.subset_length = cfg.data.subset
else:
    self.subset_length = len(pre_dataset_intrins)

self.dataset_intrins = pre_dataset_intrins.iloc[:self.subset_length]
uuids = set(self.dataset_intrins['uuid'].unique())    

# Convert remaining HF datasets to dataframes indexed by uuid
self.dataset_poses = process_and_chunk(dataset_poses, uuids)
print("Converted poses")
self.dataset_rgbs = process_and_chunk(dataset_rgbs, uuids)
print("Converted rgbs")
self.dataset_depths = process_and_chunk(dataset_depths, uuids)
print("Converted depths")
self.dataset_normals = process_and_chunk(dataset_normals, uuids)    
print("Converted normals")

print("Dataset intrin length: {}".format(self.subset_length))
\end{lstlisting}

\noindent
During and after training, the model is checkpointed and saved to disk. The frequency of mid-training checkpoints is set by the training configuration.
Namely, the files saved are:

\begin{description}
   \item[config.yaml] Training run configuration.
   \item[model\_best.pth] Weights, PSNR and iteration count of the highest scoring model from training run.
   \item[model\_latest.pth] Most recent weights, PSNR and iteration count.
   \item[scores.txt] Scores from evaluation of the model during training.
   \item[train\_network.log] Log of critical events during training run.
\end{description}

\noindent
\verb|training.ipynb| also offers code that can be directly upload these checkpoints to HuggingFace into an appropriately named folder based on the training configuration.
Pretrained models for the precomputed ShapeNet \verb|cars_priors| dataset can be found on HuggingFace at: \url{https://huggingface.co/MVP-Group-Project/splatter-image-priors} 

\begin{lstlisting}[language=Python, caption=HuggingFace automated model upload code]
# Save checkpoint to huggingface

files = []
model_path = Path(model_path)

# If model data is directly in folder, use that, otherwise we assume the wandb file structure
possible_files = [file for file in model_path.glob('*{}'.format(".pth"))]
if len(possible_files) > 0:
    files = [file for file in model_path.iterdir() if file.is_file()]
else:
    # Get top level yyyy-mm-dd folders
    date_folders = sorted([folder for folder in model_path.iterdir() if folder.is_dir()])
    
    # Verify some training run was performed
    if not date_folders:
        print("No date folders found.")
    else:
        latest_date_folder = date_folders[-1]
        print("Latest Date: {}".format(latest_date_folder.name))

        # Get hh-mm-ss folders
        time_folders = sorted([folder for folder in latest_date_folder.iterdir() if folder.is_dir()])        
        if not time_folders:
            print("No time folders found inside {}".format(latest_date_folder.name))
        else:  
            latest_time_folder = time_folders[-1]
            print(f"Latest Time: {latest_time_folder.name}")
            
            # Get files in most recent folder combination
            files = [p for p in latest_time_folder.iterdir() if p.is_file()]

login(token=hf_token)
api = HfApi()

# Upload model
for file_path in files:    
    # Title upload folder correctly
    path_in_repo = "model"
    if use_depth:
        path_in_repo += "-depth"
    if use_normal:
        path_in_repo += "-normal"
    if use_lora:
        path_in_repo += "-finetune"    

    path_in_repo += "/" + os.path.basename(file_path)

    # Upload file
    api.upload_file(
        path_or_fileobj=file_path,
        repo_id=repo_id,
        repo_type="model",
        path_in_repo=path_in_repo,
    )
\end{lstlisting}

\subsection{Training Procedures}

\noindent 
The Splatter Image model in fully implemented using PyTorch. We use the existing training loop implementation.
The \verb|train.py| script implements the training loop using PyTorch DataLoader and Datasets classes to load in dataset entries in a batched manner.
Wandb is used to perform logging, track training runs, configurations and track the model performance. 
Hyrda alongside Omegaconf is used to supply a training configuration to the training loop and model.

\begin{lstlisting}[language=Python, caption=Initialisation excerpt from training code]
@hydra.main(version_base=None, config_path='configs', config_name="default_config")
def main(cfg: DictConfig):
...
\end{lstlisting}


\begin{lstlisting}[language=Python, caption=Default YAML config for training]
---
defaults:
  - wandb: defaults
  - hydra: defaults
  - cam_embd: defaults
  - _self_
general:
  device: 0
  random_seed: 0
  num_devices: 1
  mixed_precision: false
data:
  training_resolution: 128
  subset: -1
  input_images: 1
  origin_distances: false
  use_pred_depth : false
  use_pred_normal : false
  custom_embedding : false
opt:
  iterations: 800001
  base_lr: 0.00005
  batch_size: 8
  betas:
    - 0.9
    - 0.999
  loss: l2
  imgs_per_obj: 4
  ema: 
    use: true
    update_every: 10
    update_after_step: 100
    beta: 0.9999
  lambda_lpips: 0.0
  pretrained_ckpt: null
  pretrained_hf: False
  lora_finetune : false

model:
  max_sh_degree: 1
  inverted_x: false
  inverted_y: true
  name: SingleUNet
  opacity_scale: 1.0
  opacity_bias: -2.0
  scale_bias: 0.02
  scale_scale: 0.003
  xyz_scale: 0.1
  xyz_bias: 0.0
  depth_scale: 1.0
  depth_bias: 0.0
  network_without_offset: false
  network_with_offset: true
  attention_resolutions:
  - 16
  num_blocks: 4
  cross_view_attention: true
  base_dim: 128
  isotropic: false

logging:
  ckpt_iterations: 1000
  val_log: 10000
  loss_log: 10
  loop_log: 10000
  render_log: 10000

\end{lstlisting}

\noindent
All training in Splatter Image is performed with the PyTorch Adam optimizer. 
The betas and learning rate and EMA toggle hyperparameters are initialized based on the training configuration,
which as seen in the default config listing above, have default values of:

\begin{description}
   \item[Betas:] (0.9, 0.999)
   \item[Learning Rate:] 0.00005
   \item[EMA:] On
\end{description}

\noindent
Exponential Moving Average (EMA) stabilizes learning and improves generalization by smoothing out weight changes during backpropagation.
\newline

\noindent
The three pretrained prior models available on HuggingFace were trained starting from the original pretrained Splatter Image models following a graft.
Those models were then trained for 60,000 additional iterations, with the LPIPS metric disabled.
\newline

\noindent
In order to isolate the parameter-efficient adaptation due to LoRA, we used a dedicated training script (\texttt{train\_network\_lora.py}). While preserving the data pipeline and training loop architecture as the standard framework, it is necessary to enforce strict freezing where the gradients for the U-Net backbone are disabled. Optimisation is therefore restricted solely to the injected low-rank matrices (as described in \textbf{2.2.1 Low-Rank Adaptation (LoRA)}). 
All LoRA models were fine-tuned using the same optimizer settings as the baseline, but with LPIPS enabled. Training was conducted for 10,000 iterations, as discussed later, with checkpoints saved every 1000 iterations to handle restarts. 

\subsection{Testing and Validation Procedures}

\noindent
As discussed in previous sections, our changes often include a variety of runtime assertions to verify 
correctness, an example can be found in our input preparation changes, where assertions are performed to check if
every prior requested for training is available in the training/test/validation datasets requested.
\newline

\begin{lstlisting}[language=Python, caption=Excerpt from Splatter Image input tensor preparation code]
if "use_pred_depth" in model_cfg.data and model_cfg.data.use_pred_depth:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicted maps!"
    input_images = torch.cat([input_images,
                    data["pred_depths"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
if "use_pred_normal" in model_cfg.data and model_cfg.data.use_pred_normal:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicted maps!"
    input_images = torch.cat([input_images,
                    data["pred_normals"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
\end{lstlisting}

\noindent
Notebooks performing unit and integration tests were additionally written. Namely, 3 such notebooks can be found in 
the \verb|testing| folder of the \verb|3DGS-priors| repository.
\newline

\noindent
The \verb|eval_test.ipynb| notebook performs tests to verify if the modified evaluation script 
successfully performs evaluation both on previous datasets (such as ShapeNet cars) and our new dataset
\verb|cars_priors|. It does so by running correctly configured evaluations and checking whether the resulting
\verb|scortes.txt| files exist.
\newline

\noindent
The \verb|graft_test.ipynb| notebook performs tests checking whether the 
\verb|graft_weights_with_channel_expansion| function returns successfully, for a variety of model configurations.
It additionally verifies whether an automatically grafted model's state dictionary exactly matches the state dictionary of a 
manually grafted reference model.
\newline

\noindent
The \verb|dataloader_test.ipynb| notebook tests our custom PyTorch Dataset and ready generated \verb|cars_priors| data.
It does so by loading both the previous reference \verb|srn.py| Dataset and our new \verb|srn_priors.py| Dataset, walking both in an ordered mannered using Torch DataLoaders with shuffling disabled,
and then comparing the resulting batches, which in our case means comparing common dictionary entries, where common key/values entries should be identical.
\newline

\noindent
All tests in these notebooks were run on Google Colab Pro+, using an A100 backend, and passed successfully; each notebook should have visible cell outputs showing this. No additional testing libraries 
or frameworks were used.

\section{Chapter 3: Experiments and Evaluation}
\subsection{Datasets}
\label{sec:datasets}
[Explain the datasets utilized: what they contain, why they are utilized, assumptions, limitations, possible extensions.]

\noindent
The standard benchmark for evaluating single-view 3D reconstruction is ShapeNet-SRN \cite{srn}, hence we used this to test and evaluate our main model implementation. For this dataset, we specifically use the "Car" class, which used the "car" class of ShapeNet v2 \cite{ShapeNet} with 2.5k 3D CAD model instances. The SRN dataset was generated by disabling transparencies and specularities and training on 50 observations of each instance at a resolution of $128 \times 128$ pixels, with camera poses being randomly generated on a sphere with the object at the origin. A limitation of this dataset is the lack of subject variety in the dataset as the model may end up overfitting to cars. A possible extension to address this limitation could be to include other classes in the ShapeNet-SRN database to make sure that the model can still generalise to other types of objects.

\noindent
We also use an extension of this dataset at \url{https://github.com/Xharlie/ShapenetRender_more_variation} which was produced by \cite{xu2019disn}, which presents a Deep Implicit Surface Network to generate a 3D mesh from a 2D image by predicting the underlying signed distance fields. \cite{xu2019disn} generated a 2D dataset composed of renderings of the models in ShapeNet Core \cite{ShapeNet}. For each mesh model, the dataset provides 36 renderings with smaller variation and 36 views with larger variation (bigger yaw angle range and larger distance variation). The object is allowed to move away from the origin, which provides more degrees of freedom in terms of camera parameters, and the "roll" angle of the camera is ignored since it was deemed very rare in real-world scenarios. The images were rendered at a higher resolution of $224 \times 224$ pixels and were paired with a depth image, a normal map and an albedo image as shown in figure \ref{fig:ground_truth}. This dataset was mainly used as a ground truth to evaluate the generation of geometry priors (e.g. normal map and depth map). The ground truth subset also included an alpha channel, which allowed us to determine a deterministic binary marks. This allowed us to quantitatively evaluate the performance of our Salient Object Detection (SOD) models. The samples within this dataset are categorised into `Easy' and `Hard', which allowed us to evaluate the performance of geometry prior generation on different levels of difficulty. 

\noindent
A limitation of this dataset would be its small size since only 72 samples are available for us to use, such that the performance of geometry prior generation may not be evaluated correctly. However, in the same GitHub repository, the script to generate these images from the ShapeNet Core dataset is provided, so a possible extension given more time could be to include more images by running the script on other objects in the ShapeNet Core dataset. Another limitation is that some of the depth maps provided were flawed as shown in figure \ref{fig:depth_poor_gt}, so using them as ground truths to evaluate the performance of the depth map generator model was not ideal. In the future, evaluation could benefit from regenerating these depth maps to be more accurate, perhaps by exploring a different method of depth map generation from the 3D model provided in the ShapeNet Core dataset.

\subsection{Training and testing results}
[Explain the training and testing results with graphs and elaborating on why they make sense, what could be improved.]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/Validation_loss_comparison_PSNR_novel.png}
    \caption{Validation PSNR from 5000-10000 iterations}
    \label{fig:val_psnr}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/Validation_loss_comparison_SSIM_novel.png}
    \caption{Validation SSIM from 5000-10000 iterations}
    \label{fig:val_ssim}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/Validation_loss_comparison_LPIPS_novel.png}
    \caption{Validation LPIPS from 5000-10000 iterations}
    \label{fig:val_lpips}
\end{figure}

\noindent
The metrics introduced here are discussed further in \textbf{Section \ref{sec:lora_results}: LoRA Experiment Results}. 

\noindent
When considering LoRA, the validation metrics from 5 to 10,000 iterations of finetuning reveal a distinct plateau, as we can note the y-axis scale as being extremely tight for the metric scores. Despite testing different rank configurations, we reached what appears to be the convergence limit of the weights early in the fine-tuning process. This behavior could be attributed to the specific initialisation used, where the adapters are initialised with $B$ as zero, allowing a quick plateau (in a few thousand iterations) if the base model's weights were already residing in or close to a local optimum, and this the loss gradients with respect to the LoRA parameters are negligible. Similarly, the similar trajectories between the low and high rank configurations suggest that model capacity may not be the bottleneck. 

\noindent
To verify if this plateau is a hard convergence limit, we could explore several improvements, such as using a larger learning rate for LoRA, or restricting LoRA adaptation to other layers in the U-Net (this could mean considering adding LoRA to the cross-attention blocks in the U-Net). Performing a full finetuning experiment would also help indicate whether the bottleneck comes from the low-rank enforcement or the data used. 

\subsection{Qualitative results}
[Show in figures and explain visual results. Include different interesting cases covering different aspects/ limitations/ dataset diversity. If not converged, explain what we can expect once converged. Include any other didactic examples here.]
\newline

\noindent
To qualitatively assess the reconstruction quality, we can evaluate our models on two categories of viewpoints as defined in the evaluation protocol: conditional Views (the input angle) and novel views (unseen angles).

\noindent
Figure (Figure \ref{fig:conditional_comparison}) looks at conditional views which correspond to the exact camera pose used as input to the model. We expect high performance here, as the model should successfully encode input image features into the Gaussian representation for the object. 

\noindent
As observed below, both tests visually achieve very similar results in comparison to the ground truth. We see an accurate silhouette, with sharp features. 

\begin{figure}[H]
    \centering
   \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.4\textwidth]{images/ExampleImageGT_Cond.png}
        \includegraphics[width=0.4\textwidth]{images/ExampleImageP_Cond.png}
        \caption{Geometry Priors Model (Input View)}
    \end{subfigure}
    \vspace{0.5cm}
    \begin{subfigure}[b]{\textwidth} 
        \centering
        \includegraphics[width=0.4\textwidth]{images/Lora_GT.png}
        \includegraphics[width=0.4\textwidth]{images/Lora_Cond.png} 
        \caption{LoRA Model (Input View)}
    \end{subfigure}
    
    \caption{Conditional View Comparison of the input view reconstruction. Left: Ground Truth, Right: Prediction.}
    \label{fig:conditional_comparison}
\end{figure}

\noindent
The primary challenge of single-view reconstruction lies in novel view synthesis (which is consequently the focus of \textbf{Section \ref{sec:quant}: Quantitative Results}), where the model must infer occluded geometry without direct data. Figure \ref{fig:novel_comparison} illustrates this capability.

\noindent
The results in the following for a LoRA-adapted model is for a configuration of rank 32, $\alpha$ 16, and dropout 0.05. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \includegraphics[width=0.4\textwidth]{images/ExampleImageGT_Novel.png} 
        \includegraphics[width=0.4\textwidth]{images/ExampleImageP_Novel.png} 
        \caption{Geometry Priors Model: Novel View (Left: GT, Right: Pred)}
        \label{fig:priors_novel}
    \end{subfigure}
    
    \vspace{1em}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \includegraphics[width=0.4\textwidth]{images/lora_example246_gt.png}  % Both here are novel views.
        \includegraphics[width=0.4\textwidth]{images/lora_example246_produced.png} 
        \caption{LoRA Model: Novel View (Left: GT, Right: Pred)}
        \label{fig:lora_novel}
    \end{subfigure}
    
    \caption{Novel View Comparison considering unseen angles.}
    \label{fig:novel_comparison}
    
\end{figure}
\subsection{[Optional] Quantitative Results}
\label{sec:quant}
[A table and associated explanations for quantitative results.]
\subsubsection{Metrics}
For novel view synthesis evaluation, PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index) and LPIPS (Learned Perceptual Image Patch Similarity) are standard metrics used to evaluate image quality from different perspectives\cite{zhang2025advancesfeedforward3dreconstruction}. 
PSNR is calculated using the mean squared error and is commonly used to quantify reconstruction quality, such that a high PSNR suggests higher reconstruction quality. Meanwhile, SSIM depends on 3 metrics mimicking the human visual perception system: luminance, contrast and structure, where an SSIM score close to 1 indicates high similarity while a score closer to -1 indicates low similarity. Lastly, unlike pixel-wise metrics like PSNR and SSIM that assume pixel independence, LPIPS measures the perceptual similarity between images by comparing their features extracted from a deep neural network, where a low LPIPS score indicates that the compared images are perceptually similar to humans.

To evaluate whether integrating different geometry priors into the model is effective, we performed a set of experiments testing different combinations of geometry priors. We measured the performance using PSNR, SSIM and LPIPS. 

\subsubsection{Ablation Study Results}
\label{sec:ablation}
We train our models with different combinations of added geometry priors: one with depth maps, one with normal maps and one with both. We compare these models with our baseline.

It is important to note that the original Splatter Image implementation recommends that single-view models be trained in two stages, first without LPIPS, followed by fine-tuning with LPIPS. However, due to the large computational overhead caused by fine-tuning with LPIPS and our lack of computational resources and time, we did not run the second stage with LPIPS. Hence, although we include LPIPS in our metrics in this section, we do not use it to evaluate model performance as it is not an accurate assessment.

\begin{table}[H]
    \centering
    \caption{Significance testing for metrics of augmented models vs baseline (to 3 s.f.)}
    \label{tab:priors_results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Added Geometry Priors} & \textbf{Metric} & \textbf{P-value} \\
        \midrule
        \multirow{3}{*}{Depth Only} 
         & PSNR & 0.000247 \\
         & SSIM & 0.000273 \\
         & LPIPS & 0.0311 \\
        \midrule
        \multirow{3}{*}{Normals Only} 
         & PSNR & 0.000247 \\
         & SSIM & 0.000187 \\
         & LPIPS & 0.0369 \\
         \midrule
         \multirow{3}{*}{Both} 
         & PSNR & 0.000100 \\
         & SSIM & 0.000100 \\
         & LPIPS & 0.0385 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/stats-results/priors_psnr_distribution.png}
    \caption{PSNR distribution for different combinations of added geometry priors}
    \label{fig:priors_psnr}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/stats-results/priors_ssim_distribution.png}
    \caption{SSIM distribution for different combinations of added geometry priors}
    \label{fig:priors_ssim}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/stats-results/priors_lpips_distribution.png}
    \caption{LPIPS distribution for different combinations of added geometry priors}
    \label{fig:priors_lpips}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Results for ablation study}
    \label{tab:ablation}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Added priors} & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$ & \textbf{LPIPS (not considered)} $\downarrow$ \\
        \midrule
        Baseline & 23.999 & 0.923 & 0.077 \\
        Depth Only & 24.796 & 0.929 & 0.081 \\
        Normal Only & 24.814 & \textbf{0.93} & 0.081 \\
        Both & \textbf{24.855} & \textbf{0.93} & 0.081 \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
We conduct a two-sided significance test with non-parametric resampling to calculate the P-value, which is the probability that the metrics of the augmented model are not significantly different from that of the original model and any difference is only due to random variation. We select the power of the test to be 0.05. Since all P-values are less than 0.05 from table \ref{tab:priors_results}, we conclude that the metrics of augmented model significantly differ from that of the original model.

\noindent
Adding combinations of depth and normal priors to augment the model with geometry priors seems to make the model perform more consistently, as shown by the lower variance in all metrics as compared to the baseline in figures \ref{fig:priors_psnr}, \ref{fig:priors_ssim} and \ref{fig:priors_lpips}. All augmented models have a higher mean PSNR and SSIM from figures \ref{fig:priors_psnr} and \ref{fig:priors_ssim} respectively which suggests better image reconstruction quality, with the model adding both depth and normal priors performing the best. As expected, all augmented models are worse than the baseline with regards to LPIPS (as shown in figure \ref{fig:priors_lpips}), since these models were not trained with LPIPS.

\subsubsection{LoRA Experiment Results}
\label{sec:lora_results}
To evaluate whether Low-Rank Adaptation (as described in \textbf{Section \ref{sec:lora}: Manual LoRA Integration}) is effective, we performed a set of fine-tuning experiments on the ShapeNet-SRN Cars dataset. As mentioned in \textbf{Section \ref{sec:lora_parameters}: Low-Rank Adaptation (LoRA)}, LoRA behaviour is primarily controlled by three hyperparameters: rank ($r$), alpha ($\alpha$), and dropout ($d$). We tested multiple configurations by varying these hyperparameters while keeping all other components constant, measuring performance using PSNR, SSIM, and LPIPS.

\noindent
Table \ref{tab:lora_5k} presents the quantitative results for novel view synthesis early in the process (after 5,000 iterations).

\begin{table}[h]
    \centering
    \caption{Results for LoRA at 5,000 iterations. All configurations cluster tightly around 24.75 for PSNR.}
    \label{tab:lora_5k}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Configuration} & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$ & \textbf{LPIPS} $\downarrow$ \\
        \midrule
        $r=8, \alpha=8, d=0.05$ & 24.74 & 0.930 & 0.072 \\
        $r=32, \alpha=32, d=0.5$ & 24.74 & 0.929 & 0.075 \\
        $r=32, \alpha=16, d=0.5$ & 24.75 & 0.930 & 0.075 \\
        $r=16, \alpha=16, d=0.0$ & 24.77 & 0.930 & 0.076 \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
We continued training for 10,000 iterations. Table \ref{tab:lora_10k} details the results for three distinct model configurations at the end of this longer run. The differences between configurations become slightly more pronounced after 10k iterations compared to the 5k mark.

\begin{table}[h]
    \centering
    \caption{Comparison of different LoRA models after 10,000 iterations.}
    \label{tab:lora_10k}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Configuration} & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$ & \textbf{LPIPS} $\downarrow$ \\
        \midrule
        $r=8, \alpha=8, d=0.05$ & 24.81 & \textbf{0.931} & \textbf{0.068} \\
        $r=32, \alpha=16, d=0.05$ & 24.83 & \textbf{0.931} & 0.072 \\
        $r=8, \alpha=8, d=0.1$ & \textbf{24.84} & \textbf{0.931} & 0.072 \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
Here we can see distribution plots for the scores files:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/stats-results/LoRA_psnr_distribution.png}
    \caption{PSNR distribution for different LoRA configurations}
    \label{fig:lora_psnr}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/stats-results/LoRA_ssim_distribution.png}
    \caption{SSIM distribution for different LoRA configurations}
    \label{fig:lora_ssim}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/stats-results/LoRA_lpips_distribution.png}
    \caption{LPIPS distribution for different LoRA configurations}
    \label{fig:lora_lpips}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Significance testing for metrics of different LoRA configurations vs baseline (to 3 s.f.)}
    \label{tab:lora_results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Configuration} & \textbf{Metric} & \textbf{P-value} \\
        \midrule
        \multirow{3}{*}{$r=8, \alpha=8, d=0.05$} 
         & PSNR & 0.00141 \\
         & SSIM & 0.0000600 \\
         & LPIPS & 0.0000133 \\
        \midrule
        \multirow{3}{*}{$r=32, \alpha=16, d=0.05$} 
         & PSNR & 0.000180 \\
         & SSIM & 0.000 \\
         & LPIPS & 0.00408 \\
         \midrule
         \multirow{3}{*}{$r=8, \alpha=8, d=0.1$} 
         & PSNR & 0.000107 \\
         & SSIM & 0.00000670 \\
         & LPIPS & 0.00309 \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
We conduct a two-sided significance test with non-parametric resampling to calculate the P-value, similar to \textbf{Section \ref{sec:ablation}: Ablation Study Results}. We select the power of the test to be 0.05. Since all P-values are less than 0.05 from table \ref{tab:lora_results}, we conclude that the metrics for models with LoRA significantly differ from that of the original model.

% From figures \ref{fig:lora_psnr}, \ref{fig:lora_ssim} and \ref{fig:lora_lpips}, we can see that all changed models now perform more consistently, as shown by the lower variance in all metrics as compared to the baseline. The PSNR metrics in figure \ref{fig:lora_psnr} and the SSIM metrics in figure \ref{fig:lora_ssim} also suggest better image reconstruction overall due to having higher mean PSNRs and SSIMs than the baseline, with the configuration $r=4, \alpha=1$ performing the best with the highest mean PSNR and SSIM. This configuration also has a lower mean LPIPS score than the baseline, further supporting our claim that this configuration is best for augmenting the model, while the other 2 configurations have higher mean LPIPS scores.

\section{Chapter 4: Conclusions and Future Directions}
\subsection{Conclusions}
[Summarize what the project was about and the main conclusions.]

\noindent
Altogether, we managed to successfully augment the baseline Splatter Image implementation with our geometry priors being normal and depth maps. We fed these priors to the model both through the input channels and by using LoRA, and found that both approaches resulted in a higher reconstruction quality than the baseline as measured by metrics PSNR, SSIM and LPIPS. By making these lightweight improvements while preserving Splatter Image's key architecture, we increase its quality while still allowing it to maintain its original computational efficiency with its 3D Gaussian Splatting approach. 

\noindent
Through our ablation study, we also saw that Splatter Image performed best when augmented with both depth and normal maps, showing that both priors are very useful for informing the model about how to reconstruct unseen parts of the input image.

\subsection{Discussion of limitations}
[Explain the limitations of your technique. You may want to refer to previous sections or show figures on the limitations.]
\newline

\noindent
Our biggest limitations stemmed from a lack of time, compute resources, and tooling problems.
Splatter Image, despite its relative high performance for a deep learning reconstruction model, still requires powerful hardware to run.
Not only does it need access to powerful hardware, but it needs access for lengthy periods of time, as to perform tasks like training. 
In terms of tooling and hardware, we used Google Colab Pro+ to give us access to A100 GPUs and 24 hour execution notebooks; despite these promises, we found 
that our notebooks running overnight were very often randomly killed by Google, this caused the complete loss of the VM/notebook and all associated progress 
(namely model training progress). This meant we could not reliably train models to achieve comparable results to the original pretrained models, or truly
show the effectiveness of our modifications, where for comparison, the Splatter Image provided pretrained models were trained continuously on A6000 GPUs over a period 
of 7 days.  Should we have reached adequate training time, the quality of our selected prior models would have likely been the limiting factor
for improvements to Splatter Image, as Splatter Image can only be as good as the quality of the input (principle of garbage-in-garbage-out).
\newline

\noindent
Another set of limitations came in the form of poor documentation. During research of prior models
we often struggled to run the models in the first place, due to incorrectly provided 
package requirements, serious (or in some cases total) lack of instructions on how
to launch and use the models, or no reference implementations being publicly provided at all, as was often the case with research papers.
\newline

\noindent
We were also given access to the reference SRN cars dataset at a very late stage in the project, which did not leave us enough time 
to implement desired things like calculating our own reliable ground truths, or being able to implement additional reconstruction metrics like 
Chamfer distance, where we could have generated point clouds from SRN car models necessary for the calculation.

\subsection{Future Directions}
\label{sec:future}
[State a few future directions for research and development. These typically follow from the discussion on limitations.]
\newline

\noindent
One avenue for future work is generating and testing more geometry priors for future research. 
Due to the lack of time, there were some priors that we deemed not worth investigating. 
For example, in \textbf{Section \ref{sec:normal}: Planes and Normal Maps Exploration}, we discuss how generating predicted scene planes for input images and using them as added priors to augment the model seemed unlikely to help guide reconstruction. 
In future studies, we could challenge this assumption by generating this prior and testing it alongside other geometry priors in the ablation study. 
We could also use the SOD models we explored in \textbf{Section \ref{sec:segmentation}: Segmentation and Salient Object Detection Exploration} to generate binary masks and use them as priors to augment our model and see how they affect model performance.
In general, since Splatter Image has most trouble generating sections of novel views that are obscured from the original input image, we could focus on generating and testing more priors oriented to hidden areas.

\noindent
Some of these priors may be in a different format that is not compatible with the current input channels, such as classifications from segmentation models. Hence, an extension could be to implement cross attention and FiLM to the original model such that it can support multimodal input data. We can then compare the two approaches to see which is most useful for Splatter Image.

\noindent
We could also investigate the integration of explicit geometric loss functions to supplement the current standard losses. A specialised `rendering loss' could be introduced that targets depth discontinuities, especially following on from the discussion in \textbf{Section \ref{sec:depth}: Depth Map Exploration}. The model could be penalised for failing to reconstruct structural boundaries (by applying edge operators on rendered views and comparing them with ground truth edge maps if possible). This strategy was successfully demonstrated in \cite{gong2024eggsedgeguidedgaussian}. This loss can also be dynamically weighted based on the size of the 3D Gaussian's covariance matrices, as `smaller' Gaussians, with smaller covaraince determinants, generally represent fine details. If this loss is prioritised, i.e. we prioritise areas contributed to by small scale Gaussians, the optimisation process could be forced to resolve fine details, rather than being treated as noise or being approximated by larger Gaussians.  

\noindent
Another avenue for future work could be to train and evaluate the models with a more diverse set of datasets. 
As mentioned in \textbf{Section \ref{sec:datasets}: Datasets}, our training and tested was conducted with the ShapeNet-SRN cars dataset, which may cause our models to overfit to cars and not generalise well to other types of objects. 
We could try to address this by training the model with other datasets with different objects, such as the Objaverse \cite{objaverse}\cite{objaversexl} datasets and the CO3D \cite{co3d} dataset. 
With more computational resources, we could also explore increasing training times for the models to see if their performances can be improved further with more iterations of training.

\noindent
Another possible extension could be to try using alternative metrics for 3D reconstruction such as Chamfer distance, where we could generate point clouds from SRN models and compare them to the 3D Gaussians predicted by Splatter Image.

\subsection{Project Contributions}

\noindent
\textbf{Report Writing Contributions:}
\newline
Section 1.1: Kacper and Alex
\newline
Section 1.2: Kacper and Alex
\newline
Section 1.3: Kacper
\newline
Section 2.1: Alex
\newline
Section 2.2: Kacper and Radhika
\newline
Section 2.3: Kacper and Radhika and Alex
\newline
Section 2.4: Kacper and Alex
\newline
Section 2.5: Kacper and Radhika
\newline
Section 2.6: Kacper
\newline
Section 3.1: Alex and Radhika
\newline
Section 3.2: Radhika and Alex
\newline
Section 3.3: Radhika
\newline
Section 3.4: Radhika and Alex
\newline
Section 4.1: Alex
\newline
Section 4.2: Kacper and Alex
\newline
Section 4.3: Radhika and Alex
\newline
Image and citation collection: Kacper and Radhika and Alex
\newline

\noindent
\textbf{Presentation Contributions:}
\newline
Slides and Recording: Kacper and Radhika and Alex
\newline
Video Editing: Alex
\newline

\noindent
\textbf{Technical Contributions:}
\newline
Depths exploration: Radhika
\newline
Edge operators exploration: Radhika
\newline
Segmentation exploration: Radhika
\newline
Normals exploration: Alex
\newline
Planes exploration: Alex
\newline
Splatter Image setup and bugfixes: Radhika and Alex
\newline
Splatter Image architectural modification (Grafting, Channel changes, FiLM): Kacper
\newline
Splatter Image LoRA integration: Radhika
\newline
Splatter Image Training Modification: Kacper and Radhika
\newline
Splatter Image Eval Modification: Kacper
\newline
Optimised depth generation: Kacper
\newline
Optimised normals generation: Kacper and Alex
\newline
Optimised segmentation generation: Radhika
\newline
Splatter Image \verb|cars_priors| custom Dataset: Kacper
\newline
HuggingFace dataset data pipeline: Kacper
\newline
Results processing code: Alex
\newline
Graphing code: Alex
\newline
Testing custom Dataset: Kacper
\newline
Testing evaluation: Kacper and Radhika
\newline
Testing prior configurations: Kacper
\newline
Testing LoRA configurations: Radhika 

\bibliographystyle{unsrt}
\bibliography{citations}

\end{document}