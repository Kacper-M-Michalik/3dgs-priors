\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{multirow} 
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage[section]{placeins}

\titleformat{\section}
  {\normalfont\Large\bfseries} % Text formatting (Bold, Large)
  {}                           % <--- THE LABEL: Left empty to hide the number
  {0pt}                        % Space between number and title (set to 0)
  {}                           % Code before title

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1, 1, 1}
\definecolor{moonstoneblue}{rgb}{0.45, 0.66, 0.76}

\lstdefinestyle{markdown-style}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,    
    breakatwhitespace=false,         
    breaklines=true,                  
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,    
    frame=single,
    rulecolor=\color{moonstoneblue},
    framerule=1.5pt,            
    captionpos=b,  
    aboveskip=1.5em,
    belowskip=1.5em
}

\lstset{
    style=markdown-style
}

\title
{
    Single-Image 3DGS Scene Reconstruction with Geometry-Aware Priors
}
\author
{
    Machine Visual Perception Course Project Report
}

\begin{document}
\maketitle

\section*{Information}
Authors: Kacper Michalik, Radhika Iyer, Alex Loh
\newline
Group Number: 7
\newline
Supervisor: Ahmet Canberk Baykal

\section{Chapter 1: Introduction and Motivation}
\subsection{Introduction to the problem}
[Provide a thorough introduction to the problem and why it is important. Briefly explain what general techniques there are and how your project fits.]
\newline

\noindent
The advancement of 3D data acquisition, reconstruction, and rendering methods remains a fundamental and persistent open problem in computer vision.
Efficient, high-quality 3D reconstruction is increasingly critical for applications ranging from augmented reality (AR/VR), autonomous devices 
(for example in perception or navigation in robotics or self-driving cars) to digital artistry 
(geometry acquisition for models in VFX, games or other digital products), driving significant research interest in this domain.

\noindent
Historically 3D reconstruction has been a challenging task that requires large number of reference images and even larger amounts of compute.
Advancements in computer hardware and machine-learning methods have significantly improved the efficiency and accuracy 
of 3D reconstruction, extending its applicability for a wider variety of tasks and hardware platforms; continuing this trend, 
one area of focus is made on further reducing the number of input images required for reconstruction, 
thereby improving computational efficiency and reducing data acquisition hardware requirements,
this has lead to interest in the issue of novel view synthesis.
Namely, in 2020 Neural Radiance Fields\cite{nerf} (NeRF) introduced a revolutionary method for high quality novel view synthesis.
This is done by learning a continuous volumetric density and radiance function, which can then be ray-marched from new camera poses, enabling novel views to be synthesized.
In 2023, 3D Gaussian Splatting\cite{3dgs} (3DGS) introduced an alternative method, offering major improvements in computational performance.
Instead of an implicit function, 3DGS optimizes a set of 3D Gaussians, defined by parameters such as position, colour, opacity, rotation and scale, 
which can be efficiently rasterized to generate novel views.
Developments in 3D Gaussian Splatting methods have allowed for 3D scene reconstruction using few or even single RGB images.
While faster than other scene reconstruction techniques and requiring only a "one-shot" pass, 
these approaches often suffer from challenges such as layout/scale drift, over-smooth geometry and hallucinations in occluded regions \cite{tang2024hisplathierarchical3dgaussian} (NOTE TODO: for now this only talks mostly about over-smooth geometry and kind of hallucinations, still looking for a layout/scale drift one). 
\newline

\noindent
This project focuses on one recent method, Splatter Image\cite{splatterImage}, as a baseline. 
Splatter Image allows single or few RGB image 3DGS reconstruction. Achieved by predicting 3D Gaussians as pixels in a multichannel image; this representation reduces reconstruction to 
learning an image-to-image neural network, allowing the use of a 2D U-Net to form the representation.
Each pixel stores the parameters for a corresponding 3D Gaussian, allowing for reconstruction in a single feed-forward pass. This overall architecture allows for a compute-efficient model. 
Despite its speed, Splatter Image does have some issues that have been noted in related works,
particularly in reconstructing structures unseen in the input view, including for views significantly different from the source. 
One source of this problem may be that the prediction of 3D Gaussians in Splatter Image is based on input image features alone,
which do not have sufficient conditional information for Gaussian splatting to represent structures that are not visible in the
input view\cite{shen2024pixelworth3dgaussians}; shown in figure \ref{fig:splatter_image_problems}, Splatter Image has trouble generating the 
occluded chair leg in \ref{fig:chair_splatter_1} and \ref{fig:chair_splatter_2}.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_input.png}
        \caption{Input image}
        \label{fig:chair_input}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_truth_1.png}
        \caption{Ground truth}
        \label{fig:chair_truth_1}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_splatter_1.png}
        \caption{Splatter Image output}
        \label{fig:chair_splatter_1}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_truth_2.png}
        \caption{Ground truth}
        \label{fig:chair_truth_2}
    \end{subfigure}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/chair_splatter_2.png}
        \caption{Splatter Image output}
        \label{fig:chair_splatter_2}
    \end{subfigure}

    \caption{Splatter Image outputs compared with ground truth taken from \cite{shen2024pixelworth3dgaussians}}
    \label{fig:splatter_image_problems}
\end{figure}

\noindent
This project aims to address this by first researching inferable geometry priors (such as planes, normals, visibility cues, depth, segmentation or edge maps) 
which can be dynamically produced for input images by existing specialized models, then proposing a lightweight augmentation for Splatter Image, allowing
predicted priors to be fed alongside the RGB images, allowing them to guide reconstruction in a more accurate manner
compared to current case, where the neural network needs to learn these geometric features itself.

\subsection{Background and related work}
[Include a few very relevant related works and how your work relates to those, expanding on the previous section. We do not expect you to cover all previous works.]
\newline

ADD DIAGRAMS OF PHTOGRAMETRRYT GAUSSAIN SPLAT AND NERF
\newline

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/photogrammetry.png}
    \caption{An example of 3D reconstruction with many overlapping images from \cite{photogrammetry}}
\end{figure}

\noindent
Traditionally, 3D reconstruction has been performed by multi-stage photogrammetry pipelines, relying on explicit geometric
representations such as meshes or point clouds. The industry-standard workflow begins with Structure from Motion (SfM), 
which matches sparse feature points across many overlapping images to estimate camera parameters and generate a sparse point cloud.
This is typically followed by Multi-View Stereo (MVS) algorithms to compute dense depth maps, which are combined to
generate a standard 3D mesh using techniques like Delaunay triangulation or Moving Least Squares with Marching Cubes. TODO: CITE 
While effective for static, diffuse environments, these methods struggle significantly with surfaces such as 
transparent windows or reflective metals, this as they rely on strict photometric consistency and lack a mechanism to deal with view-dependent radiance.
Additionally, these methods require large numbers of high-resolution images with substantial overlap to achieve high-quality reconstruction.
This heavy data acquisition requirement means the methods create a computational bottleneck, requiring hours of processing time on high-end hardware, and
are impractical in the first place without complex image acquisition setups, such as in Figure TODO: LINK.

\noindent
A paradigm shift occurred in 2020 with the introduction of Neural Radiance Fields\cite{nerf}. NeRF moves away from explicit geometry representations 
to an implicit volumetric representation. In NeRF an underlying continuous volumetric scene function, represented using a deep learning model, 
is optimized using a set of input images with known camera poses; the input to the function/model
is a single continuous 5D coordinate (spatial location and viewing direction) and the output is the volume density and view-dependent emitted radiance at that location.
Novel views are synthesized by querying 5D coordinates along camera rays for some new camera pose (ray-marching), and output colours and densities are rendered
into an image. NeRF allowed for higher quality 3D reconstruction, eventually using sparser image sets (PixelNeRF, RegNeRF), compared to traditional photogrammetry methods,
achieving state-of-the-art results and becoming the gold standard for novel view synthesis. TODO: CITE

\noindent
In 2023 the field was further revolutionized by 3D Gaussian Splatting, offering a computationally high-performance alternative to NeRF.
3DGS offers a new explicit geometry representation, modelling a scene as a collection of parameterized 3D Gaussians. TODO: CITE
Unlike NeRF, 3D Gaussian Splatting does not rely on a neural network to generate a scene. Instead, in the original paper, reconstruction 
is achieved by first initializing a set of Gaussians, either randomly or from a sparse point cloud (typically from Structure from Motion), where
each Gaussian is defined by a set of parameters: position (mean), covariance (scale and rotation), opacity, and colour 
(represented using Spherical Harmonics to allow for view-dependent radiance). Then an optimization process is run
that first adjusts the Gaussians' parameters to minimize the error between rendered images and the ground truth; and secondly performs dynamic management
of the density of Gaussians within the scene, by splitting, cloning or pruning Gaussians in an interleaved manner; namely Gaussians in under-reconstructed areas 
are cloned, Gaussians with high variance are split, and Gaussians in areas of low opacity and of excessive size are pruned.
Finally, for rendering, the Gaussians are rasterized using a custom tile-based rasterizer to produce resulting views, allowing millions of Gaussians rendered in
real-time, allowing for real-time novel view synthesis. 
While the original method relies on per-scene optimization to generate 3D Gaussian Splats, recent works have begun using deep learning methods to 
predict sets of Gaussians directly, with most recent developments allowing for 3D scene reconstruction using few or even single RGB images.
\newline

ADD IAMGES OF F3D GAUS, GAUSS VIDEODREAMER, TGS, SPLATTERIMAGE
\newline

\noindent
Since the introduction of 3DGS, a number of deep learning architectures and processing pipelines based on the method have been developed 
to find the most accurate and efficient implementation capable of producing high quality 3D Gaussian Splats.
Recent examples include ExScene, Wonderland, F3D-Gaus, Gauss VideoDreamer, TGS and Splatter Image.\cite{splatterImage}

\noindent
F3D-Gaus

\noindent
Gauss VideoDreamer

\noindent
The triplane representation was proposed to efficiently and expressively represent 3D volumes \cite{triplane}, as a compromise between rendering speed and memory consumption. 
They were shown to scale to large datasets like Objaverse \cite{objaversexl}\cite{objaverse}, but at the cost of hundreds of GPUs for multiple days \cite{triplanebasedmodel}.

\noindent
Another recent method is Splatter Image \cite{splatterImage} then applies Gaussian Splatting to monocular reconstrution by using a set of 3D Gaussians as the 3D representation. 
It predicts a 3D Gaussian for each of the input image pixels and uses a 2D image as the container of the 3D Gaussians, storing the parameters of one Gaussian per pixel. 
This reduces the reconstruction problem to learning an image-to-image neural network, allowing the reconstructor to be implemented utilizing only efficient 2D operators. 
The use of Gaussian Splatting in this approach increases rendering and space efficiency, which benefits inference and training. 
Our work continues to expand on this method through investigating different geometry priors and integrating them into the current model as appropriate.

\subsection{Overview of the idea}
[Provide an overview stating why the idea of the project makes sense and what the main motivation is.]

whilst providing excellent quality recsontruction from popint of vie wof refernec iamge, the actual 3d reocusntrion and resulting novel views
suffer from hallucinations in the form of floating geometry, etc etc,
this is fundamanetally due to the lack of inforamtion the model has from a single input image
feeding models additional data improves reconstruction 
with modern compute and ml advancements there now exist many good quality pretrained geometry related models
for example generating depth, normal maps, segmentation
we propose exploting the knowledge/capacibiltiy of these models to predict additional priors of input images, then creating a modified model that accepts these
priors, allowing priors to guide reconsutrcution, improvingt recosntructio quality
we also propose performing ablation study to see which priors are most effective/significant in changing the reconstruction quality

\section{Chapter 2: Method}
\subsection{Baseline algorithm}
[Explain the baseline architecture you used to build your algorithm on. You may reproduce figures from the original papers.]

\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/splatter_image_diagram.png}
        \caption{Overview of SplatterImage\cite{splatterImage}}
    \label{fig:splatter_img_diagram}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/u-net.png}
        \caption{U-net architecture\cite{unet} that Song U-Net\cite{songunet} is based on}
        \label{fig:u-net_diagram}
    \end{minipage}
\end{figure}

\noindent
Splatter Image uses a standard image-to-image neural network architecture to predict a Gaussian for each pixel of the input image I, 
generating the output image M as the Splatter Image. Learning to predict the Splatter Image can be done on a single GPU using at most 
20GB of memory at training time for most single-view reconstruction experiments (except for Objaverse, where 2 GPUs were used and 26GB 
of memory was used on each). Most of this neural network architecture is identical to the SongUNet of \cite{songunet}, but the last layer
is replaced with a $1 \times 1$ convolutional layer with $12 + k_c$ output channels, where $k_c \in \{3, 12\}$ depending on the colour model. 
The output tensor codes for parameters that are then transformed to opacity, offset, depth, scale, rotation and colour respectively. 
These parameters are then activated by non-linear functions to obtain the Gaussian paramters, such as the opacity and depth. 
The Gaussian Splatting implementation of \cite{3dgs} is used for rasterization to generate $360^\circ$ views of the original input image.

TALKA BOUT ARCH MORE, CROSS ATTN FILM, Each blue box corresponds to a multi-channel feature map, with the number of channels denoted on top of each box. The x-y size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.


\subsection{Algorithm improvements}
[Explain what you implemented to improve over the baseline. You may include figures to explain the idea and logic. Focus on the ideas and not the implementation.]

explain insertion of additional layers
explain addition of transofrmer/FiLM layers to allow multimodal input from segmentation tokens 

FiLM layers learn by training a separate, small neural network called a FiLM generator.
This is a new, small network (like an MLP or RNN), segmentation embedding is then fed into the FiLM generator
The generator outputs a gamma (scale) and beta (shift) 
The gamma and beta values are applied to intercepted alyer using the FiLM equation: y = gamma * x + beta
This modulated feature map is passed to the next layer of the U-Net.
Exisitng U-Blocks support Film for camera embeddings, we modidy U-net blocks to have additional film layer for segemtnation tokens

Add new cross-attention modules inside the U-Net's blocks (in the bottleneck and decoder).
The U-Net's image features act as the Query (Q). The multimodal input embeddings act as the Key (K) 
and Value (V). More expressive than FiLM as it allows for fine-grained spatial conditioning (per-pixel).,
this method is consdiered state-of-the-art for high-performance conditional generation.

explain addition of LORA matrices due to compute limitations/allowign working of exisitng model

\subsubsection{Low-Rank Adaptation (LoRA)}
To address the computational constraints of fine-tuning the full U-Net architecture, we considered Low-Rank Adaptation (LoRA) \cite{HuLoRA}. LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into layers within the architecture. 

We utilize three key hyperparameters to control this adaptation:
\begin{itemize}
    \item Rank ($r$): The rank of the low-rank matrices $A$ and $B$. Higher ranks increase the number of parameters and the capacity of the adaptation, but also increase computational cost
    \item Alpha ($\alpha$): A scaling factor applied to the LoRA update during the forward pass. The update is scaled relative to the weights that have been frozen from the pretrained model.
    \item Dropout ($p$): The dropout probability applied to the LoRA layers during training. This randomly disables activations, which aims to prevent overfitting. 

\end{itemize}

\subsection{Implementation details}
[Explain how you implemented the improvements. You may include code snippets with the corresponding explanations.]
\newline

\noindent
All code associated with the project can be found in the following repositories:
\newline

\noindent
3DGS-priors (Top level repository for the project): \url{https://github.com/Kacper-M-Michalik/3dgs-priors}
\newline
\noindent
Splatter Image Fork: \url{https://github.com/Kacper-M-Michalik/splatter-image}
\newline

\noindent
Generated datasets and model weights can be found in the following repositories:
\newline

\noindent
Datasets with Predicted Priors: \url{https://huggingface.co/datasets/MVP-Group-Project/srn_cars_priors}
\newline
\noindent
Pretrained Models: \url{https://huggingface.co/MVP-Group-Project/splatter-image-priors}

\subsubsection{Planes and Normal Maps Exploration}

WE CONSDIERED PROVIDING MODEL WITH SPTAIL INFORAMTION ABOTU THE SCENE AS BEING ONE OF 
THGE MSOT LIEKLY AENUES OF IMPROVEMENT, THIS CAME IN 2 FLAVOURs, PREDICITNG PLANES AND STREUCTURES IN SCENE
AND SURFACE NORMAL MAPS

WHEN CONSDIERING GENERTING PLANES
For example teddybears (as seen on the CO3D Teddybears) do not have dominant planes, they are complex convex shapes,
using a planar prior might confuse the network or cause it to flatten the bear's features; as such we decided against
using planes as a prior.

A much more favourable option were normal maps. Normal maps store surface normal data as RGB colour information, 

showing how light interacts with the surface at a per-pixel level, hence we wanted to investigate if 
that could help the model generate the 3D surface of the input image.

\noindent
For our ground truths we use \cite{xu2019disn} which provides a dataset of higher resolution images of the ShapeNet models 
from \cite{ShapeNet} each paired with a depth image, a normal map and an albedo image at \url{https://github.com/Xharlie/ShapenetRender_more_variation}. 
We then feed these images into the normal map generation models and compare against the ground truth normal maps to evaluate their performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/normal_gnd_truth.png}
    \caption{Example of image with maps used as ground truth taken from \cite{Bae2021}}
    \label{fig:ground_truth}
\end{figure}

For the models we referenced \cite{Bae2021} which implements a network which estimates the per-pixel surface normal probability distribution and uses uncertainty-guided sampling to improve the quality of prediction of surface normals. The paper provided code at \url{ https://github.com/baegwangbin/surface_normal_uncertainty} that implemented this method on a network trained on ScanNet \cite{dai2017scannet}, with the ground truth and data split provided by FrameNet \cite{huang2019framenet}, and another trained on NYUv2 \cite{NYUv2}, with the ground truth and data split provided by GeoNet \cite{qi2018geonet} \cite{qi2020geonet++}. Both models take in the original image and dimensions of the image as input and return a corresponding normal map with the same dimensions as the given input dimensions.

We run both pretrained models on the dataset.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/original.png}
        \caption{Original}
        \label{fig:highdefinput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN.png}
        \caption{ScanNet output (224x224)}
        \label{fig:highdefoutput}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/GN.png}
        \caption{NYUv2 output (224x224)}
        \label{fig:gn1}
    \end{subfigure}

    \caption{Comparison of original input and two model outputs}
\end{figure}

We then pass in input dimensions larger than the actual ones into the models, such that a normal map larger than the original input is produced. We then resize the image to the original input dimensions.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/gnd_truth.png}
        \caption{Ground truth from \cite{xu2019disn}}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_resized.png}
        \caption{ScanNet output (448x448)}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/GN_resized.png}
        \caption{NYUv2 output (448x448)}
        \label{fig:gn2}
    \end{subfigure}

    \caption{Comparison of model outputs when setting input dimensions as 448x448 instead of 224x224 alongside ground truth}
\end{figure}

The normal map generated for images when given larger input dimensions seem to have more clearly defined edges and surface contouring. It is also important to note that the ground truth for NYUv2 is only defined for the centre crop of the image and the prediction is therefore not accurate outside the centre. This is shown in figures \ref{fig:gn1} and \ref{fig:gn2} where noise is generated around the borders of the normal maps.

To compare our generated normal maps to the ground truth normal maps provided in \cite{xu2019disn}, we first mask out the background of the generated normal maps such that the difference in background colour does not contribute to the evaluation metrics for normal map generation.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/gnd_truth.png}
        \caption{Ground truth from \cite{xu2019disn}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_resized.png}
        \caption{ScanNet output (448x448)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/BN_filtered.png}
        \caption{Output with background masking}
    \end{subfigure}

    \caption{Example of masking out background for model evaluation against ground truth}
\end{figure}

We then use Pixel Based Visual Information Fidelity to compare the normal maps generated by the two models to the ground truth. Visual Information Fidelity is a reference image quality metric that quantifies the amount of visual information preserved after image processing \cite{vifIntro} and can be used to measure various image quality attributes such as noise level and sharpness \cite{vifMetrics}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/vif_plot.png}
    \caption{Comparison of VIF between ground truth and different models}
    \label{fig:vifgraph}
\end{figure}

From Figure \ref{fig:vifgraph} we see that the model trained on ScanNet generates normal maps that are closer to the ground truth compared to that trained on NYUv2 on average. Hence, in the final model we decided to use the model trained on ScanNet on the ShapeNet database in \cite{ShapeNet}.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_orig.png}
        \caption{Original image from \cite{ShapeNet} of size 128x128}
        \label{fig:lowdefinput}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_output.png}
        \caption{Output from ScanNet model with 128x128 passed in as input dimensions}
        \label{fig:lowdefoutput}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/shapenet_output_resized.png}
        \caption{Output from ScanNet model with 512x512 passed in as input dimensions}
        \label{fig:lowdefoutputfixed}
    \end{subfigure}

    \caption{Original ShapeNet image and normal map outputs}
\end{figure}

Without passing in dimensions larger than the input image into the model, we can see from comparing Figures \ref{fig:highdefinput} and \ref{fig:highdefoutput} to Figures \ref{fig:lowdefinput} and \ref{fig:lowdefoutput} that the quality of the normal map generated decreases as the resolution of the original input image decreases. Hence, we pass in much larger input dimensions (512x512) to generate a normal map of higher quality, as shown in Figure \ref{fig:lowdefoutputfixed}.

\subsubsection{Depth Map Exploration}
Depth maps store the distance of a surface from the camera per-pixel. These distances vary in type, such as metric, which considers the physical distance from the camera to the observed point, and relative (such as those produced by the models below). Monocular depth estimation (MDE) models input just a singular image, and produce a depth map (relative distance). 

Produced depth maps were compared against the ``ground truths'' produced by \url{https://github.com/Xharlie/ShapenetRender_more_variation}, as was done in the normal priors exploration. An example of the depth map produced by them is visible in Figure  \ref{fig:ground_truth}. However, it is important to note that these depth map ``ground truths'' were not always perfect, as can be seen in the following example: 

- (INSERT) Image needed here of poor ground truth. 

This inclined us to take the quantitative results produced by comparing MDE models tested against these ground truths with a pinch of salt. For each produced depth map, the following metrics were used to compare against the ground truths. 
\begin{enumerate}
\item \textbf{Absolute Relative Error}: Measures the average difference between the predicted depth and the ground truth, normalised by the ground truth depth.
\item \textbf{Root Mean Squared Error (RMSE)}: Calculates the standard deviation of the residual errors.
\item \textbf{Scale-invariant RMSE (SI-RMSE)}: Computes the RMSE while ignoring the unknown absolute scale and shift between the prediction and ground truth.
\item $\mathbf{\delta}$ at $\mathbf{1.25}$ ($\delta_{1.25}$): Represents the percentage of predicted pixels $p$ that satisfy the condition $\max(\frac{p}{p^{gt}}, \frac{p^{gt}}{p}) < 1.25$, which takes into account close pixel-wise agreement.
\end{enumerate}

The following table (LINK to label) summarises the mean metrics across the MiDaS models tested. 
 
\subsubsection{Segmentation and Salient Object Detection Exploration}

Separating pixels belonging to the foreground object, through a segmentation mask or, as will be detailed below, using a salient object detection (SOD) model, can be another prior. This involves producing a binary mask that separates an object from its background. 

Initially, we explored standard semantic and panoptic segmentation models, such as those found in the Detectron2 \cite{wu2019detectron2} model zoo, and the Segment Anything Model (SAM) \cite{kirillov2023segany}. These models are often used for segmentation, but as illustrated in Figure \ref{fig:seg_issues}, these produced non-contiguous masks that often had sections that included more background pixels. Segmentation models are also limited on their training classes, and despite being tested on categories in this set, their masks were improved on by salient object detection models. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/SAM_output.png} 
        \caption{SAM Results (3 candidate masks produced per input)}
        \label{fig:sam_example}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/PanopticFPN_output.png}
        \caption{Panoptic Segmentation (Detectron2 Model) Result}
        \label{fig:panoptic_example}
    \end{subfigure}
    
    \caption{Sample outputs from standard segmentation approaches.}
    \label{fig:seg_issues}
\end{figure}

SOD models identify the most visually distinct object in a scene, which allows producing a binary mask that tightly hugs the object boundary. 

To quantitatively evaluate SOD models, we noted that the ShapeNet images used (the same as in the normal and depth priors section) had transparent backgrounds, allowing using the alpha channel to be used as the `ground truth' for the object silhouette. 

We tested three SOD architectures: \texttt{rembg} (based on the U-2-Net architecture) \cite{Gatis_rembg_2025}, \texttt{InSPyReNet} \cite{kim2022inspyrenet}, and \texttt{BiRefNet} \cite{zheng2024birefnet}. 
We evaluated performance using Mean Absolute Error (MAE), Intersection over Union (IoU), and $F_{\beta}$-Measure.

$F_{\beta}$ is a weighted harmonic mean of precision and recall, defined as:
\begin{equation}
F_{\beta} = \frac{(1 + \beta^2)\,\text{Precision} \cdot \text{Recall}}
                 {\beta^2 \cdot \text{Precision} + \text{Recall}}
\end{equation}


It is commonly used is salient object detection to assess the quality of binary masks produced. When $\beta = 1$, precision and recall are equally weighted. Greater values of $\beta$ prioritise recall, while lower ones prioritise precision. We set $\beta^2$ to 0.3, following conventional practice in SOD literature, to emphasise precision over recall \cite{achantaFbeta}, \cite{chengFbeta}, \cite{borjiFbeta}.   When identifying a single salient object, false positives (background pixels incorrectly classified as foreground)  are often considered worse than small false negative sections along the boundary of the object. 

Note that the ShapeNet images used are split into `Easy' and `Hard' as categories, as before. 
\begin{table}[H]
    \centering
    \caption{Comparison of Salient Object Detection models on ShapeNet renders. $\uparrow$ indicates higher is better, $\downarrow$ indicates lower is better.}
    \label{tab:sod_results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Difficulty} & \textbf{Model} & \textbf{IoU} $\uparrow$ & \textbf{$F_{\beta}$} $\uparrow$ & \textbf{MAE} $\downarrow$ \\
        \midrule
        \multirow{3}{*}{Easy} 
         & rembg (U-2-Net) & \textbf{0.986} & 0.991 & \textbf{0.004} \\
         & InSPyReNet      & 0.983 & \textbf{0.996} & \textbf{0.004} \\
         & BiRefNet        & 0.966 & 0.979 & 0.006 \\
        \midrule
        \multirow{3}{*}{Hard} 
         & rembg (U-2-Net) & \textbf{0.980} & 0.988 & \textbf{0.005} \\
         & InSPyReNet      & 0.966 & \textbf{0.991} & 0.006 \\
         & BiRefNet        & 0.952 & 0.973 & 0.007 \\
        \bottomrule
    \end{tabular}
\end{table}

Figure \ref{fig:fbeta_boxplot} illustrates the distribution of $F_{\beta}$ scores across both ``Easy'' and ``Hard'' datasets. \texttt{InSPyReNet} has the tightest interquartile range, particularly on the Easy set.  \texttt{rembg} demonstrates similar stability but with a slightly broader spread on the `Hard ' dataset. \texttt{BiRefNet} is competitive (note the scale of the y-axis, with all achieving scores greater than 0.95), but comparatively shows a lower median score and higher variance, suggesting it is more sensitive to specific geometries. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/SOD_Fbeta.png}
    \caption{Distribution of $F_{\beta}$ scores for each model across the two difficulty levels.}
    \label{fig:fbeta_boxplot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/SODPrecisionRecall.png}
    \caption{Precision vs. Recall trade-off}
    \label{fig:prec_recall}
\end{figure}


The scatter plot in Figure \ref{fig:prec_recall} shows how the models operate with different aims; . \texttt{InSPyReNet} (in orange) clusters tightly towards the top of the high-performance region (top right), suggesting a priority of precision (often very close to $1.0$). In practice, this may mean eliminating some background noise, but this may mean missing parts of the object. \texttt{rembg} (blue) also has  high recall and precision, and it could also be used as a robust general-purpose model for segmentation (that does not clip parts of the object of interest). 


To identify the most frequent ``winner," we counted the number of images where each model achieved the highest $F_{\beta}$ score. \texttt{InSPyReNet} achieved the highest score on the majority (28 vs \texttt{rembg}'s 8). 

However, to contextualise these results, all three models achieved very high scores (with $F_{\beta} > 0.97$ and IOU $> 0.95$), largely as the ShapeNet images have a clear foreground object against a uniform background. This reduces the number of possible cases of background vs foreground confusion, which means the models differ meaningfully here on fine structures at the object boundary. In practice, these can include wing mirrors, antennae, etc. Therefore, the differences in the models come from how well they capture these fine details.

\subsubsection{Selected Prior Integration}
During model selection, basic notebooks were written to operate and evaluate the relevant models. 
Once a model was selected, the corresponding notebook would be refactored into a high-performance 
script designed to process full datasets, producing ready-processed priors for every RGB image within the given dataset. 
Specifically, inference code would be rewritten to ensure it operates on high performance hardware such as the GPU,
and that all operations were executed in a batched manner.

\begin{lstlisting}[language=Python, caption=Batched processing of depths]
with torch.no_grad():
    for batch_imgs, batch_filenames in loader:      
        # Prediction, B C H W format              
        batch_imgs = batch_imgs.to(device)            
        preds = model(batch_imgs)

        # Batched downsize
        preds = torch.nn.functional.interpolate(
            preds.unsqueeze(1),
            size=target_size,
            mode="bicubic",
            align_corners=False
        ).squeeze(1)

        # Batched 0 to 1 normalization
        batch_flat = preds.flatten(start_dim=1)
        min_val, max_val = torch.aminmax(batch_flat, dim=1, keepdim=True)
        min_val = min_val.view(preds.size(0), 1, 1)
        max_val = max_val.view(preds.size(0), 1, 1)
        preds_normalized = (preds - min_val) / (max_val - min_val + 1e-8)                    
        preds_uint8 = preds_normalized.mul(255).byte().cpu().numpy()

        # H W format -> 128 * 128
        for j, file_id in enumerate(batch_filenames):   
            full_batch.append(ProcessedImage(
                uuid=uuid,                             
                file_id=file_id.item(),
                image=preds_uint8[j].tobytes()
            ))
\end{lstlisting}

\noindent
An example of a performant rewrite is that of the depth generation python script. 
As shown in the excerpt, inference operates on batches supplied by a DataLoader, with all other operations also being executed 
batchwise on the device (the GPU in our case); only once the batch is fully processed is it moved back to regular (host) 
memory, and the individual priors extracted with relevant metadata for use/saving.
\noindent
We also note that predicted depths are quantized to 8-bit unsigned integers, all predicted priors are quantized this way;
for example normals are quantized from 3 * 32-bit floats to 3 * 8-bit unsigned integers; this is due to compute and storage limitations.
For example the SRN cars dataset contain 387,956 images, each 128 * 128 pixels, storing only priors such as depths and normals
as 32-bit floats for each image would require: 

\begin{align*}
    \textbf{Depths:} \quad & 387,956 \times 128 \times 128 \times 4 \text{ bytes} \\
    &= 25,425,084,416 \text{ bytes} \approx \mathbf{25.43 \text{ GB}} \\[1em]
    \textbf{Normals:} \quad & 387,956 \times 128 \times 128 \times 3 \times 4 \text{ bytes} \\
    &= 76,275,253,248 \text{ bytes} \approx \mathbf{76.28 \text{ GB}}
\end{align*}

\noindent
This makes storing the dataset with calculated priors impractical, be it in memory or disk, quantization allows us to cut
these requirements down to a quarter of the original size.
\newline

\noindent
Prior generation scripts were succesfully implemented for:
\begin{itemize}
    \item Depth
    \item Surface Normals
    \item Segmentation
\end{itemize}
All prior generation scripts can be found in the \verb|/geometry-priors| folder within the 3DGS-priors repository.
\newline

\noindent
As to improve training and evaluation performance, we chose to generate priors in advance for selected datasets. As such we developed
a pipeline that executes the prior generation scripts and constructs a ready-to-use dataset, alongside a custom DataLoader that can read said dataset.
Implementation details can be found in \textbf{Section 2.4: Data pipelines}.

\subsubsection{Model Changes}

\noindent
The first change to the model was to have the top-level \verb|GaussianSplatPredictor| class
dynamically calculate the number of input channels required based on the training configuration, 
this information was then passed to the underlying Convolutional layers and UNet blocks during initialisation.

\begin{lstlisting}[language=Python, caption=Channel calculation code]
def calc_channels(cfg):    
    # Base RGB channels
    in_channels = 3

    # Older configs may not have relvant options, select() returns None if the option is missing
    if OmegaConf.select(cfg, "data.use_pred_depth") is True:
        in_channels += 1
    if OmegaConf.select(cfg, "data.use_pred_normal") is True:
        in_channels += 3

    return in_channels
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=New model initialisation code]
# Calculate number of input channels        
self.in_channels = calc_channels(cfg)

# Initialise correct model depending on if Gaussian mean offsets are to be calculated
if cfg.model.network_with_offset:
    split_dimensions, scale_inits, bias_inits = self.get_splits_and_inits(True, cfg)
    self.network_with_offset = networkCallBack(cfg, 
                                cfg.model.name,
                                self.in_channels,
                                split_dimensions,
                                scale = scale_inits,
                                bias = bias_inits)
    assert not cfg.model.network_without_offset, "Can only have one network"
if cfg.model.network_without_offset:
    split_dimensions, scale_inits, bias_inits = self.get_splits_and_inits(False, cfg)
    self.network_wo_offset = networkCallBack(cfg, 
                                cfg.model.name,
                                split_dimensions,
                                scale = scale_inits,
                                bias = bias_inits)
    assert not cfg.model.network_with_offset, "Can only have one network"
\end{lstlisting}

added film layer for text input
\newline

\noindent
One of our desired features was to allow a model with new priors to be fine-tuned using existing weights, namely those from the
pretrained Splatter Image models. This was achieved by grafting the old weights of a pretrained model onto a newly generated
GaussianSplatPredictor. This grafting mechanism was implemented externally to the model, as such its implementation details are covered in
\textbf{Section 2.3.6: Training and Evaluation Changes}.
\newline

\noindent
\subsubsection{Manual LoRA Integration}
To adapt the Splatter Image architecture for fine-tuning on the ShapeNet-SRN Cars dataset (with depth and normal priors), we added Low-Rank Adaptation (LoRA) \cite{HuLoRA} into the GaussianPreidictor module of the Splatter Image. LoRA adds low-rank trainable matrices into frozen, pretrained layers. This preserves the base model, while further finetuning it to the new data. 

\noindent
Existing libraries such as Hugging Face's PEFT \cite{peft} provide out-of-the-box LoRA integration, we found them incompatible with the weight-grafting mechanism required by the SplatterImage pipeline. Splatter Image relies on grafting weights from pre-trained checkpoints where input channel dimensions change (e.g., when adding depth or normal map channels). This leads to shape mismatches during initialisation steps (such as for PEFT's LoRA layers) since the pre-trained weights cannot be directly loaded into layers with modified input dimensions. 

\noindent
We added LoRA manually within the \texttt{GaussianPredictor} and \texttt{train\_network} modules, using code from Microsoft's \texttt{loralib} \cite{hu2022lora}. We modified the underlying \texttt{Linear} and \texttt{Conv2d} layers of the U-Net architecture to include a \texttt{LoRALayer} mixin. This allows the freezing of pre-trained base weights while injecting the LoRA matrices ($A$ and $B$) directly into the forward pass. As mentioned earlier, this preserves the frozen pretrained weights from the original 3-channel RGB model, but is compatible with the new channels. 

\subsubsection{Training and Evaluation Changes}
The existing Splatter Image evaluation script needed minimal changes, thanks to how we integrated our priors into the project.
The training script initially needed minimal changes, but then was split into two versions, one with minimal changes for standard training,
and one with additional changes required for LoRA support. 
\newline

\noindent
For evaluation code, the first change was adding support for loading our pretrained models available on HuggingFace.

\begin{lstlisting}[language=Python, caption=New model loading code]
# Load pretrained model from HuggingFace if no local model specified
if args.experiment_path is None:        
    # Eval run on the our new dataset with priors
    if dataset_name in ["cars_priors"]:
        cfg_path = hf_hub_download(repo_id="MVP-Group-Project/splatter-image-priors", 
                                filename="model-depth-normal/config.yaml")
        model_path = hf_hub_download(repo_id="MVP-Group-Project/splatter-image-priors",
                            filename="model-depth-normal/model_best.pth")
    
    # Eval run on previous Splatter Image datasets
    else:            
        cfg_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", 
                                filename="config_{}.yaml".format(dataset_name))
        if dataset_name in ["gso", "objaverse"]:
            model_name = "latest"
        else:
            model_name = dataset_name
        model_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-v1", 
                            filename="model_{}.pth".format(model_name))
    
else:
    cfg_path = os.path.join(experiment_path, ".hydra", "config.yaml")
    model_path = os.path.join(experiment_path, "model_latest.pth")

# load cfg
training_cfg = OmegaConf.load(cfg_path)   
\end{lstlisting}

\noindent
Input preparation was also changed, namely priors are concatenated (if enabled) as additional channels to the input RGB images,
before being fed into the network. Splatter Image uses a DataLoader to receives batches of RGB images to perform infrfence on. 
The DataLoader does not directly return a tensor of images, but a dictionary containing relevant batch data, namely XXX XXX XX. 
This allowed our priors to be introduced into the preparation code easily, by simply providing them a key/value pairs in 
the dictionary provided by the DataLoader, in the form of tensors matching the RGB image batch. The priors can be accessed 
by KERY AND then concatenated with RGB images in a specific order as to generate the input tensors for the model to 
perform inference on. Assertions are also performed to verify that priors are indeed available for the currently
active dataset.

\begin{lstlisting}[language=Python, caption=Network input preparation code]
# Concatenate selected priors
input_images = data["gt_images"][:, :model_cfg.data.input_images, ...]
if "use_pred_depth" in model_cfg.data and model_cfg.data.use_pred_depth:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicated maps!"
    input_images = torch.cat([input_images,
                    data["pred_depths"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
if "use_pred_normal" in model_cfg.data and model_cfg.data.use_pred_normal:
    assert model_cfg.data.category == "cars_priors", "Dataset does not have predicated maps!"
    input_images = torch.cat([input_images,
                    data["pred_normals"][:, :model_cfg.data.input_images, ...]],
                    dim=2)

# Get camera to center depth
if model_cfg.data.origin_distances:
    input_images = torch.cat([input_images,
                    data["origin_distances"][:, :model_cfg.data.input_images, ...]],
                    dim=2)
\end{lstlisting}

\noindent
For the training code, .

ADDED HF LAODING, PERFORMS GRAFT

ADDED CONATENATION< DATASET LAODING REMAINS THE SAME

For the LoRA specific version, additional changes

\subsection{Data pipelines}
[Explain your data format, how you consume the data in your algorithms, and data augmentation.]

DIAGRAM: SRN FILE STRUCTURE -> ORCHESTRATOR -> PARALLEL MODELS -> PARQUET
DIAGRAM: PARQUET -> HUGGINGFACE STRUCTURE -> HUGGINGFACE


\noindent
For training and evaluation performance reasons, we chose to generate priors in advance for selected datasets, as such we developed
a pipeline, run by an orchestrator pattern notebook, that would set up a Virtual Machine for every model, then proceed to install the correct depednecies into each VM, using requiremtns files stored alongside the scripts in \verb|/geometry-priors|, and then
execute the prior generation notebooks in order to geenrate a complete, ready-to-use dataset.
CODE BLOCK - VM setup
CODE BLOCK - REQUIREMTN LOADING
The parquet files can also be processed into a file structure appropriate for upload to HuggingFace, where the pregenereated datset remains
avaiable. 
Models use the pregenerated datasets via a custom laoder we implemented, taht slots in into all of the exisitng training/evlaution code.

TO SPEED UP TRAINIG AND EVUALTION, WE PREGENERATE THE PRIORS FOR ALL MODELS USING THE MODELS SELECTED AS PER OUR research
DOING SO WE GENERATE A NEW DATASET

FIRST THE USER DOWNLAODS AND SETS UP THE APPRIPORAITE DATASET AS PER SPLATTER IMAGE DOC
RUNS GEENRTE DATASET NTOEBOOK WITH CORRECT ARGS, THIS WILL FOLLW ORDCHESTOR PTTERN DESCRIBED IN EARIELR SECTION
MODELS OUTPUT PARQUET FIELS IN THE SET DESTIATION, WITH EXAMPLE ROWS:
EXAMPLE RGBS
EXAMPLE PSOES
EXAMPLE INTRINS
EXMAPLE DEPTHS
EXAMPE NORMALS
These can now be used however the user wants to, we add an additional processing step to transform aprquet files into
file structure appropriate for uplaod to huggingface, matching the following config:

YAML CONFIG

A DIAGRAM OF HOW THIS WHOLE PEIPLEINE WORKS IS SEEN IN FIGURES

Take a concretne example, we offer one ready pregenerated dataset on HF:
We took our input data from the ShapeNet-SRN dataset from \cite{srn} at $128 \times 128$ resolution.
transform pipeline standard, the depth, rgb, normal columns
store images as uint8s in C H W format, this is as there is X images, do math, thus this manmy GB of data,
we must un fortunately for compute reasons take this loss of accuracy.

TRAINIGN LOADS HF DATASET
LOAD HF PRETRAINED WEIGHTS
PERFORMS WEIGHT GRAFT
TRAINING OCCURS
WANDB SAVES WEIGHT OCCASNIONALLY
WEIGHTS ARE FUSED AT SAVE
THESE WEIGHTS ARE UPLOADED TO HF WITH THIS NAMING SCHEME:

\subsection{Training procedures}
[Explain which framework and optimizers you use, how you implemented the training logic.]

THE ENTIRE MODEL IS PROGRAMMED USING TORCH
WE USE PEFT OR WHATEVRE TO ADD LORA Layers

OUR TRAINING IS SET BY CONFIG FROM WADB?

SHWO CONFGI

WHEN MODEL TRAINIGN OCCURS CONFIG OPTIOSN ARE VERFIEID TO BE COMPATIABLE
OPTIONS INCLUIDE USING LORA vs NOT, WHICH PRIORS TO USE, AND WHETEHR YOU WNAT TO START THE MODEL WITH EXISITNG WEIGHTS (if so grafted)
WEIGHTS FUSED AT SAVE

TRAINGING IS RUN USING DEFAULT SPALTTER IAMGE APRAMSTERS, Namely:
LIST OPTIMSIER
OtHER OPTIOSM

THE AVAIABLE MODCEL WEIGHST ON HF WERE TRAINIG NNTOEBOOK RUSN ON CLOAB WITH A100
ITERS: 60K

\subsection{Testing and validation procedures}
[Explain which framework you use, how you implemented the testing/ validation logic.]

\section{Chapter 3: Experiments and Evaluation}
\subsection{Datasets}
[Explain the datasets utilized: what they contain, why they are utilized, assumptions, limitations, possible extensions.]

The standard benchmark for evaluating single-view 3D reconstruction is ShapeNet-SRN \cite{srn}, hence we used this to test and evaluate our main model implementation. For this dataset, we specifically use the "Car" class, which used the "car" class of ShapeNet v2 \cite{ShapeNet} with 2.5k 3D CAD model instances. The SRN dataset was generated by disabling transparencies and specularities and training on 50 observations of each instance at a resolution of $128 \times 128$ pixels, with camera poses being randomly generated on a sphere with the object at the origin. A limitation of this dataset is the lack of subject variety in the dataset as the model may end up overfitting to cars. A possible extension to address this limitation could be to include other classes in the ShapeNet-SRN database to make sure that the model can still generalise to other types of objects.

An extension of this dataset is implemented in \cite{xu2019disn}, which presents a Deep Implicit Surface Network to generate a 3D mesh from a 2D image by predicting the underlying signed distance fields. In the paper, they generated a 2D dataset composed of renderings of the models in ShapeNet Core \cite{ShapeNet}. For each mesh model, the dataset provides 36 renderings with smaller variation and 36 views with larger variation (bigger yaw angle range and larger distance variation). The object is allowed to move away from the origin, which provides more degrees of freedom in terms of camera parameters, and the "roll" angle of the camera is ignored since it was deemed very rare in real-world scenarios. The images were rendered at a higher resolution of $224 \times 224$ pixels and were paired with a depth image, a normal map and an albedo image as shown in figure \ref{fig:ground_truth}. This dataset was mainly used as a ground truth to evaluate the generation of geometry priors (e.g. normal map and depth map). A limitation of this dataset would be its small size since only 72 samples are available for us to use, such that the performance of geometry prior generation may not be evaluated correctly. However, in the same GitHub repository, the script to generate these images from the ShapeNet Core dataset is provided, so a possible extension given more time could be to include more images by running the script on other objects in the ShapeNet Core dataset.

\subsection{Training and testing results}
[Explain the training and testing results with graphs and elaborating on why they make sense, what could be improved.]

\subsection{Qualitative results}
[Show in figures and explain visual results. Include different interesting cases covering different aspects/ limitations/ dataset diversity. If not converged, explain what we can expect once converged. Include any other didactic examples here.]

\subsection{[Optional] Quantitative results}
[A table and associated explanations for quantitative results.]

\subsubsection{LoRA experiment results}
To evaluate whether Low-Rank Adaptation (as described in \textbf{Section 2.3.5: Model Changes}) is effective, we performed a set of finetuning experiments. LoRA behaviour can be adjusted through three hyperparameters: (LINK BACK TO THE SUBSECTIONS IN SECTION 2 HERE)

We tested multiple configurations by varying these hyperparameters, while keeping all other components. We measured the performance using PSNR, SSIM and LPIPS. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/lora_experiments_psnr_distribution.png}
    \caption{PSNR distribution for different LoRA configurations}
    \label{fig:lora_psnr}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/lora_experiments_ssim_distribution.png}
    \caption{SSIM distribution for different LoRA configurations}
    \label{fig:lora_ssim}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stats-results/lora_experiments_lpips_distribution.png}
    \caption{LPIPS distribution for different LoRA configurations}
    \label{fig:lora_lpips}
\end{figure}

\subsection{[Optional] Comparison to state-of-the-art}
[Qualitative and/ or quantitative comparisons to one or more recent works, especially the baseline work.]

\section{Chapter 4: Conclusions and Future Directions}
\subsection{Conclusions}
[Summarize what the project was about and the main conclusions.]

\subsection{Discussion of limitations}
[Explain the limitations of your technique. You may want to refer to previous sections or show figures on the limitations.]

halluciantion in hidden areas still a problem, as at end of the day you dont know what there, only doign best guess basd on alrge datasets
lot of data cnd comptue needed to perofrm accurate reweults
we lacked compute to generate and test more models priors

\subsection{Future directions}
[State a few future directions for research and development. These typically follow from the discussion on limitations.]

generating and testing mroe priors, trying longer training times, largert datasets
more priors aoriented to hidden areas, could bring up planes again,

\subsection{Project Contributions}
"You may find the template for the project report here. We do not enforce any page limits but please make sure to address each section appropriately as explained in the document. In particular, please pay special attention to clarifying the contribution of each group member."
Should we clarify here or throughout document?

Section 1.1: Kacper and Alex
Section 1.2: Kacper
Section 1.3: Kacper
Section 2.1: Kacper and Radhika and Alex
Section 2.2: Kacper
Section 2.3: Kacper and Radhika and Alex
Section 2.4: Kacper
Section 2.5: Kacper
Section 2.6: Kacper
Section 3.1: FILL IN
Section 3.2: Radhika
Section 3.3: FILL IN
Section 3.4: FILL IN
Section 3.5: FILL IN
Section 4.1: FILL IN
Section 4.2: FILL IN
Section 4.3: FILL IN

Depths exploration: Radhika
Segmentation exploration: Radhika
Normals exploration: Alex
Planes exploration: Alex

Splatter Image setup and bugfixes: Radhika and Alex
Splatter Image architectural modification (Grafting, Channel changes, FiLM, Cross-Attention): Kacper
Splatter Image LoRA integration: Radhika
Splatter Image Training Modification: Kacper
Splatter Image Eval Modification: Radhika and Kacper
generate depths: Kacper
generate normals: Kacper
generate segmentation: Radhika
Splatter Image cars priors dataloader: Kacper
HF dataset orchestorator: Kacper

Testing dataloader: Kacper
Testing model: Kacper
Testing LoRA configurations: Radhika 

Results processing/graphing code: Alex
Image collection, citation collection and verification: Alex

\bibliographystyle{unsrt}
\bibliography{citations}

\end{document}